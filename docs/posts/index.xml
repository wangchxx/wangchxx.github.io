<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on My Notes</title>
    <link>https://wangchxx.github.io/posts/</link>
    <description>Recent content in Posts on My Notes</description>
    <generator>Hugo -- 0.138.0</generator>
    <language>en</language>
    <lastBuildDate>Thu, 24 Oct 2024 00:01:41 +0200</lastBuildDate>
    <atom:link href="https://wangchxx.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DL | Diffusion Models 4 - Conditional Diffusion Models</title>
      <link>https://wangchxx.github.io/posts/diffusion_4_conditional_df/</link>
      <pubDate>Thu, 24 Oct 2024 00:01:41 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/diffusion_4_conditional_df/</guid>
      <description>&lt;h1 id=&#34;conditional-diffusion-models&#34;&gt;Conditional diffusion models&lt;/h1&gt;
&lt;p&gt;Given $Y$, the forward-time SDE generates $X_{t}$ conditioned on $Y$ and has $X_{t}\sim p_{t}(\cdot|Y)$:
&lt;/p&gt;
$$
d X_{t} = fdt + gdW_{t}, \quad X_{0} \sim q_{0} = p_{data}(\cdot|Y).
$$&lt;p&gt;
The reverse-time SDE generates $\bar{X}_{0}\sim p_{0}(\cdot|Y)=p_{data}(\cdot|Y)$:
&lt;/p&gt;
$$
d\bar{X}_{t} = (f-g^2 \nabla_{x}\log p_{t}(\bar{X}_{t}|Y))dt + gd\bar{W}_{t}, \quad \bar{X}_{T}\sim p_{T}(\cdot|Y)\approx \mathcal{N}(0, \sigma^2_{T} I).
$$&lt;p&gt;
So we need to learn the conditional score $\nabla_{x} \log p_{t}(X_{t}|Y)$. But how to do it?&lt;/p&gt;
&lt;h2 id=&#34;classifier-guidance&#34;&gt;Classifier guidance&lt;/h2&gt;
&lt;p&gt;By Bayes&amp;rsquo; rule
&lt;/p&gt;</description>
    </item>
    <item>
      <title>DL | Diffusion Models 3 - Understanding DDMP from a Unified Prospective</title>
      <link>https://wangchxx.github.io/posts/diffusion_3_from_a_unified_prospective/</link>
      <pubDate>Wed, 23 Oct 2024 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/diffusion_3_from_a_unified_prospective/</guid>
      <description>&lt;h2 id=&#34;understand-ddmp-from-vlb&#34;&gt;Understand DDMP from VLB&lt;/h2&gt;
&lt;p&gt;In the &lt;a href=&#34;https://wangchxx.github.io/posts/diffusion_2_preliminary_sde/&#34;&gt;Diffusion-models-1&lt;/a&gt; we have introduced the DDMP model
&lt;/p&gt;
$$
X_t|X_{t-1} = \mathcal{N}(\sqrt{1-\beta_t}X_{t-1}, \beta_t I), \quad X_t|X_0 = \mathcal{N}(\sqrt{\bar{\alpha}_t} X_0, (1-\bar{\alpha}_{t})I),
$$&lt;p&gt;
with $X_{0}\sim q_{0} = p_{data}$, $\alpha_{t} = 1-\beta_{t}$ and $\bar{\alpha}_{t} = \prod_{s=1}^t (1-\beta_{s})$.&lt;/p&gt;
&lt;p&gt;The loss function is derived by minimizing the negative log-likelihood $-\log p_{\theta}(X_{0})$ with a variational lower bound $L_{VLB}$
&lt;/p&gt;
$$
L_{VLB} = \mathbb{E}_{q}\left[ \log  \frac{q(x_{1:T}|x_{0})}{p_{\theta}(x_{0:T})}  \right]  = \sum_{t=0}^T L_{t}
$$&lt;p&gt;
with
&lt;/p&gt;
$$
L_{t} = D_{KL}(q(x_{t}|x_{t+1},x_{0})\| p_{\theta}(x_{t}|x_{t+1})),\; 1\leq t\leq T-1. 
$$&lt;p&gt;
Since the conditional distribution $X_{t}|(X_{t-1}, X_{0})$ is Gaussian
&lt;/p&gt;</description>
    </item>
    <item>
      <title>DL | Diffusion Models 2 - Preliminary ODE and SDE</title>
      <link>https://wangchxx.github.io/posts/diffusion_2_preliminary_sde/</link>
      <pubDate>Tue, 22 Oct 2024 00:01:41 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/diffusion_2_preliminary_sde/</guid>
      <description>&lt;h2 id=&#34;basics&#34;&gt;Basics&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;ODE definition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Consider the ordinary differential equation (ODE)
&lt;/p&gt;
$$
\frac{dX}{dt}(t) = f(X(t), t),
$$&lt;p&gt;
which we also express as $dX_{t} = f(X_{t}, t)dt$, where $X(t), f(X(t),t)\in  \mathbb{R}^d$. Then, $\{X(t)\}_{t}$ is a deterministic curve.&lt;/p&gt;
&lt;p&gt;We can define the ODE by the limit
&lt;/p&gt;
$$
X_{k+1} = X_{k} + \Delta _{t} f(X_{k}, k\Delta_{t}), \quad k = 0,1,2,\dots,
$$&lt;p&gt;
under $\Delta_{t}\to 0$ with $t = k\Delta_{t}$. Precisely, $\left\{ X_{\left\lfloor  \frac{t}{\Delta_{t}}  \right\rfloor} \right\}_{t} \to \{X_{t}\}_{t}$ uniformly on compact intervals.&lt;/p&gt;</description>
    </item>
    <item>
      <title>DL | Diffusion Models 1 - DDMP</title>
      <link>https://wangchxx.github.io/posts/diffusion_1_ddmp/</link>
      <pubDate>Mon, 21 Oct 2024 04:34:23 +0800</pubDate>
      <guid>https://wangchxx.github.io/posts/diffusion_1_ddmp/</guid>
      <description>&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;A diffusion probabilistic model is a parameterized Markov chain that gradually adds noise to the data and then learn to reverse the diffusion process to generate data samples from noise.&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://wangchxx.github.io/posts/diffusion_1_ddmp/ddmp_graph.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;Compared with other AI tasks, image generation is harder, since it does not have a standard answer. To solve this issue, GAN and VAE are propsed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;GAN uses another model (discriminator) to decide the quality of generated images.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RL | Multiplayer Monte Carlo Tree Search</title>
      <link>https://wangchxx.github.io/posts/rl/rl_09_mcts/</link>
      <pubDate>Mon, 20 Sep 2021 23:01:33 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/rl/rl_09_mcts/</guid>
      <description>&lt;h2 id=&#34;1-multi-agent-systems&#34;&gt;1. Multi-Agent Systems&lt;/h2&gt;
&lt;p&gt;A multi-agent system (MAS) consists of multiple decision-making agents which interact in a shared environment to achieve common or conflicting goals.&lt;/p&gt;
&lt;p&gt;MAS research spans a range of problems, such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;how to design MAS to incentivize certain behaviors in agents,&lt;/li&gt;
&lt;li&gt;how to design algorithms enabling agents to achieve specific goals in a MAS,&lt;/li&gt;
&lt;li&gt;how information is communicated and propagated among agents.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-monte-carlo-tree-search-mcts&#34;&gt;2. Monte Carlo Tree Search (MCTS)&lt;/h2&gt;
&lt;p&gt;The MCTS focus on the analysis of the most promising moves, expanding the search tree based on random sampling of the search space. MCTS is based on many palyouts, and the result of each playout is then used to weight the nodes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RL | Dynamics Approximation for Model-Based RL</title>
      <link>https://wangchxx.github.io/posts/rl/rl_08_dynamics_approximation/</link>
      <pubDate>Thu, 09 Sep 2021 13:26:05 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/rl/rl_08_dynamics_approximation/</guid>
      <description>&lt;p&gt;One shortcoming to DP methods is that they assume the knowledge of the dynamics. To overcome it, we introduced two model-free methods: MC and TP. They are capable of learning a wide range of tasks. However, such methods suffer from very high sample complexity. In this chapter, we consider a model-based method with dynamics approximation.&lt;/p&gt;
&lt;h2 id=&#34;1-dynamics-approximation&#34;&gt;1. Dynamics Approximation&lt;/h2&gt;
&lt;p&gt;Let $p(s&#39;|s,a)$ be the unknown dynamics function, and $f_\theta(s_t, a_t)$ be the learned dynamics function parameterized by $\theta$. A straightforward parameterization for $f_\theta(s_t, a_t)$ would take as input the current state-action pair and ouput the predicted next state $\hat{s}_{t+1}$. However, this function can be difficult to learn as $s_t$ and $s_{t+1}$ can be too similar and the action has seemingly little effect on the ouput. Instead predict the next state directly, we predict the change in state $s_t$ over the time step
&lt;/p&gt;</description>
    </item>
    <item>
      <title>RL | Policy Gradient Methods</title>
      <link>https://wangchxx.github.io/posts/rl/rl_07_policy_gradient/</link>
      <pubDate>Thu, 26 Aug 2021 03:20:07 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/rl/rl_07_policy_gradient/</guid>
      <description>&lt;p&gt;So far all methods have been action-value methods. In this chapter we consider methods that learn a parameterized policy that can select actions with consulting a value function. We write a parametric policy $\pi_\theta(a|s) = \Pr(A_t = a| S_t = s, \theta)$.&lt;/p&gt;
&lt;p&gt;One advantage of parameterizing policies is that it can learn stochastic policies and change action probabilities smoothly as a function of the learned parameter. Largely because of this, stronger convergence guarantees are available for policy-gradient methods.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RL | Value Function Approximation</title>
      <link>https://wangchxx.github.io/posts/rl/rl_06_value_function_approximation/</link>
      <pubDate>Wed, 25 Aug 2021 22:17:58 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/rl/rl_06_value_function_approximation/</guid>
      <description>&lt;p&gt;So far we have represented value function by a lookup table. But sometimes we may have large MDPs where there are too many states or actions to store in memory. One solution to this problem is estimating value function with function approximation&lt;/p&gt;
$$ 
 \hat{v}(s,w) \approx v_\pi(s), \quad \hat{q}(s,a,w) \approx q_\pi(s,a).
$$&lt;p&gt;For the simplicity of notation, we denote the function approximator by $f$. With a differentiable loss function $J_w$, e.g.
&lt;/p&gt;
$$ 
 J_w = \mathbb{E}_\pi (f(s,w) - v_\pi(s))^2,
$$&lt;p&gt;
we can approximate $v$ or $q$ by GD, i.e.
&lt;/p&gt;</description>
    </item>
    <item>
      <title>RL | Model-Free: Temporal Difference Learning</title>
      <link>https://wangchxx.github.io/posts/rl/rl_05_temporal_difference/</link>
      <pubDate>Tue, 24 Aug 2021 18:28:15 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/rl/rl_05_temporal_difference/</guid>
      <description>&lt;p&gt;Temporal-difference (TD) learning is a combination of Monte Carlo ideas and DP ideas. Like MC methods, TD can learn from raw experience without knowledge of the environment&amp;rsquo;s dynamics. Like DP, TD updates estimates based in part on other learned estimates, without waiting for a final output ($G_t$).&lt;/p&gt;
&lt;h2 id=&#34;1-td-prediction&#34;&gt;1. TD Prediction&lt;/h2&gt;
&lt;p&gt;Recall that a simple MC methods for nonstationary environment updates $V(S_t)$ by
&lt;/p&gt;
$$ 
 V(S_t) \leftarrow V(S_t) + \alpha [ G_t - V(S_t)],
$$&lt;p&gt;
where $G_t$ can only be known when a visit to $S_t$ occurs. The simplest TD method updates $V$ by
&lt;/p&gt;</description>
    </item>
    <item>
      <title>RL | Model-Free: Monte Carlo Methods</title>
      <link>https://wangchxx.github.io/posts/rl/rl_04_monte_carlo/</link>
      <pubDate>Tue, 24 Aug 2021 18:27:52 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/rl/rl_04_monte_carlo/</guid>
      <description>&lt;p&gt;We have introduced the DP algorithm for estimating value functions and optimal policies. One drawback to DP is that assumes complete knowledge of the environment. Now we will introduce two model-free methods: &lt;em&gt;Monte Carlo&lt;/em&gt; methods and &lt;em&gt;temporal-difference&lt;/em&gt; learning.&lt;/p&gt;
&lt;p&gt;In this chapter we will consider the Monte Carlo methods for prediction and control in an unknown MDP.&lt;/p&gt;
&lt;h2 id=&#34;1-monte-carlo-prediction&#34;&gt;1. Monte Carlo Prediction&lt;/h2&gt;
&lt;p&gt;We begin by consider Monte Carlo methods for learning the value function $v_\pi$ for a given policy $\pi$. Suppose that we wish to estimate $v_\pi(s)$ given a set of episodes under policy $\pi$
&lt;/p&gt;</description>
    </item>
    <item>
      <title>RL | Dynamic Programming</title>
      <link>https://wangchxx.github.io/posts/rl/rl_03_dynamic_programming/</link>
      <pubDate>Tue, 24 Aug 2021 03:00:31 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/rl/rl_03_dynamic_programming/</guid>
      <description>&lt;p&gt;There are two common tasks in an MDP&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;prediction: estimating the valued function $\pi_\pi$ given a MDP and a policy $\pi$.&lt;/li&gt;
&lt;li&gt;control: finding optimal policy $\pi_*$ and corresponding optimal value function $v_*$ given a MDP.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this chapter we will show how to use DP to solve the prediction and control problems in an MDP. DP is of limited utility in RL both because of their assumption of a perfect model (environment&amp;rsquo;s dynamics are completely known) and because of their great computational expense, but they are important theoretically.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RL | Markov Decision Processes</title>
      <link>https://wangchxx.github.io/posts/rl/rl_02_markov_decision_process/</link>
      <pubDate>Tue, 24 Aug 2021 03:00:02 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/rl/rl_02_markov_decision_process/</guid>
      <description>&lt;h2 id=&#34;notations&#34;&gt;Notations&lt;/h2&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{S}$: state space.&lt;/li&gt;
&lt;li&gt;$\mathcal{A}$: action space.&lt;/li&gt;
&lt;li&gt;$\mathcal{R}$: reward space.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-agent-environment-interaction&#34;&gt;1. Agent-Environment Interaction&lt;/h2&gt;
&lt;p&gt;On each step, the agent selects an action $A_t\in\mathcal{A}$ based on the state $S_t$, and then the environment reveals a reward $R_{t+1}$ and moves to a new state $S_{t+1}$ corresponding the selected action $A_t$ and state $S_t$.&lt;/p&gt;
&lt;p&gt;Write
&lt;/p&gt;
$$ 
 p(s&#39;,r|s,a) = \Pr(S_t = s&#39;, R_t = r| S_{t-1} = s, A_{t-1} = a).
$$&lt;p&gt;
The function $p$ defines the &lt;em&gt;dynamics&lt;/em&gt; of the MDP. Given the four-argument function $p$, one can compute anything else one might want tot know about the environment, such as&lt;/p&gt;</description>
    </item>
    <item>
      <title>RL | Multi-armed Bandits</title>
      <link>https://wangchxx.github.io/posts/rl/rl_01_multi_armed_bandits/</link>
      <pubDate>Tue, 24 Aug 2021 00:56:37 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/rl/rl_01_multi_armed_bandits/</guid>
      <description>&lt;h2 id=&#34;notations&#34;&gt;Notations&lt;/h2&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$A_t$: the action selected at time $t$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$R_t$: the reward received at time $t$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$q_*(a)$: the expected reward given an action $a$, i.e. $q_*(a) = \mathbb{E}[R_t|A_t = a]$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$Q_t(a)$: the estimated value of action $a$ at time $t$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-a-k-armed-bandit-problem&#34;&gt;1. A k-armed Bandit Problem&lt;/h2&gt;
&lt;p&gt;Consider a problem that we have k different actions, and each choice leads to a reward with a certain probability distribution depending on the action selected. Our objective is to maximize the expected total reward.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reading | Estimating a smooth function on a large graph (2)</title>
      <link>https://wangchxx.github.io/posts/reading/gp_graph_2/</link>
      <pubDate>Sat, 21 Aug 2021 04:18:02 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/reading/gp_graph_2/</guid>
      <description>&lt;p&gt;This paper follows from the one discussed in the previous article and shows that the introduced estimator achieves the optimal rate.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kirichenko, A., &amp;amp; van Zanten, H. (2018). Minimax lower bounds for function estimation on graphs. ArXiv:1709.06360 [Math, Stat]. &lt;a href=&#34;http://arxiv.org/abs/1709.06360&#34;&gt;http://arxiv.org/abs/1709.06360&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1-main-results&#34;&gt;1. Main results&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Theorem 1 (&lt;em&gt;Regression&lt;/em&gt;)&lt;/dt&gt;
&lt;dd&gt;Under conditions (G), (L) and (S),
$$ 
 \inf_{\hat{f}} \sup_{f\in H^\beta(Q)} \mathbb{E}_f || \hat{f} - f||_n^2 \asymp n^{-2\beta/(2\beta+r)}.
$$
where the infimum is taken over all estimators $\hat{f} = \hat{f}(Y_1,...,Y_n)$.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;This theorem shows that the minimax rate for the regression problem on the graph is equal to $n^{-\beta/(2\beta+r)}$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reading | Estimating a smooth function on a large graph (1)</title>
      <link>https://wangchxx.github.io/posts/reading/gp_graph_1/</link>
      <pubDate>Fri, 20 Aug 2021 23:28:03 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/reading/gp_graph_1/</guid>
      <description>&lt;p&gt;This paper proposed an approach to estimating smooth functions on graphs using GP priors.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kirichenko, A., &amp;amp; van Zanten, H. (2015). Estimating a smooth function on a large graph by Bayesian Laplacian regularisation. ArXiv:1511.02515 [Math, Stat]. &lt;a href=&#34;http://arxiv.org/abs/1511.02515&#34;&gt;http://arxiv.org/abs/1511.02515&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1-problem-setup&#34;&gt;1. Problem setup&lt;/h2&gt;
&lt;p&gt;Let $G = (V,E)$ be a connected undirected graph and $A$ its adjacency matrix, $D$ its degree matrix. Then $L = D-A$ is the Laplacian of the graph. Suppose that there is a function $f:[0,1]\to \mathbb{R}$ on the graph. We are interested in the $n$-dimentional vector $f = (f_1,...,f_n)^T$, where $f_i = f(i/n)$. We measure distances and norms of functions using the norm $||\cdot||_n$ defined by
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reading | Bayesian Classification of Multiclass Functional Data</title>
      <link>https://wangchxx.github.io/posts/reading/gp_multiclass_classification/</link>
      <pubDate>Sun, 08 Aug 2021 17:02:19 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/reading/gp_multiclass_classification/</guid>
      <description>&lt;p&gt;Traditionally, we reduce the multiclass classification problem to a binary problem by 1-vs-1 or 1-vs-rest. This article proposed an alternative method to solve the multiclass classification problem.&lt;/p&gt;
&lt;p&gt;Li, X., &amp;amp; Ghosal, S. (2018). Bayesian Classification of Multiclass Functional Data. ArXiv:1808.00662 [Stat]. &lt;a href=&#34;http://arxiv.org/abs/1808.00662&#34;&gt;http://arxiv.org/abs/1808.00662&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-problem-setup&#34;&gt;1. Problem setup&lt;/h2&gt;
&lt;p&gt;Consider a response $Y$ taking values $k = 1,...,K$, with functional covariate $(X(t), t\in[0,1])$. The main problem is to estimate the probability $P(Y = k|X)$, which can be modeld by
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reading | Posterior consistency of Gaussian process prior for nonparametric binary regression</title>
      <link>https://wangchxx.github.io/posts/reading/gpconsistency_for_binary_classification/</link>
      <pubDate>Sat, 07 Aug 2021 18:24:27 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/reading/gpconsistency_for_binary_classification/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Ghosal, S., &amp;amp; Roy, A. (2006). Posterior consistency of Gaussian process prior for nonparametric binary regression. The Annals of Statistics, 34(5). &lt;a href=&#34;https://doi.org/10.1214/009053606000000795&#34;&gt;https://doi.org/10.1214/009053606000000795&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The previous article discussed the methodology of GP priors for binary classification. This article would show the consistency results.&lt;/p&gt;
&lt;h2 id=&#34;1-problem-setup&#34;&gt;1. Problem setup&lt;/h2&gt;
$$ 
 p(x) = P(Y=1|x) = H(\eta(x)).
$$&lt;p&gt;
A GP prior with mean function $\mu(x)$ and covariance kernle $\sigma(x,x&#39;)$ is put on the function $\eta$. The covariance kernel is assumed to be of the form
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reading | Nonparametric binary regression using a Gaussian process prior</title>
      <link>https://wangchxx.github.io/posts/reading/gppriors_for_binary_classification/</link>
      <pubDate>Sat, 07 Aug 2021 08:36:14 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/reading/gppriors_for_binary_classification/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s discuss this paper&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choudhuri, N., Ghosal, S., &amp;amp; Roy, A. (2007). Nonparametric binary regression using a Gaussian process prior. Statistical Methodology, 17.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1-problem-setup&#34;&gt;1. Problem Setup&lt;/h2&gt;
&lt;p&gt;Consider a binary classification probem given by
&lt;/p&gt;
$$ 
 p(x) = P(Y = 1|X=x) = 1- P(Y = -1|X = x).
$$&lt;p&gt;
This problem commonly occurs in many fields of application, such as medical and spatial statics. Traditionally, we would model this problem as
&lt;/p&gt;
$$ 
 p(x) = H(\eta(x)),
$$&lt;p&gt;
where $H$ is commonly choosen as a cdf, called the &lt;em&gt;link function&lt;/em&gt;, and $\eta$ can be choosen parametrically or nonparametrically. In this paper, a nonparametric approach was studied, where $H$ was known, and the function $\eta$ was estimated with a Gaussian prior.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian Statistics| Gaussian Process Priors (3)</title>
      <link>https://wangchxx.github.io/posts/bayes/bayes_7_gp3/</link>
      <pubDate>Thu, 05 Aug 2021 00:01:41 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/bayes/bayes_7_gp3/</guid>
      <description>&lt;p&gt;In GP(1) and GP(2), we studied the RKSH and consistency rate of GP priors. In this sectin we are going to consider the smoothness of the target function and see how the smoothness level impacts the consistency rates.&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;Suppose that we want to estimate a density function $p_0 \in C^\beta[0,1]$, where $C^\beta[0,1]$ denotes the H$\mathrm{\&#34;o}$lder space of order $\beta$. By Assouadâ€™s method it can shown that no estimator can achieve better rates than $n^{-\beta/(2\beta+1)}$ uniformly, in terms of the distance $d(p_0, p) = ||p_0 - p||_1$. It has been long known that some estimators can achieve this rate given known smoothness level $\beta$, for instance, kernel estimators.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian Statistics| Gaussian Process Priors (2)</title>
      <link>https://wangchxx.github.io/posts/bayes/bayes_6_gp2/</link>
      <pubDate>Thu, 22 Jul 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/bayes/bayes_6_gp2/</guid>
      <description>&lt;!-- # Bayesian Statistics| Gaussian Process Priors (2) --&gt;
&lt;p&gt;In &lt;a href=&#34;https://wangchxx.github.io/posts/bayes/bayes_4_gp1/&#34;&gt;GP(1)&lt;/a&gt; we introduced the RKHS of the GP and some of its properties. The main content of this section is to derive the &lt;strong&gt;posterior contraction rate&lt;/strong&gt; of the GP.&lt;/p&gt;
&lt;p&gt;At the same time, in &lt;a href=&#34;https://wangchxx.github.io/posts/bayes/bayes_5_contraction/&#34;&gt;Posterior Consistency and Contraction&lt;/a&gt;, we derived the most important conclusion:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;contraction_thm&#34; loading=&#34;lazy&#34; src=&#34;https://wangchxx.github.io/img_bayes_GP/Contraction_2.PNG&#34;&gt;&lt;/p&gt;
&lt;p&gt;To show the posterior contraction of GP, we only need to check conditions 5.7-5.9.&lt;/p&gt;
&lt;h2 id=&#34;1-posterior-contraction&#34;&gt;1. Posterior Contraction&lt;/h2&gt;
&lt;p&gt;Theorem (GP contraction)
:&lt;img alt=&#34;GP contraction&#34; loading=&#34;lazy&#34; src=&#34;https://wangchxx.github.io/img_bayes_GP/GPrate_1.PNG&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian Statistics| Gaussian Process Priors (1)</title>
      <link>https://wangchxx.github.io/posts/bayes/bayes_4_gp1/</link>
      <pubDate>Wed, 21 Jul 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/bayes/bayes_4_gp1/</guid>
      <description>&lt;!-- # Bayesian Statistics| Gaussian Process Priors (1) --&gt;
&lt;p&gt;In the previous chapters, we introduced the &lt;strong&gt;Dirichlet Process (DP) prior&lt;/strong&gt;, which is primarily used as a prior on measure spaces. In this chapter, we will introduce the &lt;strong&gt;Gaussian Process (GP)&lt;/strong&gt;, which can be used as a prior on function spaces. Consider the scenario where we have sample pairs $(X_i,Y_i),i\leq n$. We are interested in studying the relationship between the inputs $X_i$ and the outputs $Y_i$.  A common model for such a relationship is
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian Statistics| Posterior Consistency and Contraction</title>
      <link>https://wangchxx.github.io/posts/bayes/bayes_5_contraction/</link>
      <pubDate>Tue, 20 Jul 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/bayes/bayes_5_contraction/</guid>
      <description>&lt;!-- # Bayesian Statistics| Posterior Consistency and Contraction --&gt;
&lt;p&gt;This chapter discusses the theoretical question of whether non-parametric Bayes methods truly work. In other words, it addresses whether the posterior distribution really converges to the so-called &amp;ldquo;true&amp;rdquo; parameter $\theta_0$. Contraction is a richer concept than consistency, as it also involves the rate of convergence.&lt;/p&gt;
&lt;p&gt;Dirichlet Process priors and Gaussian Process priors are common non-parametric Bayesian methods, along with their various variants. It would be too cumbersome to discuss the convergence issues of these methods on a case-by-case basis. Therefore, this chapter will focus on some general theory, which can then be applied to specific models as needed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian Statistics| Dirichlet Process</title>
      <link>https://wangchxx.github.io/posts/bayes/bayes_3_dirichlet/</link>
      <pubDate>Mon, 19 Jul 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/bayes/bayes_3_dirichlet/</guid>
      <description>&lt;!-- # Bayesian Statistics| Dirichlet Process --&gt;
&lt;p&gt;The Dirichlet Process (DP) is widely used in &lt;strong&gt;Bayesian nonparametrics&lt;/strong&gt;, where it is often treated as a default prior on spaces of probability measures. As a &lt;strong&gt;prior&lt;/strong&gt; on probability measures, the DP reflects our prior belief about the potential distributions of data, with the assumption that the data might be drawn from an infinite mixture of possible underlying distributions. This is why the Dirichlet Process is often called a &lt;strong&gt;nonparametric prior&lt;/strong&gt;: it does not assume a fixed, finite number of parameters but rather allows the model to grow with the complexity of the data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian Statistics| Bernstein-von Mises Theorem</title>
      <link>https://wangchxx.github.io/posts/bayes/bayes_2_bvm/</link>
      <pubDate>Sun, 18 Jul 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/bayes/bayes_2_bvm/</guid>
      <description>&lt;!-- # Bayesian Statistics| Bernstein-von Mises Theorem  --&gt;
&lt;p&gt;From a non-Bayesian perspective, when we aim to estimate the parameters of a certain model, we often apply the Central Limit Theorem (CLT) and obtain a result similar to this.&lt;/p&gt;
$$
\sqrt{n} (\hat{\theta}_n - \theta_0) \to N(0,\Sigma)
$$&lt;p&gt;From a Bayesian perspective, a similar conclusion can be drawn, often referred to as the Bayesian Central Limit Theorem. This is precisely the Bernstein-von Mises Theorem (BvM), which will be introduced in this article.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Functional Analysis | 4 Duality and Hahn-Banach Theorem</title>
      <link>https://wangchxx.github.io/posts/analysis_3/functional_4/</link>
      <pubDate>Sat, 24 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_3/functional_4/</guid>
      <description>&lt;h2 id=&#34;1-dual-spaces&#34;&gt;1. Dual Spaces&lt;/h2&gt;
&lt;p&gt;In this section we describe the dual of several of the standard spaces.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;Theorem 1 &lt;span id = &#34;c4_functional_norm&#34;&gt;&lt;/dt&gt;
&lt;dd&gt;If $X$ is a finite-dimensional normed linear space with basis $\{v_1,...,v_n\}$, then $X&#39;$ has a basis $\{f_1,...,f_n\}$ s.t. $f_j(v_k) = \delta_{jk}$. In particular, $dim (X&#39;) = dim (X)$.&lt;/dd&gt;
&lt;dt&gt;proof&lt;/dt&gt;
&lt;dd&gt;Let $x\in X$, we can represent is as $x = \sum_{k=1}^n \alpha_k v_k$. Define $f_j:X\to\mathbb{F}$ by
$$f_j(x) = \alpha_j.$$
It can be verified that $f_j$ is a linear transformation s.t. $f_j(v_k) = \delta_{jk}$. Moreover, $f_j$ is continuous since $X$ is finite-dimensional, and thus $f_j\in X&#39;$.
&lt;p&gt;Next, we show that $\{f_1,...,f_n\}$ is a basis for $X&#39;$. Suppose that $\beta_1,...,\beta_n$ are scalars s.t. $\sum_{j=1}^n \beta_j f_j = 0$. Then
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Functional Analysis | 3 Linear Operators</title>
      <link>https://wangchxx.github.io/posts/analysis_3/functional_3/</link>
      <pubDate>Fri, 23 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_3/functional_3/</guid>
      <description>&lt;h2 id=&#34;1-continuous-linear-transformations&#34;&gt;1. Continuous Linear Transformations&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Lemma 1 (&lt;strong&gt;Continuity&lt;/strong&gt;) &lt;span id = &#34;c3_lem_continuity&#34;&gt;&lt;/dt&gt;
&lt;dd&gt;Let $X$ and $Y$ be two normed linear spaces and let $T:X\to Y$ be a linear transformation. The following are equivalent:
&lt;ol&gt;
&lt;li&gt;T is uniformly continuous,&lt;/li&gt;
&lt;li&gt;$T$ is continuous,&lt;/li&gt;
&lt;li&gt;$T$ is continuous at $0$,&lt;/li&gt;
&lt;li&gt;there exists a number $M$ s.t. $\|T(x)\|_Y\leq M$ whenever $x\in X$ with $\|x\|_X\leq 1$,&lt;/li&gt;
&lt;li&gt;there exists a number $M$ s.t. $\|T(x)\|_Y \leq M\|x\|_X$ for all $x\in X$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/dd&gt;
&lt;dt&gt;proof&lt;/dt&gt;
&lt;dd&gt;$1 \Rightarrow 2\Rightarrow 3$ are trivial. We prove the left assertions only.
$3\Rightarrow 4$. As $T$ is continuous at $0$, there exist $\delta$ s.t. $\|T(x)\|_Y &lt; 1$ whenveer $\|x\|_X &lt;\delta$. Let $w\in X$ and $\|w\|_X \leq 1$. As
$$ 
   \| \delta w/2 \|_X \leq \delta/2&lt;\delta,
  $$
$\|T(\delta w/2)\|_Y&lt;1$ and as $T$ is linear, it implies that $ \|T(w)\|_Y&lt; 2/\delta$. We draw the conclusion by taking $M = 2/\delta$.
&lt;p&gt;$4 \Rightarrow 5$. Since $T(0) = 0$, it is clear that $\|T(0)\|_Y \leq M \|0\|_X$. For any $w\in X, w\neq 0$, we have $\|T(\frac{w}{\|w\|_X}) \|_Y &lt; M$. By linearity again we have
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Functional Analysis | 2 Hilbert Spaces</title>
      <link>https://wangchxx.github.io/posts/analysis_3/functional_2/</link>
      <pubDate>Thu, 22 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_3/functional_2/</guid>
      <description>&lt;h2 id=&#34;1-inner-product-spaces&#34;&gt;1. Inner Product spaces&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 1 (&lt;strong&gt;Inner product&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;Let $X$ be a vector space. An inner product on $X$ is a function $\langle\cdot,\cdot\rangle:X\times X\to \mathbb{F}$, s.t. for all $x,y,z\in X$, $\alpha,\beta\in\mathbb{F}$,
&lt;ol&gt;
&lt;li&gt;$\langle x,x \rangle\geq 0$,&lt;/li&gt;
&lt;li&gt;$\langle x,x \rangle = 0 \Leftrightarrow x = 0$ ,&lt;/li&gt;
&lt;li&gt;$\langle \alpha x + \beta y ,z\rangle = \alpha \langle x,z \rangle + \beta \langle y,z \rangle$,&lt;/li&gt;
&lt;li&gt;$ \langle x,y \rangle = \overline{\langle y,x \rangle}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A vector space $X$ with an inner product $\langle \rangle$ is called an &lt;code&gt;inner product space&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Functional Analysis | 1 Normed Spaces</title>
      <link>https://wangchxx.github.io/posts/analysis_3/functional_1/</link>
      <pubDate>Wed, 21 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_3/functional_1/</guid>
      <description>&lt;h2 id=&#34;1-finite-dimensional-normed-spaces&#34;&gt;1. Finite-dimensional Normed Spaces&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 1 (Norm)&lt;/dt&gt;
&lt;dd&gt;Let $X$ be a vector space over $\mathbb{F}$. A &lt;code&gt;norm&lt;/code&gt; on $X$ is a function $\|\cdot\|:X\to\R$ s.t. for all $x,y\in X$ and $\alpha\in\mathbb{F}$,
&lt;ol&gt;
&lt;li&gt;$\| x\|\geq 0$ ,&lt;/li&gt;
&lt;li&gt;$\|x\|=0$ iff $x=0$,&lt;/li&gt;
&lt;li&gt;$\| \alpha x\| = |\alpha| \| x\|$,&lt;/li&gt;
&lt;li&gt;$\|x+y\|\leq \|x\| + \|y\|$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A vector $X$ equipped with a norm is called a &lt;code&gt;normed vector space&lt;/code&gt; or just a &lt;code&gt;normed space&lt;/code&gt;. In particular, a &lt;code&gt;unit vector&lt;/code&gt; in $X$ is a vector $x$ s.t. $\|x\|=1$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Functional Analysis | 0 Preliminaries</title>
      <link>https://wangchxx.github.io/posts/analysis_3/functional_0/</link>
      <pubDate>Tue, 20 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_3/functional_0/</guid>
      <description>&lt;h1 id=&#34;functional-analysis--0-preliminaries&#34;&gt;Functional Analysis | 0 Preliminaries&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;Notation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\R$: the set of real numbers,&lt;/li&gt;
&lt;li&gt;$\mathbb{C}$: the set of complex numbers,&lt;/li&gt;
&lt;li&gt;$\N$: the set of positive integers.&lt;/li&gt;
&lt;li&gt;$\mathbb{F} = \{\R,\mathbb{C}\}$,&lt;/li&gt;
&lt;li&gt;$\Re_z$: the real part of complex number $z$,&lt;/li&gt;
&lt;li&gt;$\Im_z$: the imaginary part of complex number $z$,&lt;/li&gt;
&lt;li&gt;$sp(A)$: span of set $A$.&lt;/li&gt;
&lt;li&gt;$dim V$: dimension of vector space $V$.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-linear-algebra&#34;&gt;1. Linear Algebra&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 1 (&lt;strong&gt;Vector space&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;A &lt;code&gt;vector space&lt;/code&gt; over $\mathbb{F}$ is a non-empty set $V$ together with two function $x+y:V\times V\to V, \alpha x: \mathbb{F}\times V\to V$ satisfying for all $x,y,z\in V$ and all $\alpha,\beta\in\mathbb{F}$,
&lt;ol&gt;
&lt;li&gt;$x+y = y+x$,&lt;/li&gt;
&lt;li&gt;there exists a unique $0\in V$ s.t. $x+0 = x$,&lt;/li&gt;
&lt;li&gt;there exists a unique $-x\in V$ s.t. $x + (-x) = 0$,&lt;/li&gt;
&lt;li&gt;$1x = x$,&lt;/li&gt;
&lt;li&gt;$\alpha(x+y) = \alpha x + \alpha y, (\alpha+\beta)x = \alpha x + \beta x$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/dd&gt;
&lt;dt&gt;Definition 2 (&lt;strong&gt;Linear subspace&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;Let $V$ be a vector space. A non-empty set $U\subset V$ is a &lt;code&gt;linear subspace&lt;/code&gt; of $V$ if $U$ is itself a vector space, which is equivalent to the condition
$$ 
   \alpha x + \beta y \in U, \;\forall \alpha,\beta\in \mathbb{F}, x,y\in U.
$$&lt;/dd&gt;
&lt;dt&gt;Definition 3&lt;/dt&gt;
&lt;dd&gt;Let $S$ be a set and let $V$ be a vector space over $\mathbb{F}$. We denote the set of functions $f:S\to V$ by $F(S,V)$. With the definition of scalar multiplication and vector addition, $F(S,V)$ is a vector space over $\mathbb{F}$.&lt;/dd&gt;
&lt;dt&gt;Definition 4 (Linear transformation)&lt;/dt&gt;
&lt;dd&gt;Let $V,W$ be vector spaces over $\mathbb{F}$. A function $T: V\to W$ is called a &lt;code&gt;linear transformation&lt;/code&gt; if, for all $\alpha,\beta\in \mathbb{F}$ and $x,y\in V$
$$ 
 T(\alpha x + \beta y) = \alpha T(x) + \beta T(y).
$$
The set of all linear transformations $T:V\to W$ is denoted by $L(V,W)$. With the definition of scalar multiplication and vector addition, $L(V,W)$ is a vector space.&lt;/dd&gt;
&lt;/dl&gt;
&lt;h2 id=&#34;2-metric-spaces&#34;&gt;2. Metric Spaces&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 5 (Metric space)&lt;/dt&gt;
&lt;dd&gt;A &lt;code&gt;metric&lt;/code&gt; on a set $M$ is a function $d:M\times M\to \R$ with the following properties,
&lt;ol&gt;
&lt;li&gt;$d(x,y)\geq 0$,&lt;/li&gt;
&lt;li&gt;$d(x,y)= 0 \Leftrightarrow x=y$,&lt;/li&gt;
&lt;li&gt;$d(x,y) = d(y,x)$,&lt;/li&gt;
&lt;li&gt;$d(x,z)\leq d(x,y) + d(y,z)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If $d$  is a metric on $M$, then the pair $(M,d)$ is called a &lt;code&gt;metric space&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mathematical Analysis | 13 Fourier Series</title>
      <link>https://wangchxx.github.io/posts/analysis_1/math_analysis_13/</link>
      <pubDate>Tue, 13 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_1/math_analysis_13/</guid>
      <description>&lt;h2 id=&#34;1-fourier-series&#34;&gt;1. Fourier Series&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 1 (&lt;strong&gt;Trigonometric polynomial&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;A &lt;code&gt;Trigonometric polynomial&lt;/code&gt; is a finite sum of the form
$$ 
 f(x) = a_0 + \sum_{n=1}^N (a_n \cos nx  + b_n \sin nx),
$$
where $a_i,b_i$ are complex numbers.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;It can be written in the form
&lt;/p&gt;
$$ 
\begin{equation}
f(x) = \sum_{n=-N}^N c_n e^{inx},
\end{equation}
$$&lt;p&gt;
which is more convenient for most purposes. It is clear that every Trigonometric polynomial has period $2\pi$. Notice that
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mathematical Analysis | 12 Power Series</title>
      <link>https://wangchxx.github.io/posts/analysis_1/math_analysis_12/</link>
      <pubDate>Mon, 12 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_1/math_analysis_12/</guid>
      <description>&lt;p&gt;In this Chapter we shall the power series formed by the sequence of power functions $\{c_n (x-x_0)^n\}$, i.e. of the form
&lt;/p&gt;
$$ 
 \sum_{n=0}^\infty c_n (x-x_0)^n. \tag{1}
$$&lt;p&gt;
W.l.o.g. we shall focus on the case where $x_0 = 0$, i.e.
&lt;/p&gt;
$$ 
   \sum_{n=0}^\infty c_n x^n. \tag{2}
$$&lt;h2 id=&#34;1-power-series&#34;&gt;1. Power Series&lt;/h2&gt;
&lt;p&gt;First, we study the convergence of series (2).&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;Lemma 1&lt;/dt&gt;
&lt;dd&gt;Suppose the series (2) converges for $|x|&lt; R$, then it is absolutely convergent on every  $[-R+\epsilon, R-\epsilon]$.&lt;/dd&gt;
&lt;dt&gt;proof&lt;/dt&gt;
&lt;dd&gt;Let $a_n = |c_n x^n|$, which goes to zero and is bounded. We have $\lim\sup_{n\to\infty}\sqrt[n]{a_n} = 0 &lt;1$. By root test we complete the proof.
$\Box$&lt;/dd&gt;
&lt;dt&gt;Theorem 2&lt;/dt&gt;
&lt;dd&gt;Suppose the series (2) converges for $|x|&lt; R$, and define
$$ 
 f(x) =   \sum_{n=0}^\infty c_n x^n, \quad (|x|&lt; R). \tag{3}
$$
Then (2) converges uniformly on every $[-R+\epsilon, R-\epsilon]$. The function $f$ is continuous and differentiable in $(-R,R)$, and
$$ 
 f&#39;(x) = \sum_{n=1}^\infty n c_n x^{n-1}  , \quad (|x|&lt; R). \tag{4}
$$&lt;/dd&gt;
&lt;dt&gt;proof&lt;/dt&gt;
&lt;dd&gt;For $|x|&lt; R-\epsilon$, we have
$$ 
 |c_n x^n|\leq |c_n (R-\epsilon)^n|
$$
and since $\sum c_n (R-\epsilon)^n$ converges absolutely by Lemma 1, the series (2) converges uniformly on $[-R+\epsilon, R-\epsilon]$.
Next, we have $\lim\sup_{n\to\infty} \sqrt[n]{n|c_n|} = \lim\sup_{n\to\infty} \sqrt[n]{|c_n|}$ since $\sqrt[n]{n} = 1$, which implies that $f$ and $f&#39;$ have the same interval of convergence. As (4) is also a power series, it converges uniformly in $[-R+\epsilon, R-\epsilon]$. We can apply Theorem 11.10 (Differentiation) and obtain the assertion (4).
Continuity of $f$ follows from the existence of $f&#39;$.
$\Box$&lt;/dd&gt;
&lt;dt&gt;Theorem 3&lt;/dt&gt;
&lt;dd&gt;Suppose $\sum c_n$ converges, Put
$$ 
 f(x) =   \sum_{n=0}^\infty c_n x^n, \quad (|x|&lt; 1).
$$
Then
$$ 
 \lim_{x\to 1_-} f(x) = \sum_{n=0}^\infty c_n.
$$&lt;/dd&gt;
&lt;dt&gt;proof&lt;/dt&gt;
&lt;dd&gt;Let $f_n(x) = \sum_{j=0}^n c_j x^j$. $f_n \to f$ at $x=1$, so $f_n\to f$ uniformly on $[0,1]$. Since $f_n$ are continuous, then $f$ is continuous on $[0,1]$ (left continuous at $x = 1$).
$\Box$&lt;/dd&gt;
&lt;/dl&gt;
&lt;!-- Let $s_n = \sum_{j=0}^n c_j$. Then 
$$ 
   \sum_{n=0}^m c_n x^n = \sum_{n=0}^m (s_n - s_{n-1}) x^n = (1-x)\sum_{n=0}^m s_n x^n + s_m x^m.
$$
For $|x|&lt;1$, we let $m\to\infty$ and obtain
$$ 
 f(x) = (1-x)  \sum_{n=0}^\infty s_n x^n.
$$
suppose $\lim_{n\to\infty} s_n= s$. Let $\epsilon&gt;0$ and choose $N$ so that $n&gt;N$ implies
$$ 
 |s_n - s|&lt;\epsilon/2.  
$$
Since $(1-x)  \sum_{n=0}^\infty  x^n = 1 , |x|&lt;1$, we obtain
$$ 
 |f(x) - s| = |(1-x)  \sum_{n=0}^\infty (s_n -s) x^n|\leq |1-x|  
$$ --&gt;
&lt;h2 id=&#34;2-power-series-expansion&#34;&gt;2. Power Series Expansion&lt;/h2&gt;
&lt;p&gt;If (1) converges for $|x-x_0|&lt; R$, and a function $f$ can be represented as $f(x) = \sum_{n=0}^\infty c_n (x-x_0)^n$, then we say $f$ is &lt;code&gt;expanded in a power series&lt;/code&gt;  about the point $x=x_0$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mathematical Analysis | 11 Sequences and Series of Functions</title>
      <link>https://wangchxx.github.io/posts/analysis_1/math_analysis_11/</link>
      <pubDate>Sun, 11 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_1/math_analysis_11/</guid>
      <description>&lt;p&gt;We have studied how to define a number by convergent sequences or series. In this chapter we discuss how to define a function by a sequence or series of functions, and study the properties of this function.&lt;/p&gt;
&lt;h2 id=&#34;1-uniform-convergence&#34;&gt;1. Uniform Convergence&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 1 (&lt;strong&gt;Pointwise convergence&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;Suppose $\{f_n\}$ be a sequence of functions defined on a set $E$, and suppose that the sequence of numbers $\{f_n(x)\}$ converges for every $x\in E$. We can define a function $f$ by
$$ 
 f(x) = \lim_{n\to\infty} f_n(x).  \tag{1}
$$
We say that $\{f_n\}$ &lt;code&gt;converges pointwise&lt;/code&gt; to $f$ on $E$. Similarly, if $\sum f_n (x)$ converges for every $x\in E$, we can define $f$ by
$$ 
 f(x) = \sum_{n=1}^\infty f_n(x).  \tag{2}
$$&lt;/dd&gt;
&lt;dt&gt;Definition 2 (&lt;strong&gt;Uniform convergence&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;We say that a sequence of functions $\{f_n\}$ &lt;code&gt;converges uniformly&lt;/code&gt; to a function $f$ on $E$ if for every $\epsilon&gt;0$ there exists an integer $N$ s.t. $n\geq N$ implies
$$ 
 |f_n(x) - f(x)|&lt;\epsilon \tag{3}
$$
for all $x\in E$.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;It is clear that every uniformly convergent sequence is pointwise convergent.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mathematical Analysis | 10 Numerical Series</title>
      <link>https://wangchxx.github.io/posts/analysis_1/math_analysis_10/</link>
      <pubDate>Sat, 10 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_1/math_analysis_10/</guid>
      <description>&lt;h2 id=&#34;1-convergence&#34;&gt;1. Convergence&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 1 (Series)&lt;/dt&gt;
&lt;dd&gt;Given a sequence $\{a_n\}$, we define a sequence $\{s_n\}$ by
$$ 
 s_n = \sum_{i=1}^n a_i.
$$
We call $\sum_{i=1}^\infty a_i$ an &lt;code&gt;infinite series&lt;/code&gt; or just a &lt;code&gt;series&lt;/code&gt;. The numbers $s_n$ are called the &lt;code&gt;partial sums&lt;/code&gt; of the series.&lt;/dd&gt;
&lt;dt&gt;Definition 2 (Convergence of series)&lt;/dt&gt;
&lt;dd&gt;If the sequence $\{s_n\}$ converges to $s$, we say that the series converges, and write $\sum_{n=1}^\infty a_n = s$. If $\{s_n\}$ diverges, the series is said to be diverge.&lt;/dd&gt;
&lt;dt&gt;Theorem 3 (Cauchy criterion)&lt;/dt&gt;
&lt;dd&gt;$\sum a_n$ converges iff for any $\epsilon&gt;0$, there exists an integer $N$ s.t.
$$ 
 |\sum_{k=n}^m a_k|&lt; \epsilon
$$
if $m\geq n\geq N$.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;In other words, if $\sum a_n$ converges, then $\lim_{n\to\infty} a_n = 0$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mathematical Analysis | 9 Riemann Integrals</title>
      <link>https://wangchxx.github.io/posts/analysis_1/math_analysis_9/</link>
      <pubDate>Fri, 09 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_1/math_analysis_9/</guid>
      <description>&lt;h2 id=&#34;1-definitions&#34;&gt;1. Definitions&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 1 (&lt;strong&gt;Partition&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;Let $[a,b]$ be a given interval, by a &lt;code&gt;partition&lt;/code&gt; $T$ of $[a,b]$, we mean a finite set of points $x_0,x_1,...,x_n$, where
$$ 
 a = x_0\leq x_1\leq \cdots\leq x_n = b
$$
We write $\Delta x_i = x_i  - x_{i-1}$, and denote $|T| = \max_i \Delta x_i$ as the &lt;code&gt;mesh&lt;/code&gt; of the partition $T$.&lt;/dd&gt;
&lt;dt&gt;Definition 2 (&lt;strong&gt;Riemann sum&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;Let $f$ be a given function defined on $[a,b]$, and $T$ be a partition of $[a,b]$. For any points $x_{i-1}\leq\xi\leq x_i$, $i=1,2,...,n$, the summation
$$ 
 \sum_{i=1}^n f(\xi_i)\Delta x_i,
$$
is called the &lt;code&gt;Riemaan sum&lt;/code&gt; of $f$ on $[a,b]$.&lt;/dd&gt;
&lt;dt&gt;Definition 3 (&lt;strong&gt;Riemann integrable&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;Let $f$ be a given function defined on $[a,b]$, and $J$ be a fixed number. If for any $\epsilon&gt;0$, there exists $\delta&gt;0$, s.t. for every partition $T$ of $[a,b]$ with $|T|&lt;\delta$ and for any choice of $\{\xi_i\}$ we have
$$ 
 |\sum_{i=1}^n f(\xi_i)\Delta x_i - J|  &lt;\epsilon,\tag{1}
$$
we say that the function $f$ is &lt;code&gt;Riemann integrable&lt;/code&gt; on the interval $[a,b]$, we write $f\in\mathcal{R}$ and we denote the $J$ by $J = \int_a^b f(x) dx$.&lt;/dd&gt;
&lt;dt&gt;Definition 4 (Stieltjes integral)&lt;/dt&gt;
&lt;dd&gt;Let $\alpha$ be an increasing and bounded function on $[a,b]$, $f:[a,b]\to \mathbb{R}$. For each partition $T$ of $[a,b]$, we write $\Delta \alpha_i = \alpha(x_i) - \alpha(x_{i-1})$. If for any $\epsilon&gt;0$, there exists $\delta&gt;0$, s.t. for every partition $T$ of $[a,b]$ with $|T|&lt;\delta$ and for any choice of $\{\xi_i\}$ we have
$$ 
 |\sum_{i=1}^n f(\xi_i)\Delta \alpha_i - J|  &lt;\epsilon,\tag{2}
$$
we say that the function $f$ is &lt;code&gt;Stieltjes integrable&lt;/code&gt; w.r.t. $\alpha$ on the interval $[a,b]$, and write $f\in \mathcal{R}(\alpha)$, denoted by $J = \int_a^b f(x) d\alpha(x)$.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;By taking $\alpha(x) = x$, the Riemann integral can be viewed as a special case of the Stieltjes integral.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mathematical Analysis | 8 Indefinite Integrals</title>
      <link>https://wangchxx.github.io/posts/analysis_1/math_analysis_8/</link>
      <pubDate>Thu, 08 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_1/math_analysis_8/</guid>
      <description>&lt;h2 id=&#34;1-definitions&#34;&gt;1. Definitions&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 1 (&lt;strong&gt;Primary function&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;Let $f$ and $F$ be functions defined on $I$. If $F&#39;(x) = f(x)$ for every $x\in I$, we say that $F$ is the &lt;code&gt;primary function&lt;/code&gt; of $f$ on $I$.&lt;/dd&gt;
&lt;dt&gt;Theorem 2 (&lt;strong&gt;Existence&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;If $f$ is continuous on $I$, then $f$ possesses a primary function $F$ on $I$.&lt;/dd&gt;
&lt;dt&gt;Definition 3 (&lt;strong&gt;Indefinite integral&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;The set of all primary functions of $f$ on $I$ is called the &lt;code&gt;indefinite integral&lt;/code&gt; of $f$ on $I$, denoted by $\int f(x) dx= F(x) + C$.&lt;/dd&gt;
&lt;/dl&gt;
&lt;h2 id=&#34;2-change-of-variable-and-integration-by-parts&#34;&gt;2. Change of Variable and Integration by Parts&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Theorem 4 (&lt;strong&gt;Change of variable&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;Let $f(x)$ defined on $I$, $\phi(t)$ differentiable on $J$ and $\phi(J)\subset I$.&lt;/dd&gt;
&lt;/dl&gt;
&lt;ol&gt;
&lt;li&gt;If the indefinite integral $\int f(x) dx = F(x) + C$ exists, then the indefinite integral $\int f(\phi(t))\phi&#39;(t) dt$ exists too, and $\int f(\phi(t))\phi&#39;(t) dt = F(\phi(t))+C$&lt;/li&gt;
&lt;li&gt;If the function $x = \phi(t)$ has inverse function $t=\phi^{-1}(x)$,and $\int f(x) dx = F(x) + C$ exists, then then the indefinite integral $\int f(\phi(t))\phi&#39;(t) dt = G(t) + C$ exists too, and $\int f(x) dx =G(\phi^{-1}(x)) + C$.&lt;/li&gt;
&lt;/ol&gt;
&lt;dl&gt;
&lt;dt&gt;Theorem 5 (&lt;strong&gt;Integration by Parts&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;If $u(x)$ and $v(x)$ are differentiable, and $\int v(x)u&#39;(x) dx$ exists, then $\int v&#39;(x) u(x) dx$ exists too, and $\int v&#39;(x) u(x) dx = v(x)u(x) - \int v(x) u&#39;(x) dx$.&lt;/dd&gt;
&lt;dt&gt;proof&lt;/dt&gt;
&lt;dd&gt;$[u(x)v(x)]&#39; = u&#39;(x)v(x) + v&#39;(x)u(x)$.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;$\Box$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mathematical Analysis | 7 Completeness of the Real Numbers</title>
      <link>https://wangchxx.github.io/posts/analysis_1/math_analysis_7/</link>
      <pubDate>Wed, 07 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_1/math_analysis_7/</guid>
      <description>&lt;h2 id=&#34;1-nested-intervals-theorem&#34;&gt;1. Nested Intervals Theorem&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 1 (&lt;strong&gt;Nested intervals&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;A sequence of closed intervals $\{[a_n,b_n]\}$ is called a sequence of &lt;code&gt;nested closed intervals&lt;/code&gt; if&lt;/dd&gt;
&lt;/dl&gt;
&lt;ol&gt;
&lt;li&gt;$[a_{n+1}, b_{n+1}]\subset [a_n, b_n]$,&lt;/li&gt;
&lt;li&gt;$\lim_{n\to\infty}(b_n - a_n) = 0$.&lt;/li&gt;
&lt;/ol&gt;
&lt;dl&gt;
&lt;dt&gt;Theorem 2 (&lt;strong&gt;Nested intervals theorem&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;Let $\{[a_n,b_n]\}$ be a sequence of nested closed intervals, there exist a unique real number $\xi$ s.t. $\xi\in [a_n,b_n]$ for every $n$.&lt;/dd&gt;
&lt;dt&gt;proof&lt;/dt&gt;
&lt;dd&gt;Notice that $\{a_n\}$ is an increasing and bounded sequence, and thus it has limit $\xi$ s.t. $\xi\geq a_n$ for all $n$. Similarly, the sequence $\{b_n\}$ also has a limit. Combing (ii) of the definition, we have
$$ 
 \lim_{n\to\infty}a_n =   \lim_{n\to\infty}b_n = \xi,
$$
and $a_n\leq \xi\leq b_n$ for all $n$. The left work is to show the uniqueness. Suppose that there exists another $\xi&#39;$ s.t. $a_n\leq \xi&#39;\leq b_n$, we have
$$ 
 |\xi - \xi&#39;|\leq b_n - a_n \to 0,
$$
which means that $\xi = \xi&#39;$.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;$\Box$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mathematical Analysis | 6 Mean Value Theorem</title>
      <link>https://wangchxx.github.io/posts/analysis_1/math_analysis_6/</link>
      <pubDate>Tue, 06 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_1/math_analysis_6/</guid>
      <description>&lt;h2 id=&#34;1-rolles-theorem&#34;&gt;1. Rolle&amp;rsquo;s Theorem&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Theorem 1 (Rolle&amp;rsquo;s theorem)&lt;/dt&gt;
&lt;dd&gt;If a function $f$ satisfies&lt;/dd&gt;
&lt;/dl&gt;
&lt;ol&gt;
&lt;li&gt;$f$ is continuous on a closed interval $[a,b]$,&lt;/li&gt;
&lt;li&gt;$f$ is differentiable on $(a,b)$,&lt;/li&gt;
&lt;li&gt;$f(a) = f(b)$,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;then there exists a number $\xi\in(a,b)$ s.t. $f&#39;(\xi)= 0$.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;proof&lt;/dt&gt;
&lt;dd&gt;Because $f$ is continuous on $[a,b]$, $f$ has maximum and minimum, denoted by $M$ and $m$ respectively. If $m= M$, then $f$ is constant, hence the assertion is true. In the case $m&lt;M$, the assumption $f(a) = f(b)$ implies that at least one of the maximum and minimum is achieved at $\xi\in(a,b)$. Then the Fermat&amp;rsquo;s theorem completes the proof.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;$\Box$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mathematical Analysis | 5 Derivatives and Differentiations</title>
      <link>https://wangchxx.github.io/posts/analysis_1/math_analysis_5/</link>
      <pubDate>Mon, 05 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_1/math_analysis_5/</guid>
      <description>&lt;h2 id=&#34;1-derivatives&#34;&gt;1. Derivatives&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 1 (Derivative at $x_0$)&lt;/dt&gt;
&lt;dd&gt;Given a function $f$ defined on an neighbourhood of $x_0$ $U(x_0,\delta)$, we say that $f$ is &lt;code&gt;differentiable&lt;/code&gt; at $x_0$ if the following limit exists
$$ 
\lim_{x\to x_0}\frac{f(x)- f(x_0)}{x-x_0}, 
$$
denoted by $f&#39;(x_0)$.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;According to the definition, we can see that if $f$ has derivative at $x_0$, then it is continuous at $x_0$. But the converse is not true.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 2 (One-sided derivatives)&lt;/dt&gt;
&lt;dd&gt;Given a function $f$ defined on $[x_0,x_0+\delta)$, then it has a right derivative at $x_0$, if the right limit exists
$$ 
\lim_{x\to x_0^+}\frac{f(x)- f(x_0)}{x-x_0}, 
$$
denoted by $f&#39;_{+}(x_0)$.&lt;/dd&gt;
&lt;dt&gt;Theorem 3&lt;/dt&gt;
&lt;dd&gt;$f&#39;(x_0)$ exists iff both $f&#39;_{+}(x_0)$ and $f&#39;_{-}(x_0)$ exist.&lt;/dd&gt;
&lt;dt&gt;Definition 4 (Local maximum)&lt;/dt&gt;
&lt;dd&gt;If there is a neighbourhood $U(x_0,\delta)$ where for any $x\in U(x_0,\delta)$ s.t.
$$ 
 f(x_0)\geq f(x),
$$
we say that $f$ attains its local maximum at $x_0$.&lt;/dd&gt;
&lt;dt&gt;Theorem 5 (Fermat&amp;rsquo;s Theorem)&lt;/dt&gt;
&lt;dd&gt;If $f$ has a local extremum at $x_0$ and if it is differentiable at $x_0$, then $f&#39;(x_0) = 0$.&lt;/dd&gt;
&lt;/dl&gt;
&lt;h2 id=&#34;2-differentials&#34;&gt;2. Differentials&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 5 (Differential)&lt;/dt&gt;
&lt;dd&gt;We define an increment of $f$ by $\Delta y = f(x_0 + \Delta x) - f(x_0)$. If there exists a constant $A$ s.t.
$$ 
 \Delta y = A\Delta x + o(\Delta x),
$$
we say $f$ has a differential at $x_0$, denoted by $d y|_{x_0} = A\Delta x$.&lt;/dd&gt;
&lt;dt&gt;Theorem 6&lt;/dt&gt;
&lt;dd&gt;$f$ has a differential at $x_0$ iff $f$ is differentiable at $x_0$ and $A = f&#39;(x_0)$.&lt;/dd&gt;
&lt;/dl&gt;</description>
    </item>
    <item>
      <title>Mathematical Analysis | 4 The Continuity of Functions</title>
      <link>https://wangchxx.github.io/posts/analysis_1/math_analysis_4/</link>
      <pubDate>Sun, 04 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_1/math_analysis_4/</guid>
      <description>&lt;h2 id=&#34;1-definitions&#34;&gt;1. Definitions&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 1 (&lt;strong&gt;Continuity at $x_0$&lt;/strong&gt;)&lt;/dt&gt;
&lt;dd&gt;Given a function $f$ defined on $U(x_0,\delta)$. If $\lim_{x\to x_0} = f(x_0)$, $f$ is said to be &lt;em&gt;continuous at $x_0$&lt;/em&gt;.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;That $f$ is continuous at $x_0$ implies that we can interchange the order of $\lim$ and $f$,
&lt;/p&gt;
$$ 
 \lim_{x\to x_0} = f(\lim{x\to x_0} x).
$$&lt;p&gt;We introduce a new terminology &amp;ldquo;increment&amp;rdquo;, which is defined by $\Delta x = x - x_0$ and $\Delta y = y- y_0 = f(x_0 + \Delta x) - f(x_0)$. Now the continuity of function $f$ at $x_0$ is equivalent to
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mathematical Analysis | 3 Limit of Functions</title>
      <link>https://wangchxx.github.io/posts/analysis_1/math_analysis_3/</link>
      <pubDate>Sat, 03 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_1/math_analysis_3/</guid>
      <description>&lt;h2 id=&#34;1-limit-of-functions&#34;&gt;1. Limit of Functions&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 1 (Limit at $\infty$)&lt;/dt&gt;
&lt;dd&gt;Given a function $f$ defined on $[a,\infty)$ and a constant $A$, if for any $\epsilon&gt;0$ there exists $M$ s.t. for any $x\geq M$,
$$ 
 |f(x) - A|&lt;\infty,
$$
we say that $f$ has limit $A$ as $x\to+\infty$, denoted by $\lim_{x\to +\infty}f(x) = A$.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;If $\lim_{x\to +\infty}f(x)= \lim_{x\to -\infty}f(x) = A$, we deonte it by $\lim_{x\to \infty}f(x) = A$.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 2 (Limit at $x_0$)&lt;/dt&gt;
&lt;dd&gt;Given a function $f$ that is well defined on $U^o(x_0,\delta&#39;)$ and a constant $A$, if for any $\epsilon&gt;0$ there exists a $\delta$ s.t.
$$ 
 |f(x)- A|&lt;\infty,
$$
for any $x\in U^o(x_0,\delta)$, we say that $f$ has limit $A$ as $x\to x_0$, denoted by $\lim_{x\to x_0}f(x) = A$.&lt;/dd&gt;
&lt;dt&gt;Definition 3 (One-sided limit)&lt;/dt&gt;
&lt;dd&gt;Given a function $f$ that is well defined on $U^o_+(x_0,\delta&#39;)$ and a constant $A$, if for any $\epsilon&gt;0$ there exists a $\delta$ s.t.
$$ 
 |f(x)- A|&lt;\infty,
$$
for any $x\in U^o_+(x_0,\delta)$, we say that $f$ has right limit $A$ as $x\to x_0$ from right, denoted by $\lim_{x\to x_0^+}f(x) = A$. Similarly, the left limit can be defined.&lt;/dd&gt;
&lt;dt&gt;Theorem 4&lt;/dt&gt;
&lt;dd&gt;$\lim_{x\to x_0}f(x) = A\Leftrightarrow \lim_{x\to x_0^+}f(x) = \lim_{x\to x_0^-}f(x) = A$.&lt;/dd&gt;
&lt;dt&gt;Proposition 5 (Properties)&lt;/dt&gt;
&lt;dd&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;ol&gt;
&lt;li&gt;uniqueness. If $\lim_{x\to x_0}f(x)$ exists, then the limit is unique.&lt;/li&gt;
&lt;li&gt;local boundedness. If $\lim_{x\to x_0}f(x)$ exists, then $f$ is bouned on $U^o(x_0,\delta)$ for some $\delta$.&lt;/li&gt;
&lt;li&gt;signed. If $\lim_{x\to x_0}f(x) = A&gt;0$, then for any $0&lt;r&lt;A$, there exist a $U^o(x_0,\delta)$ on which $f(x)&gt;r&gt;0$.&lt;/li&gt;
&lt;li&gt;ordered. If both $\lim_{x\to x_0}f(x)$ and $\lim_{x\to x_0}g(x)$ exist, and  $f(x)\leq g(x)$ on some $U^o(x_0,\delta)$, then $\lim_{x\to x_0}f(x)\leq \lim_{x\to x_0}g(x)$.&lt;/li&gt;
&lt;li&gt;squeeze. If $\lim_{x\to x_0}f(x) =  \lim_{x\to x_0}g(x) = A$, and $f(x)\leq h(x)\leq g(x)$ on some $U^o(x_0,\delta)$, then  $\lim_{x\to x_0}h(x) = A$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;2-existence-of-limit&#34;&gt;2. Existence of Limit&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Theorem 6 (Heine-Cantor)&lt;/dt&gt;
&lt;dd&gt;Suppose $f$ is well defined on $U^o(x_0,\delta&#39;)$. The limit of $f(x)$ exists iff for any sequence $\{x_n\}\subset U^o(x_0,\delta&#39;)$ that is convergent to $x_0$, the sequence $\{f(x_n)\}$  converges to a fixed point.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Proof: &amp;ldquo;$\Rightarrow$&amp;rdquo; is trivial. We only show the inverse. Suppse that  $f(x_n)\to A$ as $n\to\infty$ for all sequences described as the theorem. If $f(x)$ does not converge to $A$ as $x\to x_0$, then there exists $\epsilon_0$ s.t $\forall \delta&gt;0$, there is some $x\in U^o(x_0,\delta)$ with $|f(x)- A|\geq \epsilon_0$. We take $\delta = \delta&#39;/n$, $n = 1,2,\ldots$, and thus obtain a sequence $\{x_n\}$ s.t.
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mathematical Analysis | 2 Limit of Sequences</title>
      <link>https://wangchxx.github.io/posts/analysis_1/math_analysis_2/</link>
      <pubDate>Fri, 02 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_1/math_analysis_2/</guid>
      <description>&lt;h2 id=&#34;1-limit-of-sequences&#34;&gt;1. Limit of Sequences&lt;/h2&gt;
&lt;p&gt;Given a function $f:\mathbb{N}_{+}\to \mathbb{R}$, we call $f( \mathbb{N}_{+})$ as a &lt;em&gt;sequence&lt;/em&gt;, enumerated as $a_1,a_2,\ldots$, denoted by $\{a_n\}$.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 1 (Convergence)&lt;/dt&gt;
&lt;dd&gt;We say a sequence $\\{a_n\\}$ is &lt;em&gt;convergent&lt;/em&gt; to $a$ if for any $\epsilon&gt;0$, there exits a constant $N$ s.t. for any $n\geq N$,
$$ 
 |a_n - a|&lt;\epsilon.
$$
And the point $a$ is called the &lt;em&gt;limit&lt;/em&gt; of the sequence $\{a_n\}$, denoted by $\lim_{n\to\infty}a_n = a$.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;An equivalent definition of the convergence of a sequence is given by,&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mathematical Analysis | 1 Real Numbers and Functions</title>
      <link>https://wangchxx.github.io/posts/analysis_1/math_analysis_1/</link>
      <pubDate>Thu, 01 Apr 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/analysis_1/math_analysis_1/</guid>
      <description>&lt;h2 id=&#34;1-real-numbers&#34;&gt;1. Real Numbers&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 1 (Bounds)&lt;/dt&gt;
&lt;dd&gt;$S\in\mathbb{R}$. We say that $S$ is &lt;em&gt;bounded above&lt;/em&gt; if there exits $M$ s.t. for every $x\in S$ $x\leq M$, and $M$ is called the &lt;em&gt;upper bond&lt;/em&gt; of $S$. Similarly, we say that $S$ is &lt;em&gt;bounded from below&lt;/em&gt; if $x\geq M$, and $M$ is called the &lt;em&gt;lower bound&lt;/em&gt; of $S$.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;We say that $S$ is bounded if its upper bounds and lower bounds exist.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;Definition 2 (Supremum and infimum)&lt;/dt&gt;
&lt;dd&gt;Let $\eta$ be an upper bound of $S$. If for any $\alpha&lt;\eta$ there exists $x_0\in S$ s.t. $x_0&gt;\alpha$, then $\eta$ is called the &lt;em&gt;supremum&lt;/em&gt; of $S$, denoted by $\eta = \sup S$. Similarly, $\eta$ is the &lt;em&gt;infimum&lt;/em&gt; of $S$, denoted by $\eta = \inf S$, if for any $\alpha&gt;\eta$, there exits $x_0\in S$ s.t. $x_0&lt;\alpha$.&lt;/dd&gt;
&lt;dt&gt;Theorem 3 (Existence)&lt;/dt&gt;
&lt;dd&gt;If $S$ has an upper bound, then $\sup S$ exists. If $S$ has an lower bound, then $\inf S$ exists.&lt;/dd&gt;
&lt;dt&gt;Corollary 4&lt;/dt&gt;
&lt;dd&gt;Given nonempty sets $A,B$. If $\forall x\in A,\forall y\in B$ $x\leq y$, then $\sup A$ and $\inf B$ exist. Moreover, $\sup A\leq \inf B$.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;proof:
The existence of $\sup A$ and $\inf B$ is immediate by Theorem 3. We show the inequality only. Because $y$ is an upper bound of $A$ for any $y\in B$, $\sup A\leq y$. It suggests that $\sup A$ is a lower bound of $B$, hence $\sup A\leq \inf B$.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
