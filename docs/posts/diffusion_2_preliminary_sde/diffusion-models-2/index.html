<!DOCTYPE html>
<html lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta property="og:url" content="http://localhost:1313/posts/diffusion_2_preliminary_sde/diffusion-models-2/">
  <meta property="og:site_name" content="My Math Notes">
  <meta property="og:title" content="My Math Notes">
  <meta property="og:description" content="Preliminary ODE and SDE Basics ODE definition Consider the ordinary differential equation (ODE) $$ \frac{dX}{dt}(t) = f(X(t), t), $$ which we also express as $dX_{t} = f(X_{t}, t)dt$, where $X(t), f(X(t),t)\in \mathbb{R}^d$. Then, $\{X(t)\}_{t}$ is a deterministic curve.
We can define the ODE by the limit $$ X_{k&#43;1} = X_{k} &#43; \Delta _{t} f(X_{k}, k\Delta_{t}), \quad k = 0,1,2,\dots, $$ under $\Delta_{t}\to 0$ with $t = k\Delta_{t}$. Precisely, $\left\{ X_{\left\lfloor \frac{t}{\Delta_{t}} \right\rfloor} \right\}_{t} \to \{X_{t}\}_{t}$ uniformly on compact intervals.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="My Math Notes">
  <meta name="twitter:description" content="Preliminary ODE and SDE Basics ODE definition Consider the ordinary differential equation (ODE) $$ \frac{dX}{dt}(t) = f(X(t), t), $$ which we also express as $dX_{t} = f(X_{t}, t)dt$, where $X(t), f(X(t),t)\in \mathbb{R}^d$. Then, $\{X(t)\}_{t}$ is a deterministic curve.
We can define the ODE by the limit $$ X_{k&#43;1} = X_{k} &#43; \Delta _{t} f(X_{k}, k\Delta_{t}), \quad k = 0,1,2,\dots, $$ under $\Delta_{t}\to 0$ with $t = k\Delta_{t}$. Precisely, $\left\{ X_{\left\lfloor \frac{t}{\Delta_{t}} \right\rfloor} \right\}_{t} \to \{X_{t}\}_{t}$ uniformly on compact intervals.">

  
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#262d33">
  <title>
    
    My Math Notes - 
    
  </title>
  
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;600;700&display=swap"
    rel="stylesheet">
  <link rel="stylesheet" href="https://unpkg.com/normalize.css">
  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="/css/md.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="/css/syntax.css" />
  <script src="/js/main.js"></script>
</head>
<script>
  try {
    if (!('theme' in localStorage)) {
      localStorage.theme = window.matchMedia('(prefer-color-scheme: dark)').matches ? 'dark' : 'light';
    }
    document.querySelector('html').classList.add(localStorage.theme);
  } catch (e) {
    console.error(e);
  }
</script>
<body>
  <header>
  <h1 class="row gap-1">
    <div id="theme-switcher" class="btn lg-1"></div>
    My Math Notes
  </h1>
  <nav class="row gap-1">
  
    <a href="/">Home</a>
  
    <a href="/categories">Categories</a>
  
    <a href="/series">Series</a>
  
    <a href="/about">About</a>
  
  </nav>
  <hr>
</header>
  
  
<main>
	<h1></h1>
	<div class="sm-1 mtb-1">
		Posted at &mdash; Jan 1, 0001
		
	</div>
	<p></p>
	<article class="md">
		<h1 id="preliminary-ode-and-sde">Preliminary ODE and SDE</h1>
<h2 id="basics">Basics</h2>
<p><strong>ODE definition</strong>
Consider the ordinary differential equation (ODE)
</p>
$$
\frac{dX}{dt}(t) = f(X(t), t),
$$<p>
which we also express as $dX_{t} = f(X_{t}, t)dt$, where $X(t), f(X(t),t)\in  \mathbb{R}^d$. Then, $\{X(t)\}_{t}$ is a deterministic curve.</p>
<p>We can define the ODE by the limit
</p>
$$
X_{k+1} = X_{k} + \Delta _{t} f(X_{k}, k\Delta_{t}), \quad k = 0,1,2,\dots,
$$<p>
under $\Delta_{t}\to 0$ with $t = k\Delta_{t}$. Precisely, $\left\{ X_{\left\lfloor  \frac{t}{\Delta_{t}}  \right\rfloor} \right\}_{t} \to \{X_{t}\}_{t}$ uniformly on compact intervals.</p>
<p><strong>ODE solution</strong>
We say that $\{X(t)\}_{t=0}^T$ solves ODE if it satisfies the differential form of ODE
</p>
$$
\frac{dX}{dt}(t) = f(X(t),t),
$$<p>
or the integral form of ODE
</p>
$$
X(t) = X_{0} + \int_{0}^t f(X(s),s) ds.
$$<p><strong>SDE definition</strong>
Consider the stochastic differential equation (SDE)
</p>
$$
dX_{t} = f(X_{t}, t) dt + g(X_{t}, t) dW_{t},
$$<p>
where $X_{t}, f(X_{t}, t)\in\mathbb{R}^d, g(X_{t}, t)\in\mathbb{R}^{d\times d}$, and $W_{t}$ is a $d$-dimensional Brownian motion or Wiener process. Then, $\{X_{t})\}_{t}$ is a stochastic process.</p>
<p>We can define the SDE by the limit
</p>
$$
X_{k+1} = X_{k} + \Delta _{t} f(X_{k}, k\Delta_{t}) + g(k \Delta_{t})\sqrt{ \Delta_{t} } Z_{k}, \quad k = 0,1,2,\dots,
$$<p>
under $\Delta_{t}\to 0$ with $t = k\Delta_{t}$ and $Z_{k}\sim \mathcal{N}(0,I)$ i.i.d. Precisely, $\left\{ X_{\left\lfloor  \frac{t}{\Delta_{t}}  \right\rfloor} \right\}_{t} \to \{X_{t}\}_{t}$ uniformly on compact intervals.</p>
<p><strong>SDE solution</strong>
We say that $\{X_{t}\}_{t=0}^T$ is a solution path for SDE if it is nice (right-continuous with left limits) with probability distribution defined by
</p>
$$
X_{t} = X_{0} + \int_{0}^t f(X_{s}, s) ds + \int_{0}^t g(X_{s},s) d W_{s},
$$<p>
where the Ito stochastic integral is defined as
</p>
$$
\int_{0}^t g(X_{s},s) dW_s = \lim_{ \Delta_{t} \to 0 } \sum_{k=0}^{\lfloor t  / \Delta_{t} \rfloor } g(X_{k\Delta_{t}}, k\Delta_{t}) \sqrt{ \Delta_{t} } Z_{k}.
$$<h3 id="fokker-planck-fp-equation">Fokker-Planck (FP) equation</h3>
<p>Given a fixed path $\{X_{t}\}_{t=0}^T$, we cannot determine whether it was generated from the SDE. But we can determine it with a distribution $p(x_{t}, t\in[0.T])$. Now we consider a weaker notion: the marginal distribution $\{p_{t}\}_{t=0}^T$ s.t. $X_{t}\sim p_{t}$ for all $t\in[0, T]$.</p>
<p>But how does $p_{t}$ evolve as a function of time $t$? Fokker-Planck equation (also called forward Kolmogorov equaiton) states that for $d = 1$
</p>
$$
\frac{\partial p_{t}}{\partial_{t}} = - \frac{\partial}{\partial_{x}} (f p_{t}) + \frac{g^2}{2} \frac{\partial^2}{\partial x^2} p_{t}
$$<p>
and for multi-dimensional case $d>1$,
</p>
$$
\partial_{t}p_{t} = - \nabla_{x}\cdot(f p_{t}) + \frac{1}{2} \mathrm{Tr}(g^T \nabla^2_{x}p_{t} g).
$$<p>
See proof in [[#^a35e07|FP equation proof]].</p>
<h3 id="ornstein-uhlenbeck-o-u-process">Ornstein-Uhlenbeck (O-U) process</h3>
<p>The O-U process $\{X_{t}\}$ is defined by the following SDE
</p>
$$
dX_{t} = -\beta X_{t} dt + \sigma dW_{t}
$$<p>
Using Ito formula with $f(x,t) = e^{\beta t}x$, we get
</p>
$$
df(X_{t},t) = f_{t}dt + f_{x}dX_{t} + \frac{1}{2}f_{x x} dt = \beta e^{\beta t}X_{t} dt + e^{\beta t} dX_{t} = \sigma e^{\beta t} dW_{t}.
$$<p>
Integrating gives
</p>
$$
X_{t} = e^{-\beta t}X_{0} + \sigma \int_{0}^t e^{-\beta (t-s)} dW_{s}.
$$<p>
This process is Gaussian being a linear combination of the Gaussian process $W_{t}$. Its mean and variance are (using the Ito isometry)
</p>
$$
\begin{align}
\mathbb{E} X_{t} &= e^{-\beta t} X_{0} \\
\mathbb{E}(X_{t} - \mathbb{E}X_{t})^2 &= \sigma^2 \int_{0}^t e^{-2\beta(t-s)} ds =  \frac{\sigma^2}{2\beta} (1- e^{-2\beta t}).
\end{align}
$$<p>
Thus when $\beta > 0$, $X_{t} \to \mathcal{N}\left( 0, \frac{\sigma^2}{2\beta} \right)$ in distribution as $t\to \infty$ if $X_{0} = 0$. With the density function $p_{t}$ of $X_{t}$, we can verify that $p_{t}$ satisfies the FP equation by direct calculations
</p>
$$
\partial_{t} p_{t} = -\partial_{x} (fp_{t}) + \frac{g^2}{2}\partial_{x}^2 p_{t} = 0.
$$<h3 id="ode">ODE</h3>
<p>Recall the definition of ODE
</p>
$$
X_{k+1} = X_{k} + \Delta _{t} f(X_{k}, k\Delta_{t}), \quad k = 0,1,2,\dots,
$$<p>
under $\Delta_{t}\to 0$ with $t = k\Delta_{t}$. This is the forward-time ODE. It tells us how to simulate $X_{T}$ given $X_{0}$.</p>
<p>To simulate the reverse-time ODE is easy
</p>
$$
X_{k-1} = X_{k} - \Delta _{t} f(X_{k}, k\Delta_{t}), \quad k = K,K-1,K-2,\dots 1,
$$<h3 id="sde">SDE</h3>
<p>Recall the definition of SDE
</p>
$$
X_{k+1} = X_{k} + \Delta _{t} f(X_{k}, k\Delta_{t}) + g(k \Delta_{t})\sqrt{ \Delta_{t} } Z_{k}, \quad k = 0,1,2,\dots,
$$<p>
under $\Delta_{t}\to 0$ with $t = k\Delta_{t}$ and $Z_{k}\sim \mathcal{N}(0,I)$ i.i.d.  With initial value $X_{0}$, we can simulate $X_{T}$ by this process.</p>
<p>But how to simulate the reverse-time SDE? Is it
</p>
$$
X_{k-1} = X_{k} - \Delta _{t} f(X_{k}, k\Delta_{t}) - g(k \Delta_{t})\sqrt{ \Delta_{t} } Z_{k}, \quad k = K,K-1,K-2,\dots 1 ?
$$<p>
<!-- raw HTML omitted -->Unfortunately, this does not work! <!-- raw HTML omitted --></p>
<h3 id="andersons-theorem">Anderson&rsquo;s theorem</h3>
<p>The main theorem of <a href="https://www.sciencedirect.com/science/article/pii/0304414982900515">Anderson, 1982</a> states that given a forwrad-time SDE
</p>
$$
d X_{t} = f(X_{t}, t) dt + g(X_{t}, t) d W_{t}, \quad X_{0}\sim p_{0},
$$<p>
the corresponding reverse-time SDE is
</p>
$$
d \bar{X}_{t} = \left(f(\bar{X}_{t}, t) - g^2(\bar{X}_{t}, t)\nabla_{x}\log p_{t}(\bar{X}_{t})\right) dt + g(\bar{X}_{t},t) d\bar{W}_{t}, \quad \bar{X}_{T}\sim p_{T},
$$<p>
where $\bar{W}_{t}$ is the reverse-time Brownian motion, $dt$ is an infinitesimal negative timestep, and $p_{T}$ is the pdf of $\bar{X}_{T}$ defined by the forward-time SDE.</p>
<p>Alternatively, define $\{Y_{t} = \bar{X}_{T-t}\}_{t=0}^T$ via
</p>
$$
dY_{t} = -\left(f(Y_{t}, T-t) - g^2(Y_{t}, T- t)\nabla_{y}\log p_{T-t}(Y_{t})\right) dt + g(Y_{t}, T-t) dW_{t}, \quad Y_{0}\sim p_{T}.
$$<p>
Then we have $X_{t} \overset{\mathcal{D}}{=} \bar{X}_{t} = Y_{T-t}$ for all $t\in[0,T]$. See proof in [[#^7d619b|Anderson&rsquo;s proof]].</p>
<p>Then we simulate the reverse-time SDE by
</p>
$$
\bar{X}_{k-1} = \bar{X}_{k} -  \left(f(\bar{X}_{k}, k\Delta_{t}) - g^2(\bar{X}_{k}, k\Delta_{t})\nabla_{x}\log p_{k\Delta_{t}}(\bar{X}_{k})\right) \Delta_{t} + g({\bar{X}_{k}}, k\Delta_{t}) \sqrt{ \Delta_{t} }Z_{k}, \quad k=K,K-1,\dots,1,
$$<p>
where $Z_{i}\sim \mathcal{N}(0,I)$ i.i.d.</p>
<h2 id="how-to-sample-from-revise-time-process">How to sample from revise-time process?</h2>
<h3 id="reverse-time-sde">Reverse-time SDE</h3>
<p>Given a SDE with forward-time SDE
</p>
$$
d X_{t} = f dt + g dW_{t},\quad X_{0}\sim p_{0},
$$<p>
Anderson&rsquo;s theorem allows us to sample $X_{0}$ from $X_{T}$ by the reverse-time SDE
</p>
$$
d \bar{X}_{t} = (f - g^2\nabla_{x}\log p_{t}) dt + g d \bar{W}_{t}, \quad \bar{X}_{T}\sim p_{T}.
$$<p>
More precisely, using the standard discretization (<a href="https://en.wikipedia.org/wiki/Euler%E2%80%93Maruyama_method">Euler-Maruyama method</a>), we get
</p>
$$
\bar{X}_{k-1} = \bar{X}_{k} -  \left(f - g^2{\color{red}\nabla_{x}\log p_{k\Delta_{t}}(\bar{X}_{k})}\right) \Delta_{t} + g \sqrt{ \Delta_{t} }Z_{k}, \quad k=K,K-1,\dots,1.
$$<p>
However, we do not have access to the score function $\color{red}\nabla_{x} \log p_{t}$.</p>
<h3 id="reverse-time-ode">Reverse-time ODE</h3>
<p>It can also be shown that the process $\{ X_{t} \}_{t=0}^T$ defined by the following reverse-time ODE<br>
</p>
$$
d\bar{X}_{t} = \left( f - \frac{g^2}{2} \nabla_{x}\log p_{t} \right) dt,\quad \bar{X}_{T}\sim p_{T},
$$<p>
possesses the same marginal density functions $\{ p_{t} \}_{t=0}^T$ as the original forward-time SDE.</p>
<p>Let $q_{t}$ be the density of $\bar{X}_{t}$. By the FP equation, we have
</p>
$$
\begin{aligned}
\partial_{t} q_{t} &= -\partial_{x} \left( \left( f - \frac{g^2}{2} \nabla_{x} \log p_{t}  \right)q_{t} \right) \\
    &= -\partial_{x}\left( fq_{t} - \frac{g^2}{2} \frac{\partial_{x}p_{t}}{p_{t}}q_{t} \right) \\
    &= -\partial_{x}(f q_{t}) + \frac{g^2}{2} \frac{q_{t}}{p_{t}} \partial_{x}^2 p_{t}.
\end{aligned}
$$<p>
When $q_{t} = p_{t}$ we get exactly the same FP equation as the forward-time SDE. In other words, we have verified that $q_{t} = p_{t}$ solves the FP equation for $q_{t}$.
<em>Note: This is not a strict proof, as the uniqueness of the solution to the PDE is omitted.</em></p>
<p>However, this method is still not yet implementable since we do not have access to the score function $\color{red}\nabla_{x} \log p_{t}$.</p>
<h2 id="how-to-train-the-model">How to train the model?</h2>
<p>As mentioned above, to sample from the reverse-time SDE, we need to evaluate the score function $\color{red}\nabla_{x} \log p_{t}$.  To solve this issue, we can use the model $s_{\theta}(X_{t}, t)$ to approximate the score function $\nabla_{x} p_{t}(X_{t})$.</p>
<p>A natural choice for the loss function is
</p>
$$
\mathcal{L}(\theta) = \int_{0}^T \lambda_{t} \mathbb{E}_{x_{t}\sim p_{t}} [\|s_{\theta}(x_{t}, t)- \nabla_{x}\log p_{t}(x_{t})\|^2],
$$<p>
where $\lambda_{t}>0$ is a weighting factor. However, we cannot use it since $p_{t}$ is inaccessible. We introduce two methods to estimate $\nabla_{x} \log p_{t}(x_{t})$.</p>
<h3 id="denosing-score-matching-dsm">Denosing score matching (DSM)</h3>
<p>If the drift $f$ and diffusion $g$ are nice, $p_{t|0}(x_{t}|x_{0})$ is known, so DSM use the loss function
</p>
$$
\mathcal{L}_{DSM}(\theta) = \int_{0}^T \lambda_{t} \mathbb{E}_{x_{0}\sim p_{0}}\left[\mathbb{E}_{{x_{t}\sim p_{t|0}}}[\|s_{\theta}(x_{t}, t) - \nabla_{x}\log p_{t|0}(x_{t}|x_{0})\|^2|x_{0} ]\right]dt.
$$<p>
We shall show that $\mathcal{L}_{DSM}$ is equivalent to $\mathcal{L}$.</p>
<p>First, we show that $\nabla_{x}\log p_{t}(x_{t}) = \mathbb{E}_{x_{0}\sim p_{0|t}}[\nabla_{x}\log p_{t|0}(x_{t}|x_{0})|x_{0}]$.
</p>
$$
\begin{aligned}
\nabla_{x}\log p_{t}(x_{t}) &= \frac{\nabla_{x}p_{t}(x_{t})}{p_{t}(x_{t})} \\
    &=\frac{1}{p_{t(x_{t})}} \nabla_{x}\int p_{t|0}(x_{t}|x_{0}) p_{0}(x_{0}) dx_{0} \\
    &=\int  [\nabla_{x}p_{t|0}(x_{t}|x_{0})]  \frac{p_{0}(x_{0})}{p_{t(x_{t})}} dx_{0} \\
    &= \int  [\nabla_{x} \log p_{t|0}(x_{t}|x_{0})]  \frac{p_{0}(x_{0}) p_{t|0}(x_{t}|x_{0})}{p_{t(x_{t})}} dx_{0} \\
    &= \int  [\nabla_{x} \log p_{t|0}(x_{t}|x_{0})]  p_{0|t}(x_{0}|x_{t}) dx_{0} \\
    &= \mathbb{E}_{{x_{0}\sim p_{0|t}}} [\nabla_{x} \log p_{t|0}(x_{t}|x_{0})|x_{t}].
\end{aligned}
$$<p>
Then we can show the result
</p>
$$
\begin{aligned}
\mathcal{L}(\theta) &=\int_{0}^T \lambda_{t} \mathbb{E}_{x_{t}\sim p_{t}} [\|s_{\theta}(x_{t}, t)- \nabla_{x}\log p_{t}(x_{t})\|^2] dt\\
    &= \int_{0}^T \lambda_{t} \mathbb{E}_{x_{t}\sim p_{t}} [\|s_{\theta}(x_{t}, t)\|^2-2 \langle s_{\theta}(x_{t},t), \nabla_{x}\log p_{t}(x_{t}) \rangle ] dt + C \\
    &= \int_{0}^T \lambda_{t} \mathbb{E}_{x_{t}\sim p_{t}} [\|s_{\theta}(x_{t}, t)\|^2-2 \langle s_{\theta}(x_{t},t), \mathbb{E}_{{x_{0}\sim p_{0|t}}} [\nabla_{x} \log p_{t|0}(x_{t}|x_{0})|x_{t}] \rangle ] dt + C \\
    &= \int_{0}^T \lambda_{t} \mathbb{E}_{x_{t}\sim p_{t}}\mathbb{E}_{{x_{0}\sim p_{0|t}}} [\|s_{\theta}(x_{t}, t)\|^2-2 \langle s_{\theta}(x_{t},t),  \nabla_{x} \log p_{t|0}(x_{t}|x_{0}) \rangle | x_{t}] dt + C \\
    &= \int_{0}^T \lambda_{t} \mathbb{E}_{(x_{0},x_{t})\sim p_{0,t}} [\|s_{\theta}(x_{t}, t)\|^2-2 \langle s_{\theta}(x_{t},t),  \nabla_{x} \log p_{t|0}(x_{t}|x_{0}) \rangle] dt + C \\
    &= \mathcal{L}_{DSM}(\theta).
\end{aligned}
$$<h3 id="sliced-score-matching-ssm">Sliced score matching (SSM)</h3>
<p>What if the conditional distribution $p_{t|0}(x_{t|x_{0}})$ is unknown? The loss of SSM is
</p>
$$
\mathcal{L}_{SSM}(\theta) = \int_{0}^T \lambda_{t}\mathbb{E}_{x_{t}\sim p_{t}} \left[ \|s_{\theta}(x_{t}, t)\|^2 + 2\mathbb{E}_{v}\left[ \frac{d}{d h} v^T s_{\theta}(x_{t}+h v) |_{h=0} \right] \right] dt + C,
$$<p>
where $v\in \mathbb{R}^n$ is a random vector s.t.
</p>
$$
\mathbb{E}_{v}[v_{i} v_{j}] = \delta_{i,j} = 1_{i=j}.
$$<p>Before proving that $\mathcal{L}_{SSM}$ is equivalent to $\mathcal{L}$, we introduce the Hutchinson&rsquo;s trace estimator that will be useful in later proof.
Let $A\in \mathbb{R}^{n\times n}$. Then
</p>
$$
\begin{aligned}
\mathbb{E}_{v}[v^T A v] &= \mathbb{E}_{v}[\mathrm{Tr}(v^T A v)] \\ 
    &= \mathbb{E}_{v}[\mathrm{Tr}(A v v^T)] \\ 
    &= \mathrm{Tr}(\mathbb{E}_{v}[ A vv^T]) \\
    &= \mathrm{Tr}(A\mathbb{E}_{v}[  vv^T]) \\
    &= \mathrm{A}.
\end{aligned}
$$<p>In the proof of DSM, we see that the expansion of $\mathcal{L}(\theta)$ contains term $\langle s_{\theta}(x_{t},t), \nabla_{x}\log p_{t}(x_{t}) \rangle$.
![[Screenshot from 2024-10-23 21-30-13.png]]
where the <a href="https://math.libretexts.org/Bookshelves/Calculus/Vector_Calculus_(Corral)/04%3A_Line_and_Surface_Integrals/4.06%3A_Gradient_Divergence_Curl_and_Laplacian">divergence operator</a> takes a vector field and produces a scalar value which is defined by
</p>
$$
\nabla \cdot F = \frac{\partial F_{x}}{\partial x} + \frac{\partial F_{y}}{\partial y} + \frac{\partial F_{z}}{\partial z}. \quad F = (F_{x}, F_{y}, F_{z}). 
$$<p>
And $D_{x}  F(x)$ should be the partial differentiation matrix $\left[ \frac{\partial}{\partial x_{1}} F, \frac{\partial}{\partial x_{2}} F, \dots, \frac{\partial}{\partial x_{n}} F \right]$ (I guess).</p>
<p>Overall,</p>
<ul>
<li>SSM is more broadly applicable than DSM as it does not require the conditional score $\nabla_{x} p_{t|0}(x_{t}|x_{0})$.</li>
<li>SSM requires mixed (2nd-order) derivatives, one for $h$ and another for $\theta$.</li>
</ul>
<h2 id="examples-with-o-u-process">Examples with O-U process</h2>
<p>Recall the definition of Ornstein-Uhlenbeck process
</p>
$$
dX_{t} = -\beta X_{t} dt + \sigma dW_{t}
$$<p>
With conditional distribution $X_{t}|X_{0} \sim \mathcal{N}(e^{-\beta t} X_{0}, \frac{\sigma^2}{2\beta} (1- e^{-2\beta t})I)$. In this section, we will discuss two types of O-U processes: variance-exploding (VE) SDE and variance-preserving (VP) SDE as well as their training losses and sampling methods.</p>
<h3 id="ve-sde">VE SDE</h3>
<p>Let $\sigma_{t}$ be a non-decreasing function of $t$, the VE SDE is defined by
</p>
$$
dX_{t} = \sqrt{ \frac{d(\sigma_{t}^2)}{dt} } dW_{t}.
$$<p>
It can be easily checked that $X_{t}|X_{0} \sim \mathcal{N}(X_{0}, \sigma_{t}^2)$, which means that variance explodes.</p>
<h3 id="vp-sde">VP SDE</h3>
$$
d X_{t} = - \frac{\beta_{t}}{2} X_{t} dt + \sqrt{ \beta_{t} } dW_{t}.
$$<p>
By (5.50) and (5.51) of <a href="https://users.aalto.fi/~asolin/sde-book/sde-book.pdf">Applied Stochastic Differential Equation</a> we can get the variance of $\{X_{t}\}_{t}$ is governed by the ODE
</p>
$$
\frac{d\Sigma_{t}}{dt} = \beta_{t}(I + \Sigma_{t}). 
$$<p>
Solving this ODE, we obtain
</p>
$$
\Sigma_{t} = I + \exp\left( -\int_{0}^t \beta_{s} ds \right)(\Sigma_{0}- I),
$$<p>
from which it is clear that the variance of VP SDE is bounded by $\Sigma_{0}$ from above. Moreover, if $\Sigma_{0} = I$, we have $\Sigma_{t} = I$, so its name is called variance-preserving. And the conditional mean of $X_{t}|X_{0}$ is $\exp\left( -\frac{1}{2}\int_{0}^t\beta_{s} ds\right) X_{0}$ and conditional variance is $I- \exp\left( -\int_{0}^t \beta_{s} ds\right)I$</p>
<h3 id="training-with-o-u">Training with O-U</h3>
<p>Generally, for both VE and VP SDE, we write its conditional distribution $X_{t}|X_{0}$ in the form of
</p>
$$
X_{t}|X_{0} \sim \mathcal{N}(\gamma_{t}X_{0}, \sigma_{t}^2 I).
$$<p>with
</p>
$$
\text{VE SDE:}
\begin{cases}
\gamma_{t} = 1   \\
\sigma_{t}^2 = \sigma_{t}^2
\end{cases}
,\quad
\text{VP SDE:}
\begin{cases}
\gamma_{t} = \exp\left( -\frac{1}{2}\int_{0}^t\beta_{s} ds\right)   \\
\sigma_{t}^2 = 1- \exp\left( -\int_{0}^t \beta_{s} ds\right)
\end{cases}
$$<p>
So we can write $X_{t}$ as $X_{t} = \gamma_{t} X_{0} + \sigma_{t}  \epsilon$, where $\epsilon$ is a standard Gaussian r.v.,thus the score function simplifies to
</p>
$$
\nabla_{x} \log p_{t|0}(x_{t}) = - \frac{X_{t}- \gamma_{t}X_{0}}{\sigma^2} = - \frac{\epsilon}{\sigma_{t}}.
$$<p>
Define a scaled score network
</p>
$$
\epsilon_{\theta}(x_{t}, t) = -\sigma_{t}s_{\theta}(x_{t},t),
$$<p>
then the DSM loss becomes
</p>
$$
\begin{aligned}
\mathcal{L}_{DSM}(\theta) &= \int_{0}^T \lambda_{t} \mathbb{E}_{x_{0}\sim p_{0}}\left[\mathbb{E}_{{x_{t}\sim p_{t|0}}}[\|s_{\theta}(x_{t}, t) - \nabla_{x}\log p_{t|0}(x_{t}|x_{0})\|^2|x_{0} ]\right]dt \\
    &= \int_{0}^T \frac{\lambda_{t}}{\sigma_{t}^2} \mathbb{E}_{x_{0}\sim p_{0}}\left[\mathbb{E}_{{\epsilon \sim \mathcal{N}(0,1)}}[\|\epsilon_{\theta}(\gamma_{t}x_{0} + \sigma_{t}\epsilon, t) - \epsilon\|^2|x_{0} ]\right]dt \\
    &= T \mathbb{E}_{x_{0}\sim p_{0}, t\sim U[0, T], \epsilon\sim \mathcal{N}(0,1)} \left[ \frac{\lambda_{t}}{\sigma_{t}^2} \|\epsilon_{\theta}(\gamma_{t}x_{0} + \sigma_{t}\epsilon, t) - \epsilon\|^2  \right].
\end{aligned}
$$<p>
Note that when $t=0$, $\sigma_{t} = 0$ and the loss blows up. There several ways to deal with it</p>
<ul>
<li>Do the integral on the interval $[\delta, T]$ with a small $\delta>0$.</li>
<li>Choose appropriate $\lambda_{t}$ s.t. $\frac{\lambda_{t}}{\sigma_{t}^2}<C$ for all $t$.</li>
<li>Use importance sampling to reduce the vriance.</li>
</ul>
<p>Training O-U with DSM loss
![[Pasted image 20241024004609.png|500]]</p>
<p>Training O-U with SSM loss
![[Pasted image 20241024004724.png|550]]</p>
<h3 id="sampling-with-o-u">Sampling with O-U</h3>
<p><strong>SDE sampling</strong>
Once $s_{\theta}$ has been trained, we can generate new samples with the approximated reverse-time SDE
</p>
$$
d \bar{X}_{t} = \left( f - g^2 s_{\theta} \right) dt + g dW_{t}, \quad \bar{X}_{T}\sim\mathcal{N}(0, \sigma_{T}^2 I).
$$<p>
Sampling VE SDE with reverse SDE
Start from $X_{N}\sim \mathcal{N}(0,\sigma_{T}^2I)$
For $i = N-1$ to $0$ do
$X_{i} = X_{i+1} + (\sigma_{i+1}^2 - \sigma_{i}^2) s_{\theta}(x_{i+1}, t_{i+1}) + \sqrt{ \sigma_{i+1}^2 - \sigma_{i}^2 }Z$.
Return $X_{0}$.</p>
<p>Sampling VP SDE with reverse SDE
Start from $X_{N}\sim \mathcal{N}(0,\sigma_{T}^2I)$
For $i = N-1$ to $0$ do
$X_{i} = X_{i+1} + \frac{1}{2}(\beta_{i+1} - \beta_{i}) + (\beta_{i+1} - \beta_{i}) s_{\theta}(x_{i+1}, t_{i+1}) + (\sqrt{ \beta_{i+1}} - \sqrt{ \beta_{i} })Z$.
Return $X_{0}$.</p>
<p><strong>ODE sampling</strong>
We can also use the approximated reverse-time ODE for sampling
</p>
$$
d\bar{X}_{t} = \left( f - \frac{g^2}{2} s_{\theta} \right)dt, \quad \bar{X}_{T}\sim\mathcal{N}(0, \sigma_{T}^2 I).
$$<p>
Sampling VE SDE with reverse ODE
Start from $X_{N}\sim \mathcal{N}(0,\sigma_{T}^2I)$
For $i = N-1$ to $0$ do
$X_{i} = X_{i+1} + \frac{\sigma_{i+1}^2 - \sigma_{i}^2}{2} s_{\theta}(x_{i+1}, t_{i+1})$.
Return $X_{0}$.</p>
<p>Sampling VP SDE with reverse ODE
Start from $X_{N}\sim \mathcal{N}(0,\sigma_{T}^2I)$
For $i = N-1$ to $0$ do
$X_{i} = X_{i+1} + \frac{1}{2}(\beta_{i+1} - \beta_{i}) + \frac{\beta_{i+1} - \beta_{i}}{2} s_{\theta}(x_{i+1}, t_{i+1})$.
Return $X_{0}$.</p>
<h2 id="supplementary">Supplementary</h2>
<h3 id="integration-by-parts">Integration by parts</h3>
<p>Assume $\psi$ and $f$ are sufficiently smooth and decay sufficiently quickly as $|x|\to \infty$. Then
</p>
$$
\int_{\mathbb{R}} \psi(x)f^\prime(x) dx = -\int_{\mathbb{R}} \psi(x) f(x) dx.
$$<p>
For higher-dimensional case $\psi: \mathbb{R}^d \to \mathbb{R}^d, f:\mathbb{R}^d \to \mathbb{R}$, we have
</p>
$$
\int_{\mathbb{R}^d} \psi(x) \cdot \nabla f(x) dx = - \int_{\mathbb{R}^d} (\nabla \cdot \psi(x)) f(x) dx. 
$$<h3 id="proof-of-fokker-planck-equation">Proof of Fokker-Planck equation</h3>
<p>^a35e07</p>
<p>We prove FP equation for $d=1$. Let $\psi$ be a continuous function, we have
</p>
$$
\begin{aligned}
\partial_{t} \mathbb{E}_{x\sim p_{t}}[\psi(x)] &\approx \frac{1}{\epsilon}(\mathbb{E}_{x\sim p_{t+\epsilon}}[\psi(x)] - \mathbb{E}_{x\sim p_{t}}[\psi(x)])\\
    &\approx \frac{1}{\epsilon} \mathbb{E}_{x\sim p_{t}, z\sim\mathcal{N}(0,I)}[\psi(x+\epsilon f + \sqrt{ \epsilon }) - \psi(x)] \\
    &\approx \frac{1}{\epsilon} \mathbb{E}_{x\sim p_{t}, z\sim\mathcal{N}(0,I)}\left[ \psi(x) + \epsilon \psi^\prime(x)f + \sqrt{ \epsilon }\psi^\prime(x) g z + \frac{1}{2} \epsilon\psi^{\prime\prime}(x)g^2z^2 + \mathcal{O}(\epsilon^{3 / 2}) - \psi(x) \right] ;\quad\text{taylor expansion}\\
    &\approx   \mathbb{E}_{x\sim p_{t}}\left[  \psi^\prime(x)f + \frac{1}{2}\psi^{\prime\prime}(x)g^2 \right] \\
\end{aligned}
$$<p>
It follows that
</p>
$$
\begin{align}
\partial_{t} \int \psi(x) p_{t}(x) dx &= \int \psi^\prime(x)f p_{t}(x)dx + \frac{1}{2} \int \psi^{\prime\prime}(x)g^2(t) p_{t}(x) dx \\ 
\int \psi(x) \partial_{t}p_{t}(x) dx &= \int \psi(x)(-\partial_{x}(fp_{t})) dx + \frac{1}{2}\int \psi(x) g^2 \partial_{x}^2(p_{t}) dx ;\quad \text{integration by parts}
\end{align}
$$<p>
Therefore,
</p>
$$
\partial_{t}p_{t} = -\partial_{x} fp_{t} + \frac{g^2}{2}\partial_{x}^2 (p_{t}).
$$<h3 id="proof-andersons-theorem">Proof Anderson&rsquo;s theorem</h3>
<p>^7d619b</p>
<p>For the forward-time SDE, we have the FP equation
</p>
$$
\partial_{t}p_{t} = -\partial_{x}(fp_{t}) + \frac{g^2}{2}\partial_{x}^2 p_{t}.
$$<p>Let $\{q_{t}\}_{t=0}^T$ be the marginal densities of the SDE
</p>
$$
dY_{t} = -(f(Y_{t}, T-t) -g^2(T-t)\partial_{y}\log p_{T-t}(Y_{t}) )dt + g(T-t)dW_{t},\quad  Y_{0} \sim p_{T}.
$$<p>
Then $\{ q_{t} \}$ satisfies the FP equation
</p>
$$
\partial_{t}q_{t} = \partial_{y}\left((f(y,T-t)-g^2(T-t)\partial_{y}\log p_{T-t}(y)) q_{t}(y)\right)  + \frac{g^2(T-t)}{2}\partial_{y}^2(q_{t}(y)).
$$<p>
Let $\{ \bar{p}_{t} \}$ be the marginal densities of the reverse-time SDE given by Anderson&rsquo;s theorem
</p>
$$
d \bar{X}_{t} = \left(f(\bar{X}_{t}, t) - g^2(\bar{X}_{t}, t)\nabla_{x}\log p_{t}(\bar{X}_{t})\right) dt + g(\bar{X}_{t},t) d\bar{W}_{t}, \quad \bar{X}_{T}\sim p_{T},
$$<p>
Since $\bar{p}_{t} = q_{T-t}$, the densities $\{ \bar{p}_{t} \}$ satisfies
</p>
$$
\partial_{t}\bar{p}_{t} = -\partial_{x}\left((f(x,t)-g^2(t)\partial_{x}\log p_{t}(x)) \bar{p}_{t}(y)\right)  - \frac{g^2(t)}{2}\partial_{x}^2(\bar{p}_{t}(x)).
$$<p>
If we substitute $\bar{p}_{t}$ with $p_{t}$ in the last FP equation, we get
</p>
$$
\partial_{t}p_{t} = -\partial_{x}\left((f(x,t)-g^2(t)\partial_{x}\log p_{t}(x)) p_{t}(y)\right)  - \frac{g^2(t)}{2}\partial_{x}^2(p_{t}(x)) = -\partial_{x}(fp_{t}) + \frac{g^2}{2}\partial_{x}^2(p_{t}).
$$<p>
In other words, we have verified that $\bar{p}_{t}= p_{t}$ solves the FP equation for $\bar{p}_{t}$, which proves $\bar{p}_{t}= p_{t}$ provided that the solution to the PDE is unique.</p>

	</article>
</main>
	

	

	



  <footer class="row row-mob al-c-mob col-rev-mob sm-2-mob  jc-bt mtb-2">
  <p>
    © Copyright notice |
    <a href="https://github.com/mivinci/hugo-theme-minima" target="_blank" rel="noopener noreferrer">Minima</a> theme on
    <a href="https://gohugo.io" target="_blank" rel="noopener noreferrer">Hugo</a>
  </p>
  <p class="row gap-0_5">
    
      <a class="icon" href="https://github.com/wangchxx" title="github">
      
        <svg fill="#63636f" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
      
      </a>
    
  </p>
</footer>
</body>
</html>