<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta property="og:url" content="https://wangchxx.github.io/posts/diffusion_1_ddmp/">
  <meta property="og:site_name" content="My Math Notes">
  <meta property="og:title" content="DL | Diffusion Models 1 - DDMP">
  <meta property="og:description" content="What? A diffusion probabilistic model is a parameterized Markov chain that gradually adds noise to the data and then learn to reverse the diffusion process to generate data samples from noise.
Why? Compared with other AI tasks, image generation is harder, since it does not have a standard answer. To solve this issue, GAN and VAE are propsed.
GAN uses another model (discriminator) to decide the quality of generated images.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-10-21T04:34:23+08:00">
    <meta property="article:modified_time" content="2024-10-21T04:34:23+08:00">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/diffusion_4_conditional_df/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/diffusion_3_from_a_unified_prospective/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/diffusion_2_preliminary_sde/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/diffusion_4_conditional_df/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/diffusion_2_preliminary_sde/">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="DL | Diffusion Models 1 - DDMP">
  <meta name="twitter:description" content="What? A diffusion probabilistic model is a parameterized Markov chain that gradually adds noise to the data and then learn to reverse the diffusion process to generate data samples from noise.
Why? Compared with other AI tasks, image generation is harder, since it does not have a standard answer. To solve this issue, GAN and VAE are propsed.
GAN uses another model (discriminator) to decide the quality of generated images.">

  
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#262d33">
  <title>
    
    My Math Notes - DL | Diffusion Models 1 - DDMP
    
  </title>
  
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;600;700&display=swap"
    rel="stylesheet">
  <link rel="stylesheet" href="https://unpkg.com/normalize.css">
  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="/css/md.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="/css/syntax.css" />
  <script src="/js/main.js"></script>
</head>
<script>
  try {
    if (!('theme' in localStorage)) {
      localStorage.theme = window.matchMedia('(prefer-color-scheme: dark)').matches ? 'dark' : 'light';
    }
    document.querySelector('html').classList.add(localStorage.theme);
  } catch (e) {
    console.error(e);
  }
</script>
<body>
  <header>
  <h1 class="row gap-1">
    <div id="theme-switcher" class="btn lg-1"></div>
    My Math Notes
  </h1>
  <nav class="row gap-1">
  
    <a href="/">Home</a>
  
    <a href="/categories">Categories</a>
  
    <a href="/series">Series</a>
  
    <a href="/about">About</a>
  
  </nav>
  <hr>
</header>
  
  
<main>
	<h1>DL | Diffusion Models 1 - DDMP</h1>
	<div class="sm-1 mtb-1">
		Posted at &mdash; Oct 21, 2024
		
	</div>
	<p></p>
	<article class="md">
		<h2 id="what">What?</h2>
<p>A diffusion probabilistic model is a parameterized Markov chain that gradually adds noise to the data and then learn to reverse the diffusion process to generate data samples from noise.</p>
<p><img src="ddmp_graph.png" alt=""></p>
<h2 id="why">Why?</h2>
<p>Compared with other AI tasks, image generation is harder, since it does not have a standard answer. To solve this issue, GAN and VAE are propsed.</p>
<ul>
<li>
<p>GAN uses another model (discriminator) to decide the quality of generated images.</p>
</li>
<li>
<p>VAE learns how to compress an image into a latent vector $z$ and then learns how to reconstruct the image from $z$. Then for each generated image, there exists a standard answer.GAN is able to generate images with good quality, but its training is unstable. Is there a model that is as powerful as GAN but simpler for training?</p>
</li>
</ul>
<h2 id="how">How?</h2>
<h3 id="forward-process">Forward process</h3>
<p>Given a data point sampled from a real data distribution $x_0\sim q(x)$, the forward process is defined by</p>
$$
q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I),\quad q(x_{1:T}|x_0) = \prod_{t=1}^T q(x_t|x_{t-1}).
$$<p>A nice property of the forward process is that it admits sampling $x_t$ at an arbitrary timestep $t$ in closed form:</p>
$$
\begin{equation}
q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_{t})I),
\end{equation}
$$<p>where $\alpha_t := 1-\beta_t$ and $\bar{\alpha}_t:= \prod_{s=0}^t \alpha_s$. To make sure that $q(x_t|x_0)$ is convergent, we need $\bar{\alpha}_t \to C$ as $t\to\infty$. It can be guaranteed by setting $\beta_t\in (0,1)$ as an increasing sequence and therefore $\bar{\alpha}_1>\cdots>\bar{\alpha}_T$.</p>
<h3 id="reverse-process">Reverse process</h3>
<p>If we can reverse the above process and sample from $q(x_{t-1}|x_t)$, we will be able to recreate the true sample from a Gaussian noise input $x_T\sim N(0, I)$. Note that if $\beta_t$ is small enough, $q(x_{t-1}|x_t)$ will also be Gaussian (==We will prove it in Diffusion-model-3==). However, we cannot easily estimate $q(x_{x-1}|x_t)$ because it needs to use the entire dataset. So we try to estimate it through a model $p_\theta(x_{t-1}|x_t)$. We set</p>
$$
p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t));\quad p_\theta(x_{0:T}) = p(x_T)\prod_{t=1}^T p_\theta(x_{t-1}|x_t).
$$<p>But how to define the loss function? Fitting the mean and variance of added noise from the step $q(x_{t-1}|x_t)$ is unrealistic.</p>
<p><strong>Conditional posterior</strong></p>
<p>It is noteworthy that the reverse conditional probability is tractable when conditioned on $x_0$:</p>
$$
q(x_{t-1}|x_t, x_0) = \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t, x_0), \tilde{\beta}_t I),
$$<p>where</p>
$$
\begin{equation}
\tilde{\mu}_t(x_t, x_0) := \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t +\frac{\bar{\alpha}_{t-1}\beta_t}{1-\bar{\alpha}_t}x_0;\quad \tilde{\beta}_t:= \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t.
\end{equation}
$$<p>See proof in <a href="##supplementary">supplementary section</a>.</p>
<p><strong>Loss function</strong></p>
<p>Then we can use the variational lower bound to optimize the negative log-likehood.</p>
$$
\begin{aligned}
-\log p_{\theta}(x_{0}) &\leq -\log p_{\theta}(x_{0}) + D_{KL}(q(x_{1:T}|x_{0})\|p_{\theta}(x_{1:T}|x_{0})) \\
    &= -\log p_{\theta}(x_{0}) + \mathbb{E}_{q}\left[ \log  \frac{q(x_{1:T}|x_{0})}{p_{\theta}(x_{0:T})/p_{\theta}(x_{0})} \right]\\
    &=-\log p_{\theta}(x_{0})  + \mathbb{E}_{q}\left[ \log  \frac{q(x_{1:T}|x_{0})}{p_{\theta}(x_{0:T})} +\log p_{\theta}(x_{0}) \right]\\
    &=\mathbb{E}_{q}\left[ \log  \frac{q(x_{1:T}|x_{0})}{p_{\theta}(x_{0:T})}  \right] := L_{VLB}
\end{aligned}
$$<p>Then we rewrite $L_{VLB}$ in terms of $q(x_{t-1}|x_{t},x_{0})$:</p>
$$
\begin{align}
L_{VLB} &= L_{T} + L_{T-1} + \cdots + L_{0}, \\ 
    \text{where} \; L_{T} & = D_{KL}(q(x_{T}|x_{0})\|p_{\theta}(x_{T})),  \\
	L_{t} & = D_{KL}(q(x_{t}|x_{t+1},x_{0})\| p_{\theta}(x_{t}|x_{t+1})),\; 1\leq t\leq T-1, \\
    L_{0} & = -\log p_{\theta}(x_{0}|x_{1}).
\end{align}
$$<p>See proof in <a href="##supplementary">supplementary section</a>. $L_{T}$ is constant because $x_{T}$ is a Gaussian noise. $L_{0}$ is modeled using a separate decoder. $L_{t}$ compares two Gaussian distributions and therefore can be computed in closed form.</p>
<p><strong>Simplification of $L_{t}$</strong></p>
<p>It is known that the KL divergence between two Gaussian distributions $\mathcal{N}(\mu_{p}, \Sigma_{p})$ and $\mathcal{N}(\mu_{q},\Sigma_{q})$ is</p>
$$
D_{KL}(p\|q) = \frac{1}{2} \left[ \log \frac{|\Sigma_{q}|}{|\Sigma_{p}|} + (\mu_{p} -\mu_{q})^T \Sigma_{q}^{-1}(\mu_{q} - \mu_{p}) + Tr(\Sigma_{q}^{-1}\Sigma_{p}) \right] + C.
$$<p>Since the authors set $\Sigma_{\theta}(x_{t}, t) = \sigma_{t}^2 I$, where $\sigma_{t}$ is not learnable. We can simplify $L_{t}$ as</p>
$$
\begin{equation}
L_{t} = \frac{1}{2\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \|\tilde{\mu}(x_{t},x_{0}) - \mu_{\theta}(x_{t},t)\|^2 + C.
\end{equation}
$$<p>We can expand the above loss by reparameterizing $q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha})I)$ as $x_{t}(x_{0}, \epsilon) = \sqrt{ \bar{\alpha}_{t} }x_{0} + \sqrt{ 1-\bar{\alpha}_{t} }\epsilon$ for $\epsilon\sim\mathcal{N}(0, I)$:</p>
$$
\begin{aligned}
\tilde{\mu}_{t} &= \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t +\frac{\bar{\alpha}_{t-1}\beta_t}{1-\bar{\alpha}_t}{\color{green}x_0} \\
    &=\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t +\frac{\bar{\alpha}_{t-1}\beta_t}{1-\bar{\alpha}_t} {\color{green} \frac{1}{\sqrt{ \bar{\alpha}_{t} }}(x_{t} - \sqrt{ 1-\bar{\alpha}_{t} }\epsilon)} \\
    &=\frac{1}{\sqrt{ \alpha_{t} }}\left( x_{t} - \frac{1-\alpha_{t}}{\sqrt{ 1-\bar{\alpha}_{t} }} \epsilon \right)
\end{aligned}
$$<p>Instead of approximating $\tilde{\mu}_{t}$ by $\mu_{\theta}$ directly, we can also estimate the noise $\epsilon$ using $\epsilon_{\theta}(x_{t}, t)$ (like ResNet) by setting</p>
$$
\begin{align}
\mu_{\theta}(x_{t}, t) = \tilde{\mu}_{t}(x_{t}, t) &= \frac{1}{\sqrt{ \alpha_{t} }}\left( x_{t} - \frac{1-\alpha_{t}}{\sqrt{ 1-\bar{\alpha}_{t} }} \epsilon_{\theta}(x_{t}, t) \right).\\
\end{align}
$$<p>Therefore, we can rewrite the loss function $L_{t}$ as</p>
$$
\begin{align}
L_{t} &= \mathbb{E}_{x_{0}, \epsilon} \left[\frac{(1-\alpha_{t})^2}{2 \alpha_{t}(1-\bar{\alpha}_{t})\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \| \epsilon - \epsilon_{\theta}(x_{t},t)\|^2 \right] + C \\
 & = \mathbb{E}_{x_{0}, \epsilon} \left[\frac{(1-\alpha_{t})^2}{2 \alpha_{t}(1-\bar{\alpha}_{t})\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \| \epsilon - \epsilon_{\theta}({\color{green}\sqrt{ \bar{\alpha}_{t} }x_{0} + \sqrt{ 1-\bar{\alpha}_{t} }\epsilon},t)\|^2 \right] + C 
\end{align}
$$<h3 id="algorithm">Algorithm</h3>
<p>Empirically, the authors found that training the diffusion model works better with a simplified objective that ignores the weighting term:</p>
$$
L_{t}^{simple} = \mathbb{E}_{x_{0}, \epsilon} \left[\| \epsilon - \epsilon_{\theta}(\sqrt{ \bar{\alpha}_{t} }x_{0} + \sqrt{ 1-\bar{\alpha}_{t} }\epsilon,t)\|^2 \right].
$$<p><img src="ddmp_algorithm.png" alt=""></p>
<h2 id="supplementary">Supplementary</h2>
<h3 id="langevin-dynamics">Langevin dynamics</h3>
<p>Langevin dynamics is a concept from physics, developed for statistically modeling molecular systems. Combined with stochastic gradient descent, stochastic gradient Langevin dynamics can produce samples from a probability distribution $p(x)$ using only gradients $\nabla_{x} \log p(x)$ in a Markov chain of updates:
</p>
$$
x_{t} = x_{t-1} + \frac{\delta}{2}\nabla_{x} \log p(x_{t-1}) + \sqrt{ \delta }\epsilon_{t},\quad \epsilon_{t}\sim \mathcal{N}(0,I),
$$<p>
where $\delta$ is the step size. When $T\to \infty$, $\epsilon\to 0$, $x_{T}\sim p(x)$.</p>
<p>We can see that we only need to know the gradients $\nabla_{x} \log p(x)$ (score function) to sample data from $p(x)$.</p>
<p>Let $(x, \tilde{x})$ be a pair of clean and corrupted data. The idea of denoising score matching is to estimate the score function of the noise-corrupted data distribution $q_{\sigma}(\tilde{x})$, and the objective is shown to be equivalent to
</p>
$$
\mathbb{E}_{q_{\sigma}(\tilde{x}|x) p(x)} [\|s(\tilde{x}, \theta) - \nabla_{x}\log q_{\sigma}(\tilde{x}|x)\|_{2}^2].
$$<p>
The schedule of increasing noise levels resembles the forward diffusion process $q(x_{t}|x_{0})$.  If we use the diffusion process annotation, the score approximates $s_{\theta}(x_{t}, t)\approx \nabla_{x_{t}}q(x_{t}|x_{0})$. Note that given a Gaussian distribution $x\sim\mathcal{N}(\mu,\sigma^2I)$, its score function is
</p>
$$
\nabla_{x} \log p(x) = - \frac{x-\mu}{\sigma^2} = - \frac{\epsilon}{\sigma}, \quad \epsilon\sim \mathcal{N}(0,I).
$$<p>
Recall that $q(x_{t}|x_{0})\sim \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha})I)$ and therefore,
</p>
$$
s_{\theta}(x_{t},t) \approx \nabla_{x_{t}}\log q(x_{t}) = \mathbb{E}_{q(x_{0})}[\nabla_{x_{t}}q(x_{t}|x_{0})] = - \frac{\epsilon}{\sqrt{ 1-\bar{\alpha}_{t} }},
$$<p>
which is equivalent to the loss function $L_{t}$ in DDPM.</p>
<h3 id="proof-of-conditional-posterior-px_t-1x_tx_0">Proof of conditional posterior $p(x_{t-1}|x_t,x_0)$</h3>
<p>Using Bayes&rsquo; rule, we have
</p>
$$
\begin{aligned}
q(x_{t-1}|x_{t}, x_{0}) &= q(x_{t}|x_{t}, x_{0}) \frac{q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})}  \\
 & \propto \exp\left( -\frac{1}{2}\left(  \frac{(x_{t}- \sqrt{ \alpha_{t}  }x_{t-1})^2}{\beta_{t}}  +  \frac{(x_{t-1}- \sqrt{ \bar{\alpha}_{t-1}  }x_{0})^2}{1-\bar{\alpha}_{t-1}} - \frac{(x_{t}- \sqrt{ \bar{\alpha}_{t}  }x_{0})^2}{1-\bar{\alpha}_{t}}   \right) \right) \\
 & \propto  \exp\left( -\frac{1}{2}\left(  \frac{- 2\sqrt{ \alpha_{t}} x_{t}{\color{blue}x_{t-1}} + \alpha_{t} \color{red}x_{t-1}^2}{\beta_{t}}  +  \frac{ {\color{red}x_{t-1}^2}- 2\sqrt{ \bar{\alpha}_{t-1}  }x_{0}{\color{blue}{x_{t-1}}}  }{1-\bar{\alpha}_{t-1}} \right) \right) \\ 
 & \exp\left( -\frac{1}{2} \left(  {\color{red} \left( \frac{\alpha_{t}}{\beta_{t}} + \frac{1}{1-\bar{\alpha}_{t-1}} \right)x_{t-1}^2} - {\color{blue}\left( \frac{2\sqrt{ \alpha_{t} }}{\beta_{t}} x_{t} +  \frac{2\sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_{t-1}}  \right) x_{t-1}} \right) \right)
\end{aligned}
$$<p>
By completing the square, we have
</p>
$$
\begin{aligned}
\tilde{\beta}_{t} &= \frac{1}{\left( \frac{\alpha_{t}}{\beta_{t}} + \frac{1}{1-\bar{\alpha}_{t-1}} \right)} = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\beta_{t},  \\
\tilde{\mu}_{t} &= \left( \frac{2\sqrt{ \alpha_{t} }}{\beta_{t}} x_{t} +  \frac{2\sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_{t-1}}  \right) / \tilde{\beta}_{t} = \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t +\frac{\bar{\alpha}_{t-1}\beta_t}{1-\bar{\alpha}_t}x_0.
\end{aligned}
$$<h3 id="proof-of-loss-function-l_vlb">Proof of Loss function $L_{VLB}$</h3>
$$
\begin{aligned}
L_{VLB} &= \mathbb{E}_{q}\left[ \log  \frac{q(x_{1:T}|x_{0})}{p_{\theta}(x_{0:T})}  \right]  \\
    &= \mathbb{E}_{q}\left[ \log  \frac{\prod_{t=1}^Tq(x_{t}|x_{t-1})}{p_{\theta}(x_{T})\prod_{t=1}^T p_{\theta}(x_{t-1}|x_{t})}  \right]  \\
    &= \mathbb{E}_{q}\left[  -\log p_{\theta}(x_{T}) + \sum_{t=2}^T \log  \frac{ q(x_{t}|x_{t-1})}{p_{\theta}(x_{t-1}|x_{t})} + \log \frac{q(x_{1}|x_{0})}{p_{\theta}(x_{0}|x_{1})}  \right] \\
    &= \mathbb{E}_{q}\left[  -\log p_{\theta}(x_{T}) + \sum_{t=2}^T \log  \frac{ q(x_{t}|x_{t-1}, {\color{red}{x_{0}}})}{p_{\theta}(x_{t-1}|x_{t})} + \log \frac{q(x_{1}|x_{0})}{p_{\theta}(x_{0}|x_{1})}  \right] \\
    &= \mathbb{E}_{q}\left[  -\log p_{\theta}(x_{T}) + \sum_{t=2}^T \log  \frac{ q(x_{t-1}|x_{t}, x_{0})}{p_{\theta}(x_{t-1}|x_{t})} \cdot \frac{q(x_{t}|x_{0})}{q(x_{t-1}|x_{0})} + \log \frac{q(x_{1}|x_{0})}{p_{\theta}(x_{0}|x_{1})}  \right] \\
    &= \mathbb{E}_{q}\left[  -\log p_{\theta}(x_{T}) + \sum_{t=2}^T \log  \frac{ q(x_{t-1}|x_{t}, x_{0})}{p_{\theta}(x_{t-1}|x_{t})} + \sum_{t=2}^T \log \frac{q(x_{t}|x_{0})}{q(x_{t-1}|x_{0})} + \log \frac{q(x_{1}|x_{0})}{p_{\theta}(x_{0}|x_{1})}  \right] \\
    &=  \mathbb{E}_{q}\left[  -\log p_{\theta}(x_{T}) + \sum_{t=2}^T \log  \frac{ q(x_{t-1}|x_{t}, x_{0})}{p_{\theta}(x_{t-1}|x_{t})} +  \log \frac{q(x_{T}|x_{0})}{q(x_{1}|x_{0})} + \log \frac{q(x_{1}|x_{0})}{p_{\theta}(x_{0}|x_{1})}  \right] \\
    &=  \mathbb{E}_{q}\left[ \log \frac{q(x_{T}|x_{0})}{p_{\theta}(x_{T}) }  + \sum_{t=2}^T \log  \frac{ q(x_{t-1}|x_{t}, x_{0})}{p_{\theta}(x_{t-1}|x_{t})}  - \log p_{\theta}(x_{0}|x_{1})  \right] \\
    &= \underbrace{D_{KL}(q(x_{T}|x_{0})\| p_{\theta}(x_{T}))}_{L_{T}} + \sum_{t=2}^T \underbrace{D_{KL}(q(x_{t-1}|x_{t}, x_{0})\| p_{\theta}(X_{t-1}\|x_{t}))}_{L_{t-1}}  \underbrace{-\log p_{\theta}(x_{0}|x_{1})}_{L_{0}} 
\end{aligned}
$$
	</article>
</main>
	
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '$', right: '$', display: false},  
      ],
      throwOnError : false
    });
  });
</script>

	

	

	



  <footer class="row row-mob al-c-mob col-rev-mob sm-2-mob  jc-bt mtb-2">
  <p>
    © Copyright notice |
    <a href="https://github.com/mivinci/hugo-theme-minima" target="_blank" rel="noopener noreferrer">Minima</a> theme on
    <a href="https://gohugo.io" target="_blank" rel="noopener noreferrer">Hugo</a>
  </p>
  <p class="row gap-0_5">
    
      <a class="icon" href="https://github.com/wangchxx" title="github">
      
        <svg fill="#63636f" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
      
      </a>
    
  </p>
</footer>
</body>
</html>