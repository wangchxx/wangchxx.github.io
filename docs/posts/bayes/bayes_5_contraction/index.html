<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bayesian Statistics| Posterior Consistency and Contraction | My Notes</title>
<meta name="keywords" content="">
<meta name="description" content="
This chapter discusses the theoretical question of whether non-parametric Bayes methods truly work. In other words, it addresses whether the posterior distribution really converges to the so-called &ldquo;true&rdquo; parameter $\theta_0$. Contraction is a richer concept than consistency, as it also involves the rate of convergence.
Dirichlet Process priors and Gaussian Process priors are common non-parametric Bayesian methods, along with their various variants. It would be too cumbersome to discuss the convergence issues of these methods on a case-by-case basis. Therefore, this chapter will focus on some general theory, which can then be applied to specific models as needed.">
<meta name="author" content="Chaohua Wang">
<link rel="canonical" href="https://wangchxx.github.io/posts/bayes/bayes_5_contraction/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.14b508198d6c32523f8895e6c6606da34de25e588fe390ef44ad07a0cc7dad33.css" integrity="sha256-FLUIGY1sMlI/iJXmxmBto03iXliP45DvRK0HoMx9rTM=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://wangchxx.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://wangchxx.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://wangchxx.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://wangchxx.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://wangchxx.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://wangchxx.github.io/posts/bayes/bayes_5_contraction/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '$', right: '$', display: false},  
      ],
      throwOnError : false
    });
  });
</script>



<meta property="og:title" content="Bayesian Statistics| Posterior Consistency and Contraction">
<meta property="og:description" content="
This chapter discusses the theoretical question of whether non-parametric Bayes methods truly work. In other words, it addresses whether the posterior distribution really converges to the so-called &ldquo;true&rdquo; parameter $\theta_0$. Contraction is a richer concept than consistency, as it also involves the rate of convergence.
Dirichlet Process priors and Gaussian Process priors are common non-parametric Bayesian methods, along with their various variants. It would be too cumbersome to discuss the convergence issues of these methods on a case-by-case basis. Therefore, this chapter will focus on some general theory, which can then be applied to specific models as needed.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://wangchxx.github.io/posts/bayes/bayes_5_contraction/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-07-20T10:52:59+02:00">
<meta property="article:modified_time" content="2021-07-20T10:52:59+02:00">


<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bayesian Statistics| Posterior Consistency and Contraction">
<meta name="twitter:description" content="
This chapter discusses the theoretical question of whether non-parametric Bayes methods truly work. In other words, it addresses whether the posterior distribution really converges to the so-called &ldquo;true&rdquo; parameter $\theta_0$. Contraction is a richer concept than consistency, as it also involves the rate of convergence.
Dirichlet Process priors and Gaussian Process priors are common non-parametric Bayesian methods, along with their various variants. It would be too cumbersome to discuss the convergence issues of these methods on a case-by-case basis. Therefore, this chapter will focus on some general theory, which can then be applied to specific models as needed.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://wangchxx.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bayesian Statistics| Posterior Consistency and Contraction",
      "item": "https://wangchxx.github.io/posts/bayes/bayes_5_contraction/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bayesian Statistics| Posterior Consistency and Contraction",
  "name": "Bayesian Statistics| Posterior Consistency and Contraction",
  "description": " This chapter discusses the theoretical question of whether non-parametric Bayes methods truly work. In other words, it addresses whether the posterior distribution really converges to the so-called \u0026ldquo;true\u0026rdquo; parameter $\\theta_0$. Contraction is a richer concept than consistency, as it also involves the rate of convergence.\nDirichlet Process priors and Gaussian Process priors are common non-parametric Bayesian methods, along with their various variants. It would be too cumbersome to discuss the convergence issues of these methods on a case-by-case basis. Therefore, this chapter will focus on some general theory, which can then be applied to specific models as needed.\n",
  "keywords": [
    
  ],
  "articleBody": " This chapter discusses the theoretical question of whether non-parametric Bayes methods truly work. In other words, it addresses whether the posterior distribution really converges to the so-called “true” parameter $\\theta_0$. Contraction is a richer concept than consistency, as it also involves the rate of convergence.\nDirichlet Process priors and Gaussian Process priors are common non-parametric Bayesian methods, along with their various variants. It would be too cumbersome to discuss the convergence issues of these methods on a case-by-case basis. Therefore, this chapter will focus on some general theory, which can then be applied to specific models as needed.\n1. Consistency Definition (Posterior consistency) Theorem (Doob’s consistency) Doob’s consistency result is indeed elegant, as it essentially states that, given a prior $\\Pi$, its posterior is almost always consistent with the true parameter $\\theta_0$​, under very few assumptions about the model. However, the issue lies in the fact that we don’t know where the $\\Pi$-null set (the set of parameters for which consistency may fail) is. In some cases, this null set can be quite large, potentially making the result not very useful in practice.\nTo address this, we need to impose some restrictions on the prior to ensure that the target parameter $\\theta_0$​ does not lie in the null set. One such restriction is the Kullback-Leibler (KL) property, which is often used to control the behavior of priors and guarantee good properties for posterior convergence.\nDefinition (KL property) This property ensures that prior assigns positive probability to any KL neighbourhood of the density $p_0:=p_{\\theta_0}$ determined by parameter $\\theta_0$. With this property in place, we can ensure that the posterior distribution is consistent at $\\theta_0$, meaning that as the number of observations increases, the posterior distribution converges to the true parameter $\\theta_0$. We state the following theorem with proof, a more detailed proof of this result could be given in a separate chapter if possible.\nTheorem (Schwartz’s) The theorem has two assumptions. The first is the Kullback-Leibler (KL) property, and the second is related to tests.\n$P_0^n\\theta_n$ can be understood as the Type I error in hypothesis testing, $P_\\theta^n (1-\\theta_n)$ can be viewed as the type II error. The second assumption is quite strong. These tests ensure that, under the given prior $\\Pi$, the true parameter $\\theta_0$​ is consistently detected, meaning that we can identify the true parameter $\\theta_{0}$ using any statistical method, not necessarily Bayesian methods.\nTo make the framework more flexible, we can relax the second condition. Instead of requiring that tests exist for every complement $\\mathcal{U}^\\complement$, we allow for the possibility that tests might not exist in some very small subset of the parameter space. Specifically, we can allow this to occur in a set with small prior probability.\nTheorem (Schwartz’s extension) Sketch of proof We need to prove that for any neighbourhood $\\mathcal{U}$ of $p_0$, we have $\\Pi_n(\\mathcal{U}^\\complement|X^{(n)})\\to 0$ a.s.. step 1: show that $\\Pi_n(\\mathcal{U}^\\complement\\cap \\mathcal{P}|X^{(n)})\\to 0$ a.s.\nstep 2: show that $\\Pi_n( \\mathcal{P}^\\complement|X^{(n)})\\to 0$ a.s.\n2. Tests As mentioned earlier, for posterior consistency, we need two conditions: the KL property and the existence of tests. The KL property is relatively straightforward to verify, but the existence of tests is not as intuitive. Therefore, in this section, we will discuss the existence of tests and why it’s important for establishing posterior consistency.\nTheorem (Convexity and tests) The theorem suggests that when the set of alternative hypotheses $\\mathcal{Q}$ is convex and separated from the “true” parameter $P$ (in terms of Hellinger distance), desired tests exist.\nThe assumption that alternatives is convex is a strong one, and in practice, it may not always hold. To address this, we can relax the convexity assumption. Instead of requiring $\\mathcal{Q}$ to be convex, we allow $\\mathcal{Q}$ to be covered by a collection of convex sets. In other words, even if the set of alternatives is not convex, as long as it can be decomposed into a union of convex sets, we can still construct valid tests. This idea is called convex covering or entropy.\nTheorem (Entropy) So far, by combining Schwartz’s extension and the existence of entropy under entropy, we can derive a more general posterior consistency result.\nTheorem (Posterior consistency under entropy) This is a very general theorem, which allows us to transform all posterior consistency problems into the problems related to covering number.\n3. Posterior Contraction Contraction is a more refined concept than consistency in Bayesian inference. While consistency only tells us whether the posterior distribution concentrates around the true parameter as the sample size grows, contraction goes further by quantifying the rate $\\epsilon$ at which the posterior distribution converges to the true parameter.\nDefinition (Posterior contraction) Theorrem (Posterior contraction) Sketch of proof Through condition (5.7), we restrict the covering number, so by Proposition 5.15, we can find appropriate tests, and then use bounded type I and type II errors to control the posterior. So far, we have developed a very powerful theory. For both consistency and contraction, we have transformed them into problems related to the covering number. For a specific non-parametric Bayesian problem, what we need to do is simply study the corresponding covering number.\nNext, we will discuss some concrete examples in a new chapter, and this chapter will conclude here.\nReferences [1] Van der Vaart, A. W. (2000). Asymptotic statistics. [2] Ghosal, S., \u0026 Van der Vaart, A. (2017). Fundamentals of nonparametric Bayesian inference ",
  "wordCount" : "887",
  "inLanguage": "en",
  "datePublished": "2021-07-20T10:52:59+02:00",
  "dateModified": "2021-07-20T10:52:59+02:00",
  "author":{
    "@type": "Person",
    "name": "Chaohua Wang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://wangchxx.github.io/posts/bayes/bayes_5_contraction/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My Notes",
    "logo": {
      "@type": "ImageObject",
      "url": "https://wangchxx.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://wangchxx.github.io/" accesskey="h" title="My Notes (Alt + H)">My Notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://wangchxx.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/series/" title="Series">
                    <span>Series</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Bayesian Statistics| Posterior Consistency and Contraction
    </h1>
    <div class="post-meta"><span title='2021-07-20 10:52:59 +0200 +0200'>July 20, 2021</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Chaohua Wang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-consistency" aria-label="1. Consistency">1. Consistency</a></li>
                <li>
                    <a href="#2-tests" aria-label="2. Tests">2. Tests</a></li>
                <li>
                    <a href="#3-posterior-contraction" aria-label="3. Posterior Contraction">3. Posterior Contraction</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><!-- # Bayesian Statistics| Posterior Consistency and Contraction -->
<p>This chapter discusses the theoretical question of whether non-parametric Bayes methods truly work. In other words, it addresses whether the posterior distribution really converges to the so-called &ldquo;true&rdquo; parameter $\theta_0$. Contraction is a richer concept than consistency, as it also involves the rate of convergence.</p>
<p>Dirichlet Process priors and Gaussian Process priors are common non-parametric Bayesian methods, along with their various variants. It would be too cumbersome to discuss the convergence issues of these methods on a case-by-case basis. Therefore, this chapter will focus on some general theory, which can then be applied to specific models as needed.</p>
<h2 id="1-consistency">1. Consistency<a hidden class="anchor" aria-hidden="true" href="#1-consistency">#</a></h2>
<dl>
<dt>Definition (Posterior consistency)</dt>
<dd><img alt="consistency_def" loading="lazy" src="/img_bayes_Rates/Consistency_1.PNG"></dd>
<dt>Theorem (Doob&rsquo;s consistency)</dt>
<dd><img alt="consistency_thm" loading="lazy" src="/img_bayes_Rates/Consistency_2.PNG"></dd>
</dl>
<p>Doob&rsquo;s consistency result is indeed elegant, as it essentially states that, given a prior $\Pi$, its posterior is almost always consistent with the true parameter $\theta_0$​, under very few assumptions about the model. However, the issue lies in the fact that we don&rsquo;t know where the $\Pi$-null set (the set of parameters for which consistency may fail) is. In some cases, this null set can be quite large, potentially making the result not very useful in practice.</p>
<p>To address this, we need to impose some restrictions on the prior to ensure that the target parameter $\theta_0$​ does not lie in the null set. One such restriction is the <strong>Kullback-Leibler (KL) property</strong>, which is often used to control the behavior of priors and guarantee good properties for posterior convergence.</p>
<dl>
<dt>Definition (KL property)</dt>
<dd><img alt="kl_p_def" loading="lazy" src="/img_bayes_Rates/Consistency_3.PNG"></dd>
</dl>
<p>This property ensures that prior assigns positive probability to any KL neighbourhood of the density $p_0:=p_{\theta_0}$ determined by parameter $\theta_0$. With this property in place, we can ensure that the posterior distribution is <strong>consistent</strong> at $\theta_0$, meaning that as the number of observations increases, the posterior distribution converges to the true parameter $\theta_0$. We state the following theorem with proof, a more detailed proof of this result could be given in a separate chapter if possible.</p>
<dl>
<dt>Theorem (Schwartz&rsquo;s)</dt>
<dd><img alt="consistency_schwartz" loading="lazy" src="/img_bayes_Rates/Consistency_4.PNG"></dd>
</dl>
<p>The theorem has two assumptions. The first is the <strong>Kullback-Leibler (KL) property</strong>, and the second is related to <strong>tests</strong>.</p>
<ul>
<li>$P_0^n\theta_n$ can be understood as the <strong>Type I error</strong> in hypothesis testing,</li>
<li>$P_\theta^n (1-\theta_n)$ can be viewed as the type II error.</li>
</ul>
<p>The second assumption is quite strong. These tests ensure that, under the given prior $\Pi$, the true parameter $\theta_0$​ is consistently detected, meaning that we can identify the true parameter $\theta_{0}$ using any statistical method, not necessarily Bayesian methods.</p>
<p>To make the framework more flexible, we can relax the second condition. Instead of requiring that tests exist for every complement $\mathcal{U}^\complement$, we allow for the possibility that tests might not exist in some very small subset of the parameter space. Specifically, we can allow this to occur in a set with small prior probability.</p>
<dl>
<dt>Theorem (Schwartz&rsquo;s extension)</dt>
<dd><img alt="consistency_schwartz" loading="lazy" src="/img_bayes_Rates/Consistency_5.PNG"></dd>
<dt>Sketch of proof</dt>
<dd>We need to prove that for any neighbourhood $\mathcal{U}$ of $p_0$, we have $\Pi_n(\mathcal{U}^\complement|X^{(n)})\to 0$ a.s..
<p>step 1: show that $\Pi_n(\mathcal{U}^\complement\cap \mathcal{P}|X^{(n)})\to 0$ a.s.</p>
<p>step 2: show that $\Pi_n( \mathcal{P}^\complement|X^{(n)})\to 0$ a.s.</p>
</dd>
</dl>
<h2 id="2-tests">2. Tests<a hidden class="anchor" aria-hidden="true" href="#2-tests">#</a></h2>
<p>As mentioned earlier, for <strong>posterior consistency</strong>, we need two conditions: the <strong>KL property</strong> and the <strong>existence of tests</strong>. The KL property is relatively straightforward to verify, but the <strong>existence of tests</strong> is not as intuitive. Therefore, in this section, we will discuss the <strong>existence of tests</strong> and why it&rsquo;s important for establishing posterior consistency.</p>
<dl>
<dt>Theorem (Convexity and tests)</dt>
<dd><img alt="tests_convex_H" loading="lazy" src="/img_bayes_Rates/Consistency_6.PNG"></dd>
</dl>
<p>The theorem suggests that when the set of alternative hypotheses $\mathcal{Q}$ is convex and separated from the &ldquo;true&rdquo; parameter $P$ (in terms of Hellinger distance), desired tests exist.</p>
<p>The assumption that alternatives is convex is a strong one, and in practice, it may not always hold. To address this, we can <strong>relax</strong> the convexity assumption. Instead of requiring $\mathcal{Q}$ to be convex, we allow $\mathcal{Q}$ to be <strong>covered</strong> by a collection of convex sets. In other words, even if the set of alternatives is not convex, as long as it can be decomposed into a union of convex sets, we can still construct valid tests. This idea is called convex covering or entropy.</p>
<dl>
<dt>Theorem (Entropy)</dt>
<dd><img alt="tests_entropy" loading="lazy" src="/img_bayes_Rates/Consistency_7.PNG"></dd>
</dl>
<p>So far, by combining Schwartz&rsquo;s extension and the existence of entropy under entropy, we can derive a more general posterior consistency result.</p>
<dl>
<dt>Theorem (Posterior consistency under entropy)</dt>
<dd><img alt="consistency_entropy" loading="lazy" src="/img_bayes_Rates/Consistency_8.PNG"></dd>
</dl>
<p>This is a very general theorem, which allows us to transform all posterior consistency problems into the problems related to covering number.</p>
<h2 id="3-posterior-contraction">3. Posterior Contraction<a hidden class="anchor" aria-hidden="true" href="#3-posterior-contraction">#</a></h2>
<p><strong>Contraction</strong> is a more refined concept than <strong>consistency</strong> in Bayesian inference. While <strong>consistency</strong> only tells us whether the posterior distribution concentrates around the true parameter as the sample size grows, <strong>contraction</strong> goes further by quantifying the <strong>rate</strong> $\epsilon$ at which the posterior distribution converges to the true parameter.</p>
<dl>
<dt>Definition (Posterior contraction)</dt>
<dd><img alt="contraction_def" loading="lazy" src="/img_bayes_Rates/Contraction_1.PNG"></dd>
<dt>Theorrem (Posterior contraction)</dt>
<dd><img alt="contraction_thm" loading="lazy" src="/img_bayes_Rates/Contraction_2.PNG"></dd>
<dt>Sketch of proof</dt>
<dd>Through condition (5.7), we restrict the covering number, so by Proposition 5.15, we can find appropriate tests, and then use bounded type I and type II errors to control the posterior.</dd>
</dl>
<p>So far, we have developed a very powerful theory. For both <strong>consistency</strong> and <strong>contraction</strong>, we have transformed them into problems related to the <strong>covering number</strong>. For a specific non-parametric Bayesian problem, what we need to do is simply study the corresponding covering number.</p>
<p>Next, we will discuss some concrete examples in a new chapter, and this chapter will conclude here.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ul>
<li>[1] Van der Vaart, A. W. (2000). Asymptotic statistics.</li>
<li>[2] Ghosal, S., &amp; Van der Vaart, A. (2017). Fundamentals of nonparametric Bayesian inference</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://wangchxx.github.io/posts/bayes/bayes_4_gp1/">
    <span class="title">« Prev</span>
    <br>
    <span>Bayesian Statistics| Gaussian Process Priors (1)</span>
  </a>
  <a class="next" href="https://wangchxx.github.io/posts/bayes/bayes_3_dirichlet/">
    <span class="title">Next »</span>
    <br>
    <span>Bayesian Statistics| Dirichlet Process</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://wangchxx.github.io/">My Notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
