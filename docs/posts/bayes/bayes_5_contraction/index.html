<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta property="og:url" content="https://wangchxx.github.io/posts/bayes/bayes_5_contraction/">
  <meta property="og:site_name" content="My Math Notes">
  <meta property="og:title" content="Bayesian Statistics| Posterior Consistency and Contraction">
  <meta property="og:description" content="This chapter discusses the theoretical question of whether non-parametric Bayes methods truly work. In other words, it addresses whether the posterior distribution really converges to the so-called “true” parameter $\theta_0$. Contraction is a richer concept than consistency, as it also involves the rate of convergence.
Dirichlet Process priors and Gaussian Process priors are common non-parametric Bayesian methods, along with their various variants. It would be too cumbersome to discuss the convergence issues of these methods on a case-by-case basis. Therefore, this chapter will focus on some general theory, which can then be applied to specific models as needed.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2021-07-20T10:52:59+02:00">
    <meta property="article:modified_time" content="2021-07-20T10:52:59+02:00">
    <meta property="article:tag" content="Markdown">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/bayes/bayes_7_gp3/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/bayes/bayes_6_gp2/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/bayes/bayes_4_gp1/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/bayes/bayes_3_dirichlet/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/bayes/bayes_2_bvm/">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Bayesian Statistics| Posterior Consistency and Contraction">
  <meta name="twitter:description" content="This chapter discusses the theoretical question of whether non-parametric Bayes methods truly work. In other words, it addresses whether the posterior distribution really converges to the so-called “true” parameter $\theta_0$. Contraction is a richer concept than consistency, as it also involves the rate of convergence.
Dirichlet Process priors and Gaussian Process priors are common non-parametric Bayesian methods, along with their various variants. It would be too cumbersome to discuss the convergence issues of these methods on a case-by-case basis. Therefore, this chapter will focus on some general theory, which can then be applied to specific models as needed.">

  
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#262d33">
  <title>
    
    My Math Notes - Bayesian Statistics| Posterior Consistency and Contraction
    
  </title>
  
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;600;700&display=swap"
    rel="stylesheet">
  <link rel="stylesheet" href="https://unpkg.com/normalize.css">
  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="/css/md.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="/css/syntax.css" />
  <script src="/js/main.js"></script>
</head>
<script>
  try {
    if (!('theme' in localStorage)) {
      localStorage.theme = window.matchMedia('(prefer-color-scheme: dark)').matches ? 'dark' : 'light';
    }
    document.querySelector('html').classList.add(localStorage.theme);
  } catch (e) {
    console.error(e);
  }
</script>
<body>
  <header>
  <h1 class="row gap-1">
    <div id="theme-switcher" class="btn lg-1"></div>
    My Math Notes
  </h1>
  <nav class="row gap-1">
  
    <a href="/">Home</a>
  
    <a href="/categories">Categories</a>
  
    <a href="/series">Series</a>
  
    <a href="/about">About</a>
  
  </nav>
  <hr>
</header>
  
  
<main>
	<h1>Bayesian Statistics| Posterior Consistency and Contraction</h1>
	<div class="sm-1 mtb-1">
		Posted at &mdash; Jul 20, 2021
		
	</div>
	<p></p>
	<article class="md">
		<!-- raw HTML omitted -->
<p>This chapter discusses the theoretical question of whether non-parametric Bayes methods truly work. In other words, it addresses whether the posterior distribution really converges to the so-called &ldquo;true&rdquo; parameter $\theta_0$. Contraction is a richer concept than consistency, as it also involves the rate of convergence.</p>
<p>Dirichlet Process priors and Gaussian Process priors are common non-parametric Bayesian methods, along with their various variants. It would be too cumbersome to discuss the convergence issues of these methods on a case-by-case basis. Therefore, this chapter will focus on some general theory, which can then be applied to specific models as needed.</p>
<h2 id="1-consistency">1. Consistency</h2>
<dl>
<dt>Definition (Posterior consistency)</dt>
<dd><img src="/img_bayes_Rates/Consistency_1.PNG" alt="consistency_def"></dd>
<dt>Theorem (Doob&rsquo;s consistency)</dt>
<dd><img src="/img_bayes_Rates/Consistency_2.PNG" alt="consistency_thm"></dd>
</dl>
<p>Doob&rsquo;s consistency result is indeed elegant, as it essentially states that, given a prior $\Pi$, its posterior is almost always consistent with the true parameter $\theta_0$​, under very few assumptions about the model. However, the issue lies in the fact that we don&rsquo;t know where the $\Pi$-null set (the set of parameters for which consistency may fail) is. In some cases, this null set can be quite large, potentially making the result not very useful in practice.</p>
<p>To address this, we need to impose some restrictions on the prior to ensure that the target parameter $\theta_0$​ does not lie in the null set. One such restriction is the <strong>Kullback-Leibler (KL) property</strong>, which is often used to control the behavior of priors and guarantee good properties for posterior convergence.</p>
<dl>
<dt>Definition (KL property)</dt>
<dd><img src="/img_bayes_Rates/Consistency_3.PNG" alt="kl_p_def"></dd>
</dl>
<p>This property ensures that prior assigns positive probability to any KL neighbourhood of the density $p_0:=p_{\theta_0}$ determined by parameter $\theta_0$. With this property in place, we can ensure that the posterior distribution is <strong>consistent</strong> at $\theta_0$, meaning that as the number of observations increases, the posterior distribution converges to the true parameter $\theta_0$. We state the following theorem with proof, a more detailed proof of this result could be given in a separate chapter if possible.</p>
<dl>
<dt>Theorem (Schwartz&rsquo;s)</dt>
<dd><img src="/img_bayes_Rates/Consistency_4.PNG" alt="consistency_schwartz"></dd>
</dl>
<p>The theorem has two assumptions. The first is the <strong>Kullback-Leibler (KL) property</strong>, and the second is related to <strong>tests</strong>.</p>
<ul>
<li>$P_0^n\theta_n$ can be understood as the <strong>Type I error</strong> in hypothesis testing,</li>
<li>$P_\theta^n (1-\theta_n)$ can be viewed as the type II error.</li>
</ul>
<p>The second assumption is quite strong. These tests ensure that, under the given prior $\Pi$, the true parameter $\theta_0$​ is consistently detected, meaning that we can identify the true parameter $\theta_{0}$ using any statistical method, not necessarily Bayesian methods.</p>
<p>To make the framework more flexible, we can relax the second condition. Instead of requiring that tests exist for every complement $\mathcal{U}^\complement$, we allow for the possibility that tests might not exist in some very small subset of the parameter space. Specifically, we can allow this to occur in a set with small prior probability.</p>
<dl>
<dt>Theorem (Schwartz&rsquo;s extension)</dt>
<dd><img src="/img_bayes_Rates/Consistency_5.PNG" alt="consistency_schwartz"></dd>
<dt>Sketch of proof</dt>
<dd>We need to prove that for any neighbourhood $\mathcal{U}$ of $p_0$, we have $\Pi_n(\mathcal{U}^\complement|X^{(n)})\to 0$ a.s..
<p>step 1: show that $\Pi_n(\mathcal{U}^\complement\cap \mathcal{P}|X^{(n)})\to 0$ a.s.</p>
<p>step 2: show that $\Pi_n( \mathcal{P}^\complement|X^{(n)})\to 0$ a.s.</p>
</dd>
</dl>
<h2 id="2-tests">2. Tests</h2>
<p>As mentioned earlier, for <strong>posterior consistency</strong>, we need two conditions: the <strong>KL property</strong> and the <strong>existence of tests</strong>. The KL property is relatively straightforward to verify, but the <strong>existence of tests</strong> is not as intuitive. Therefore, in this section, we will discuss the <strong>existence of tests</strong> and why it&rsquo;s important for establishing posterior consistency.</p>
<dl>
<dt>Theorem (Convexity and tests)</dt>
<dd><img src="/img_bayes_Rates/Consistency_6.PNG" alt="tests_convex_H"></dd>
</dl>
<p>The theorem suggests that when the set of alternative hypotheses $\mathcal{Q}$ is convex and separated from the &ldquo;true&rdquo; parameter $P$ (in terms of Hellinger distance), desired tests exist.</p>
<p>The assumption that alternatives is convex is a strong one, and in practice, it may not always hold. To address this, we can <strong>relax</strong> the convexity assumption. Instead of requiring $\mathcal{Q}$ to be convex, we allow $\mathcal{Q}$ to be <strong>covered</strong> by a collection of convex sets. In other words, even if the set of alternatives is not convex, as long as it can be decomposed into a union of convex sets, we can still construct valid tests. This idea is called convex covering or entropy.</p>
<dl>
<dt>Theorem (Entropy)</dt>
<dd><img src="/img_bayes_Rates/Consistency_7.PNG" alt="tests_entropy"></dd>
</dl>
<p>So far, by combining Schwartz&rsquo;s extension and the existence of entropy under entropy, we can derive a more general posterior consistency result.</p>
<dl>
<dt>Theorem (Posterior consistency under entropy)</dt>
<dd><img src="/img_bayes_Rates/Consistency_8.PNG" alt="consistency_entropy"></dd>
</dl>
<p>This is a very general theorem, which allows us to transform all posterior consistency problems into the problems related to covering number.</p>
<h2 id="3-posterior-contraction">3. Posterior Contraction</h2>
<p><strong>Contraction</strong> is a more refined concept than <strong>consistency</strong> in Bayesian inference. While <strong>consistency</strong> only tells us whether the posterior distribution concentrates around the true parameter as the sample size grows, <strong>contraction</strong> goes further by quantifying the <strong>rate</strong> $\epsilon$ at which the posterior distribution converges to the true parameter.</p>
<dl>
<dt>Definition (Posterior contraction)</dt>
<dd><img src="/img_bayes_Rates/Contraction_1.PNG" alt="contraction_def"></dd>
<dt>Theorrem (Posterior contraction)</dt>
<dd><img src="/img_bayes_Rates/Contraction_2.PNG" alt="contraction_thm"></dd>
<dt>Sketch of proof</dt>
<dd>Through condition (5.7), we restrict the covering number, so by Proposition 5.15, we can find appropriate tests, and then use bounded type I and type II errors to control the posterior.</dd>
</dl>
<p>So far, we have developed a very powerful theory. For both <strong>consistency</strong> and <strong>contraction</strong>, we have transformed them into problems related to the <strong>covering number</strong>. For a specific non-parametric Bayesian problem, what we need to do is simply study the corresponding covering number.</p>
<p>Next, we will discuss some concrete examples in a new chapter, and this chapter will conclude here.</p>
<h2 id="references">References</h2>
<ul>
<li>[1] Van der Vaart, A. W. (2000). Asymptotic statistics.</li>
<li>[2] Ghosal, S., &amp; Van der Vaart, A. (2017). Fundamentals of nonparametric Bayesian inference</li>
</ul>

	</article>
</main>
	
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css"
  integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js"
  integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js"
  integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      
      
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
        { left: '\\(', right: '\\)', display: false },
        { left: '\\[', right: '\\]', display: true },
        
      ],
      
      throwOnError: false
    });
  });
</script>
	

	

	



  <footer class="row row-mob al-c-mob col-rev-mob sm-2-mob  jc-bt mtb-2">
  <p>
    © Copyright notice |
    <a href="https://github.com/mivinci/hugo-theme-minima" target="_blank" rel="noopener noreferrer">Minima</a> theme on
    <a href="https://gohugo.io" target="_blank" rel="noopener noreferrer">Hugo</a>
  </p>
  <p class="row gap-0_5">
    
      <a class="icon" href="https://github.com/wangchxx" title="github">
      
        <svg fill="#63636f" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
      
      </a>
    
  </p>
</footer>
</body>
</html>