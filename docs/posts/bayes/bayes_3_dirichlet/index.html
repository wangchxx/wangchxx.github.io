<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bayesian Statistics| Dirichlet Process | My Notes</title>
<meta name="keywords" content="">
<meta name="description" content="
The Dirichlet Process (DP) is widely used in Bayesian nonparametrics, where it is often treated as a default prior on spaces of probability measures. As a prior on probability measures, the DP reflects our prior belief about the potential distributions of data, with the assumption that the data might be drawn from an infinite mixture of possible underlying distributions. This is why the Dirichlet Process is often called a nonparametric prior: it does not assume a fixed, finite number of parameters but rather allows the model to grow with the complexity of the data.">
<meta name="author" content="Chaohua Wang">
<link rel="canonical" href="https://wangchxx.github.io/posts/bayes/bayes_3_dirichlet/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.14b508198d6c32523f8895e6c6606da34de25e588fe390ef44ad07a0cc7dad33.css" integrity="sha256-FLUIGY1sMlI/iJXmxmBto03iXliP45DvRK0HoMx9rTM=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://wangchxx.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://wangchxx.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://wangchxx.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://wangchxx.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://wangchxx.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://wangchxx.github.io/posts/bayes/bayes_3_dirichlet/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '$', right: '$', display: false},  
      ],
      throwOnError : false
    });
  });
</script>



<meta property="og:title" content="Bayesian Statistics| Dirichlet Process">
<meta property="og:description" content="
The Dirichlet Process (DP) is widely used in Bayesian nonparametrics, where it is often treated as a default prior on spaces of probability measures. As a prior on probability measures, the DP reflects our prior belief about the potential distributions of data, with the assumption that the data might be drawn from an infinite mixture of possible underlying distributions. This is why the Dirichlet Process is often called a nonparametric prior: it does not assume a fixed, finite number of parameters but rather allows the model to grow with the complexity of the data.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://wangchxx.github.io/posts/bayes/bayes_3_dirichlet/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-07-19T10:52:59+02:00">
<meta property="article:modified_time" content="2021-07-19T10:52:59+02:00">


<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bayesian Statistics| Dirichlet Process">
<meta name="twitter:description" content="
The Dirichlet Process (DP) is widely used in Bayesian nonparametrics, where it is often treated as a default prior on spaces of probability measures. As a prior on probability measures, the DP reflects our prior belief about the potential distributions of data, with the assumption that the data might be drawn from an infinite mixture of possible underlying distributions. This is why the Dirichlet Process is often called a nonparametric prior: it does not assume a fixed, finite number of parameters but rather allows the model to grow with the complexity of the data.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://wangchxx.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bayesian Statistics| Dirichlet Process",
      "item": "https://wangchxx.github.io/posts/bayes/bayes_3_dirichlet/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bayesian Statistics| Dirichlet Process",
  "name": "Bayesian Statistics| Dirichlet Process",
  "description": " The Dirichlet Process (DP) is widely used in Bayesian nonparametrics, where it is often treated as a default prior on spaces of probability measures. As a prior on probability measures, the DP reflects our prior belief about the potential distributions of data, with the assumption that the data might be drawn from an infinite mixture of possible underlying distributions. This is why the Dirichlet Process is often called a nonparametric prior: it does not assume a fixed, finite number of parameters but rather allows the model to grow with the complexity of the data.\n",
  "keywords": [
    
  ],
  "articleBody": " The Dirichlet Process (DP) is widely used in Bayesian nonparametrics, where it is often treated as a default prior on spaces of probability measures. As a prior on probability measures, the DP reflects our prior belief about the potential distributions of data, with the assumption that the data might be drawn from an infinite mixture of possible underlying distributions. This is why the Dirichlet Process is often called a nonparametric prior: it does not assume a fixed, finite number of parameters but rather allows the model to grow with the complexity of the data.\n0. Parametric VS Non-parametric Non-parametric does not mean the absence of parameters; rather, it indicates that the parameter space $\\mathcal{\\Theta}$ is infinite-dimensional. On the other hand， a statistical model is called parametric if its parameter space $\\mathcal{\\Theta}$ is finite-dimensional.\n1. Definitions Having a definition does not guarantee the actual existence of such an entity, so its existence must be proven. A Dirichlet Process (DP) can be viewed as a stochastic process, with equation (4.3) representing its finite-dimensional distributions. According to Kolmogorov’s Extension Theorem, we can establish the existence of the process.\nTo understand the Dirichlet Process (DP) prior, for a stochastic process $(X_t:t\\in T)$, we have two main ways to interpret it:\nFor a fixed $t$, $X_t:\\Omega\\to\\mathbb{R}$ is a random variable. For a fixed $\\omega\\in \\Omega$, the mapping $t\\mapsto X_t(\\omega)$ is called a sample path of $(X_t:t\\in T)$. Therefore, the stochastic process $(X_t:t\\in T)$ can be understood as the collection of its sample paths, and this stochastic process also defines the distribution of these sample paths. For the Dirichlet Process (DP), a sample path is a probability measure, meaning that each realization of the Dirichlet Process is a distinct probability distribution over the sample space. Thus, the Dirichlet Process provides a prior $\\Pi$ over these probability measures. In Bayesian inference, this prior reflects our uncertainty about the underlying distribution from which the data are drawn. The Dirichlet Process acts as a distribution over potential distributions, offering a flexible way to model an infinite-dimensional space of possible probability measures, allowing for an infinite number of clusters or categories in a non-parametric model.\n2. Properties First, let’s introduce a simulation method known as stick-breaking. The stick-breaking process is a popular technique used to generate samples from a Dirichlet Process (DP). The idea is to iteratively break a “stick” into smaller pieces, with each piece representing the probability mass assigned to a cluster.\nThis means that we can simulate a Dirichlet Process (DP) using the base measure $\\alpha$ and the scaling parameter $M$.\nsketch of proof step 1: show that $P = \\sum_j W_j \\delta_{\\theta_j}$ is a random measure. step 2: show that $P \\sim DP(M\\bar{\\alpha})$. For $j\\geq 2$, define $W_{j-1}' = W_j/(1-Y_1), \\theta'_j = \\theta_{j+1}$, then\n$$ P= Y_1 \\delta_{\\theta_1} + (1-Y_1)\\sum_{j\\geq 1} W_j'\\delta_{\\theta'_j}. $$Notice that the random measure $P':= \\sum_{j\\geq 1} W_j'\\delta_{\\theta'_j}$ has the same structure as $P$, and hence has the same distribution. Thus we have the equation $$P = Y\\delta_{\\theta} + (1-Y) P.$$ The remaining task is to solve this equation, and it can be proven that $P\\sim DP(M\\bar{\\alpha}）$ is the unique solution. $\\Box$\nThe second very important property of the Dirichlet Process (DP) is called tail-freeness. The concept is quite complex (refer to Freedman, 1963), but simply put, it represents a form of independence. Without proof, we state the conclusion: $DP(\\alpha)$ is tail-free. The tail-free property is crucial for deriving the posterior of the Dirichlet Process. It ensures that the random measure generated by the DP exhibits a kind of independence in its behavior, making the computation of posterior distributions tractable. Now, without proof, we present a theorem related to the Dirichlet Process,\nwhere $N_\\epsilon := \\# \\{i: X_i\\in A_\\epsilon\\}$, i.e. the number of observations falling in $A_\\epsilon$.\nThe tail-free property of the Dirichlet Process is extremely useful because, with this property, when calculating the posterior, we no longer need to worry about the exact values of the observations $X_i$，Instead, we only need to know the number of observations $X_{i}$ that fall into a given set.\nTheorem (Conjugacy of DP) The posterior of DP is again a DP. where $MN$ indicates the multinomial distribution. And the Multinomial-Dirichlet distribution conjugacy is used in the proof.\nThis theorem tells us that, when updating the posterior of a Dirichlet Process, we only need to update the probabilities of the observed points (rather than the exact data values themselves). Let $\\mathbb{P}_n$ denote the empirical distribution of observations, then the posterior of DP can be written as $DP(\\alpha + n\\mathbb{P}_n)$. This is a key result in Bayesian nonparametrics, particularly in models where the number of clusters or components is unknown and can grow as more data are observed. It shows that, thanks to the tail-free property, we can focus on the distribution of observed values and their frequencies, making posterior updates computationally feasible.\n3. Consistency When we use the Dirichlet Process (DP) to estimate a probability measure, we are often concerned with its convergence properties, which are central to most learning algorithms. Specifically, we want to ensure that as we observe more data, the Dirichlet Process will converge to the true underlying distribution or probability measure. In this context, we can say that the DP prior does work, meaning that it provides a consistent way to estimate the true distribution as more data is observed, and we will now proceed to prove this result.\nTheorem (Consistency of DP) Suppose that observations $X_i\\sim P_0$ independently. sketch of proof Since Dirichlet Processes (DP) are discrete and not directly suitable for estimating continuous functions, transitioning to Gaussian Processes (GP) makes sense, as they are commonly used for estimating continuous functions in Bayesian nonparametrics.\nReferences [1] Ghosal, S. (2010). The Dirichlet process, related priors and posterior asymptotics [2] Ghosal, S., \u0026 Van der Vaart, A. (2017). Fundamentals of nonparametric Bayesian inference ",
  "wordCount" : "968",
  "inLanguage": "en",
  "datePublished": "2021-07-19T10:52:59+02:00",
  "dateModified": "2021-07-19T10:52:59+02:00",
  "author":{
    "@type": "Person",
    "name": "Chaohua Wang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://wangchxx.github.io/posts/bayes/bayes_3_dirichlet/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My Notes",
    "logo": {
      "@type": "ImageObject",
      "url": "https://wangchxx.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://wangchxx.github.io/" accesskey="h" title="My Notes (Alt + H)">My Notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://wangchxx.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/series/" title="Series">
                    <span>Series</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Bayesian Statistics| Dirichlet Process
    </h1>
    <div class="post-meta"><span title='2021-07-19 10:52:59 +0200 +0200'>July 19, 2021</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Chaohua Wang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#0-parametric-vs-non-parametric" aria-label="0. Parametric VS Non-parametric">0. Parametric VS Non-parametric</a></li>
                <li>
                    <a href="#1-definitions" aria-label="1. Definitions">1. Definitions</a></li>
                <li>
                    <a href="#2-properties" aria-label="2. Properties">2. Properties</a></li>
                <li>
                    <a href="#3-consistency" aria-label="3. Consistency">3. Consistency</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><!-- # Bayesian Statistics| Dirichlet Process -->
<p>The Dirichlet Process (DP) is widely used in <strong>Bayesian nonparametrics</strong>, where it is often treated as a default prior on spaces of probability measures. As a <strong>prior</strong> on probability measures, the DP reflects our prior belief about the potential distributions of data, with the assumption that the data might be drawn from an infinite mixture of possible underlying distributions. This is why the Dirichlet Process is often called a <strong>nonparametric prior</strong>: it does not assume a fixed, finite number of parameters but rather allows the model to grow with the complexity of the data.</p>
<h2 id="0-parametric-vs-non-parametric">0. Parametric VS Non-parametric<a hidden class="anchor" aria-hidden="true" href="#0-parametric-vs-non-parametric">#</a></h2>
<p>Non-parametric does not mean the absence of parameters; rather, it indicates that the parameter space $\mathcal{\Theta}$ is infinite-dimensional. On the other hand， a statistical model is called parametric if its parameter space $\mathcal{\Theta}$ is finite-dimensional.</p>
<h2 id="1-definitions">1. Definitions<a hidden class="anchor" aria-hidden="true" href="#1-definitions">#</a></h2>
<p><img alt="dirichlet_definition" loading="lazy" src="/img_bayes_DP/dirichlet_1.PNG"></p>
<p>Having a definition does not guarantee the actual existence of such an entity, so its existence must be proven. A Dirichlet Process (DP) can be viewed as a stochastic process, with equation (4.3) representing its finite-dimensional distributions. According to Kolmogorov&rsquo;s Extension Theorem, we can establish the existence of the process.</p>
<p>To understand the Dirichlet Process (DP) prior, for a stochastic process $(X_t:t\in T)$, we have two main ways to interpret it:</p>
<ol>
<li>For a fixed $t$, $X_t:\Omega\to\mathbb{R}$ is a random variable.</li>
<li>For a fixed $\omega\in \Omega$, the mapping $t\mapsto X_t(\omega)$ is called a sample path of $(X_t:t\in T)$.</li>
</ol>
<p>Therefore, the stochastic process $(X_t:t\in T)$ can be understood as the collection of its sample paths, and this stochastic process also defines the distribution of these sample paths. For the Dirichlet Process (DP), a sample path is a probability measure, meaning that each realization of the Dirichlet Process is a distinct probability distribution over the sample space. Thus, the Dirichlet Process provides a prior $\Pi$ over these probability measures. In Bayesian inference, this prior reflects our uncertainty about the underlying distribution from which the data are drawn. The Dirichlet Process acts as a distribution over potential distributions, offering a flexible way to model an infinite-dimensional space of possible probability measures, allowing for an infinite number of clusters or categories in a non-parametric model.</p>
<h2 id="2-properties">2. Properties<a hidden class="anchor" aria-hidden="true" href="#2-properties">#</a></h2>
<p>First, let&rsquo;s introduce a simulation method known as <strong>stick-breaking</strong>. The stick-breaking process is a popular technique used to generate samples from a Dirichlet Process (DP). The idea is to iteratively break a &ldquo;stick&rdquo; into smaller pieces, with each piece representing the probability mass assigned to a cluster.</p>
<p><img alt="stick_breaking" loading="lazy" src="/img_bayes_DP/dirichlet_2.PNG"></p>
<p>This means that we can simulate a Dirichlet Process (DP) using the <strong>base measure</strong> $\alpha$ and the <strong>scaling parameter</strong> $M$.</p>
<dl>
<dt>sketch of proof</dt>
<dd>step 1: show that $P = \sum_j W_j \delta_{\theta_j}$ is  a random measure.
step 2: show that $P \sim DP(M\bar{\alpha})$.
<p>For $j\geq 2$, define $W_{j-1}' = W_j/(1-Y_1), \theta'_j = \theta_{j+1}$, then</p>
$$
  P= Y_1 \delta_{\theta_1} + (1-Y_1)\sum_{j\geq 1} W_j'\delta_{\theta'_j}.
  $$<p>Notice that the random measure $P':= \sum_{j\geq 1} W_j'\delta_{\theta'_j}$ has the same structure as $P$, and hence has the same distribution. Thus we have the equation
</p>
$$P = Y\delta_{\theta} + (1-Y) P.$$<p>
The remaining task is to solve this equation, and it can be proven that $P\sim DP(M\bar{\alpha}）$ is the unique solution.
$\Box$</p>
</dd>
</dl>
<p>The second very important property of the Dirichlet Process (DP) is called <strong>tail-freeness</strong>. The concept is quite complex (refer to Freedman, 1963), but simply put, it represents a form of independence. Without proof, we state the conclusion: $DP(\alpha)$ is tail-free. The tail-free property is crucial for deriving the <strong>posterior</strong> of the Dirichlet Process. It ensures that the random measure generated by the DP exhibits a kind of <strong>independence</strong> in its behavior, making the computation of posterior distributions tractable. Now, without proof, we present a <strong>theorem</strong> related to the Dirichlet Process,</p>
<p><img alt="tail_free" loading="lazy" src="/img_bayes_DP/dirichlet_3.PNG">
where $N_\epsilon := \# \{i: X_i\in A_\epsilon\}$, i.e. the number of observations falling in $A_\epsilon$.</p>
<p>The tail-free property of the Dirichlet Process is extremely useful because, with this property, when calculating the posterior, we no longer need to worry about the exact values of the observations $X_i$，Instead, we only need to know the <strong>number of observations</strong> $X_{i}$ that fall into a given set.</p>
<dl>
<dt>Theorem (Conjugacy of DP)</dt>
<dd>The posterior of DP is again a DP.</dd>
</dl>
<p><img alt="dirichlet_conjugacy" loading="lazy" src="/img_bayes_DP/dirichlet_4.PNG">
where $MN$ indicates the multinomial distribution. And the Multinomial-Dirichlet distribution conjugacy is used in the proof.</p>
<p>This theorem tells us that, when updating the posterior of a Dirichlet Process, we only need to update the probabilities of the observed points (rather than the exact data values themselves). Let $\mathbb{P}_n$ denote the empirical distribution of observations, then the posterior of DP can be written as $DP(\alpha + n\mathbb{P}_n)$. This is a key result in Bayesian nonparametrics, particularly in models where the number of clusters or components is unknown and can grow as more data are observed. It shows that, thanks to the tail-free property, we can focus on the distribution of observed values and their frequencies, making posterior updates computationally feasible.</p>
<h2 id="3-consistency">3. Consistency<a hidden class="anchor" aria-hidden="true" href="#3-consistency">#</a></h2>
<p>When we use the Dirichlet Process (DP) to estimate a probability measure, we are often concerned with its <strong>convergence properties</strong>, which are central to most learning algorithms. Specifically, we want to ensure that as we observe more data, the Dirichlet Process will converge to the true underlying distribution or probability measure. In this context, we can say that the DP prior does work, meaning that it provides a consistent way to estimate the true distribution as more data is observed, and we will now proceed to prove this result.</p>
<dl>
<dt>Theorem (Consistency of DP)</dt>
<dd>Suppose that observations $X_i\sim P_0$ independently.
<img alt="dirichlet_consistency" loading="lazy" src="/img_bayes_DP/dirichlet_5.PNG"></dd>
<dt>sketch of proof</dt>
<dd><img alt="dirichlet_consistency_prof" loading="lazy" src="/img_bayes_DP/dirichlet_6.PNG"></dd>
</dl>
<p>Since Dirichlet Processes (DP) are discrete and not directly suitable for estimating continuous functions, transitioning to <strong>Gaussian Processes (GP)</strong> makes sense, as they are commonly used for estimating continuous functions in Bayesian nonparametrics.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ul>
<li>[1] Ghosal, S. (2010). The Dirichlet process, related priors and posterior asymptotics</li>
<li>[2] Ghosal, S., &amp; Van der Vaart, A. (2017). Fundamentals of nonparametric Bayesian inference</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://wangchxx.github.io/posts/bayes/bayes_5_contraction/">
    <span class="title">« Prev</span>
    <br>
    <span>Bayesian Statistics| Posterior Consistency and Contraction</span>
  </a>
  <a class="next" href="https://wangchxx.github.io/posts/bayes/bayes_2_bvm/">
    <span class="title">Next »</span>
    <br>
    <span>Bayesian Statistics| Bernstein-von Mises Theorem</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://wangchxx.github.io/">My Notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
