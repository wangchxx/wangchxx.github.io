<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta property="og:url" content="https://wangchxx.github.io/posts/bayes/bayes_3_dirichlet/">
  <meta property="og:site_name" content="My Math Notes">
  <meta property="og:title" content="Bayesian Statistics| Dirichlet Process">
  <meta property="og:description" content="The Dirichlet Process (DP) is widely used in Bayesian nonparametrics, where it is often treated as a default prior on spaces of probability measures. As a prior on probability measures, the DP reflects our prior belief about the potential distributions of data, with the assumption that the data might be drawn from an infinite mixture of possible underlying distributions. This is why the Dirichlet Process is often called a nonparametric prior: it does not assume a fixed, finite number of parameters but rather allows the model to grow with the complexity of the data.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2021-07-19T10:52:59+02:00">
    <meta property="article:modified_time" content="2021-07-19T10:52:59+02:00">
    <meta property="article:tag" content="Markdown">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/bayes/bayes_7_gp3/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/bayes/bayes_6_gp2/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/bayes/bayes_4_gp1/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/bayes/bayes_5_contraction/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/bayes/bayes_2_bvm/">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Bayesian Statistics| Dirichlet Process">
  <meta name="twitter:description" content="The Dirichlet Process (DP) is widely used in Bayesian nonparametrics, where it is often treated as a default prior on spaces of probability measures. As a prior on probability measures, the DP reflects our prior belief about the potential distributions of data, with the assumption that the data might be drawn from an infinite mixture of possible underlying distributions. This is why the Dirichlet Process is often called a nonparametric prior: it does not assume a fixed, finite number of parameters but rather allows the model to grow with the complexity of the data.">

  
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#262d33">
  <title>
    
    My Math Notes - Bayesian Statistics| Dirichlet Process
    
  </title>
  
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;600;700&display=swap"
    rel="stylesheet">
  <link rel="stylesheet" href="https://unpkg.com/normalize.css">
  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="/css/md.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="/css/syntax.css" />
  <script src="/js/main.js"></script>
</head>
<script>
  try {
    if (!('theme' in localStorage)) {
      localStorage.theme = window.matchMedia('(prefer-color-scheme: dark)').matches ? 'dark' : 'light';
    }
    document.querySelector('html').classList.add(localStorage.theme);
  } catch (e) {
    console.error(e);
  }
</script>
<body>
  <header>
  <h1 class="row gap-1">
    <div id="theme-switcher" class="btn lg-1"></div>
    My Math Notes
  </h1>
  <nav class="row gap-1">
  
    <a href="/">Home</a>
  
    <a href="/categories">Categories</a>
  
    <a href="/series">Series</a>
  
    <a href="/about">About</a>
  
  </nav>
  <hr>
</header>
  
  
<main>
	<h1>Bayesian Statistics| Dirichlet Process</h1>
	<div class="sm-1 mtb-1">
		Posted at &mdash; Jul 19, 2021
		
	</div>
	<p></p>
	<article class="md">
		<!-- raw HTML omitted -->
<p>The Dirichlet Process (DP) is widely used in <strong>Bayesian nonparametrics</strong>, where it is often treated as a default prior on spaces of probability measures. As a <strong>prior</strong> on probability measures, the DP reflects our prior belief about the potential distributions of data, with the assumption that the data might be drawn from an infinite mixture of possible underlying distributions. This is why the Dirichlet Process is often called a <strong>nonparametric prior</strong>: it does not assume a fixed, finite number of parameters but rather allows the model to grow with the complexity of the data.</p>
<h2 id="0-parametric-vs-non-parametric">0. Parametric VS Non-parametric</h2>
<p>Non-parametric does not mean the absence of parameters; rather, it indicates that the parameter space $\mathcal{\Theta}$ is infinite-dimensional. On the other hand， a statistical model is called parametric if its parameter space $\mathcal{\Theta}$ is finite-dimensional.</p>
<h2 id="1-definitions">1. Definitions</h2>
<p><img src="/img_bayes_DP/dirichlet_1.PNG" alt="dirichlet_definition"></p>
<p>Having a definition does not guarantee the actual existence of such an entity, so its existence must be proven. A Dirichlet Process (DP) can be viewed as a stochastic process, with equation (4.3) representing its finite-dimensional distributions. According to Kolmogorov&rsquo;s Extension Theorem, we can establish the existence of the process.</p>
<p>To understand the Dirichlet Process (DP) prior, for a stochastic process $(X_t:t\in T)$, we have two main ways to interpret it:</p>
<ol>
<li>For a fixed $t$, $X_t:\Omega\to\mathbb{R}$ is a random variable.</li>
<li>For a fixed $\omega\in \Omega$, the mapping $t\mapsto X_t(\omega)$ is called a sample path of $(X_t:t\in T)$.</li>
</ol>
<p>Therefore, the stochastic process $(X_t:t\in T)$ can be understood as the collection of its sample paths, and this stochastic process also defines the distribution of these sample paths. For the Dirichlet Process (DP), a sample path is a probability measure, meaning that each realization of the Dirichlet Process is a distinct probability distribution over the sample space. Thus, the Dirichlet Process provides a prior $\Pi$ over these probability measures. In Bayesian inference, this prior reflects our uncertainty about the underlying distribution from which the data are drawn. The Dirichlet Process acts as a distribution over potential distributions, offering a flexible way to model an infinite-dimensional space of possible probability measures, allowing for an infinite number of clusters or categories in a non-parametric model.</p>
<h2 id="2-properties">2. Properties</h2>
<p>First, let&rsquo;s introduce a simulation method known as <strong>stick-breaking</strong>. The stick-breaking process is a popular technique used to generate samples from a Dirichlet Process (DP). The idea is to iteratively break a &ldquo;stick&rdquo; into smaller pieces, with each piece representing the probability mass assigned to a cluster.</p>
<p><img src="/img_bayes_DP/dirichlet_2.PNG" alt="stick_breaking"></p>
<p>This means that we can simulate a Dirichlet Process (DP) using the <strong>base measure</strong> $\alpha$ and the <strong>scaling parameter</strong> $M$.</p>
<dl>
<dt>sketch of proof</dt>
<dd>step 1: show that $P = \sum_j W_j \delta_{\theta_j}$ is  a random measure.
step 2: show that $P \sim DP(M\bar{\alpha})$.
<p>For $j\geq 2$, define $W_{j-1}' = W_j/(1-Y_1), \theta'_j = \theta_{j+1}$, then</p>
$$
  P= Y_1 \delta_{\theta_1} + (1-Y_1)\sum_{j\geq 1} W_j'\delta_{\theta'_j}.
  $$<p>Notice that the random measure $P':= \sum_{j\geq 1} W_j'\delta_{\theta'_j}$ has the same structure as $P$, and hence has the same distribution. Thus we have the equation
</p>
$$P = Y\delta_{\theta} + (1-Y) P.$$<p>
The remaining task is to solve this equation, and it can be proven that $P\sim DP(M\bar{\alpha}）$ is the unique solution.
$\Box$</p>
</dd>
</dl>
<p>The second very important property of the Dirichlet Process (DP) is called <strong>tail-freeness</strong>. The concept is quite complex (refer to Freedman, 1963), but simply put, it represents a form of independence. Without proof, we state the conclusion: $DP(\alpha)$ is tail-free. The tail-free property is crucial for deriving the <strong>posterior</strong> of the Dirichlet Process. It ensures that the random measure generated by the DP exhibits a kind of <strong>independence</strong> in its behavior, making the computation of posterior distributions tractable. Now, without proof, we present a <strong>theorem</strong> related to the Dirichlet Process,</p>
<p><img src="/img_bayes_DP/dirichlet_3.PNG" alt="tail_free">
where $N_\epsilon := \# \{i: X_i\in A_\epsilon\}$, i.e. the number of observations falling in $A_\epsilon$.</p>
<p>The tail-free property of the Dirichlet Process is extremely useful because, with this property, when calculating the posterior, we no longer need to worry about the exact values of the observations $X_i$，Instead, we only need to know the <strong>number of observations</strong> $X_{i}$ that fall into a given set.</p>
<dl>
<dt>Theorem (Conjugacy of DP)</dt>
<dd>The posterior of DP is again a DP.</dd>
</dl>
<p><img src="/img_bayes_DP/dirichlet_4.PNG" alt="dirichlet_conjugacy">
where $MN$ indicates the multinomial distribution. And the Multinomial-Dirichlet distribution conjugacy is used in the proof.</p>
<p>This theorem tells us that, when updating the posterior of a Dirichlet Process, we only need to update the probabilities of the observed points (rather than the exact data values themselves). Let $\mathbb{P}_n$ denote the empirical distribution of observations, then the posterior of DP can be written as $DP(\alpha + n\mathbb{P}_n)$. This is a key result in Bayesian nonparametrics, particularly in models where the number of clusters or components is unknown and can grow as more data are observed. It shows that, thanks to the tail-free property, we can focus on the distribution of observed values and their frequencies, making posterior updates computationally feasible.</p>
<h2 id="3-consistency">3. Consistency</h2>
<p>When we use the Dirichlet Process (DP) to estimate a probability measure, we are often concerned with its <strong>convergence properties</strong>, which are central to most learning algorithms. Specifically, we want to ensure that as we observe more data, the Dirichlet Process will converge to the true underlying distribution or probability measure. In this context, we can say that the DP prior does work, meaning that it provides a consistent way to estimate the true distribution as more data is observed, and we will now proceed to prove this result.</p>
<dl>
<dt>Theorem (Consistency of DP)</dt>
<dd>Suppose that observations $X_i\sim P_0$ independently.
<img src="/img_bayes_DP/dirichlet_5.PNG" alt="dirichlet_consistency"></dd>
<dt>sketch of proof</dt>
<dd><img src="/img_bayes_DP/dirichlet_6.PNG" alt="dirichlet_consistency_prof"></dd>
</dl>
<p>Since Dirichlet Processes (DP) are discrete and not directly suitable for estimating continuous functions, transitioning to <strong>Gaussian Processes (GP)</strong> makes sense, as they are commonly used for estimating continuous functions in Bayesian nonparametrics.</p>
<h2 id="references">References</h2>
<ul>
<li>[1] Ghosal, S. (2010). The Dirichlet process, related priors and posterior asymptotics</li>
<li>[2] Ghosal, S., &amp; Van der Vaart, A. (2017). Fundamentals of nonparametric Bayesian inference</li>
</ul>

	</article>
</main>
	
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '$', right: '$', display: false},  
      ],
      throwOnError : false
    });
  });
</script>

	

	

	



  <footer class="row row-mob al-c-mob col-rev-mob sm-2-mob  jc-bt mtb-2">
  <p>
    © Copyright notice |
    <a href="https://github.com/mivinci/hugo-theme-minima" target="_blank" rel="noopener noreferrer">Minima</a> theme on
    <a href="https://gohugo.io" target="_blank" rel="noopener noreferrer">Hugo</a>
  </p>
  <p class="row gap-0_5">
    
      <a class="icon" href="https://github.com/wangchxx" title="github">
      
        <svg fill="#63636f" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
      
      </a>
    
  </p>
</footer>
</body>
</html>