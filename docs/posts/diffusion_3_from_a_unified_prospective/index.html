<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta property="og:url" content="https://wangchxx.github.io/posts/diffusion_3_from_a_unified_prospective/">
  <meta property="og:site_name" content="My Math Notes">
  <meta property="og:title" content="DL | Diffusion Models 3 - Understanding DDMP from a Unified Prospective">
  <meta property="og:description" content="Understand DDMP from VLB In the Diffusion-models-1 we have introduced the DDMP model $$ X_t|X_{t-1} = \mathcal{N}(\sqrt{1-\beta_t}X_{t-1}, \beta_t I), \quad X_t|X_0 = \mathcal{N}(\sqrt{\bar{\alpha}_t} X_0, (1-\bar{\alpha}_{t})I), $$ with $X_{0}\sim q_{0} = p_{data}$, $\alpha_{t} = 1-\beta_{t}$ and $\bar{\alpha}_{t} = \prod_{s=1}^t (1-\beta_{s})$.
The loss function is derived by minimizing the negative log-likelihood $-\log p_{\theta}(X_{0})$ with a variational lower bound $L_{VLB}$ $$ L_{VLB} = \mathbb{E}_{q}\left[ \log \frac{q(x_{1:T}|x_{0})}{p_{\theta}(x_{0:T})} \right] = \sum_{t=0}^T L_{t} $$ with $$ L_{t} = D_{KL}(q(x_{t}|x_{t&#43;1},x_{0})\| p_{\theta}(x_{t}|x_{t&#43;1})),\; 1\leq t\leq T-1. $$ Since the conditional distribution $X_{t}|(X_{t-1}, X_{0})$ is Gaussian">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-10-23T10:52:59+02:00">
    <meta property="article:modified_time" content="2024-10-23T10:52:59+02:00">
    <meta property="article:tag" content="Markdown">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/diffusion_4_conditional_df/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/diffusion_2_preliminary_sde/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/diffusion_1_ddmp/">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="DL | Diffusion Models 3 - Understanding DDMP from a Unified Prospective">
  <meta name="twitter:description" content="Understand DDMP from VLB In the Diffusion-models-1 we have introduced the DDMP model $$ X_t|X_{t-1} = \mathcal{N}(\sqrt{1-\beta_t}X_{t-1}, \beta_t I), \quad X_t|X_0 = \mathcal{N}(\sqrt{\bar{\alpha}_t} X_0, (1-\bar{\alpha}_{t})I), $$ with $X_{0}\sim q_{0} = p_{data}$, $\alpha_{t} = 1-\beta_{t}$ and $\bar{\alpha}_{t} = \prod_{s=1}^t (1-\beta_{s})$.
The loss function is derived by minimizing the negative log-likelihood $-\log p_{\theta}(X_{0})$ with a variational lower bound $L_{VLB}$ $$ L_{VLB} = \mathbb{E}_{q}\left[ \log \frac{q(x_{1:T}|x_{0})}{p_{\theta}(x_{0:T})} \right] = \sum_{t=0}^T L_{t} $$ with $$ L_{t} = D_{KL}(q(x_{t}|x_{t&#43;1},x_{0})\| p_{\theta}(x_{t}|x_{t&#43;1})),\; 1\leq t\leq T-1. $$ Since the conditional distribution $X_{t}|(X_{t-1}, X_{0})$ is Gaussian">

  
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#262d33">
  <title>
    
    My Math Notes - DL | Diffusion Models 3 - Understanding DDMP from a Unified Prospective
    
  </title>
  
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;600;700&display=swap"
    rel="stylesheet">
  <link rel="stylesheet" href="https://unpkg.com/normalize.css">
  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="/css/md.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="/css/syntax.css" />
  <script src="/js/main.js"></script>
</head>
<script>
  try {
    if (!('theme' in localStorage)) {
      localStorage.theme = window.matchMedia('(prefer-color-scheme: dark)').matches ? 'dark' : 'light';
    }
    document.querySelector('html').classList.add(localStorage.theme);
  } catch (e) {
    console.error(e);
  }
</script>
<body>
  <header>
  <h1 class="row gap-1">
    <div id="theme-switcher" class="btn lg-1"></div>
    My Math Notes
  </h1>
  <nav class="row gap-1">
  
    <a href="/">Home</a>
  
    <a href="/categories">Categories</a>
  
    <a href="/series">Series</a>
  
    <a href="/about">About</a>
  
  </nav>
  <hr>
</header>
  
  
<main>
	<h1>DL | Diffusion Models 3 - Understanding DDMP from a Unified Prospective</h1>
	<div class="sm-1 mtb-1">
		Posted at &mdash; Oct 23, 2024
		
	</div>
	<p></p>
	<article class="md">
		<h2 id="understand-ddmp-from-vlb">Understand DDMP from VLB</h2>
<p>In the <a href="https://wangchxx.github.io/posts/diffusion_2_preliminary_sde/">Diffusion-models-1</a> we have introduced the DDMP model
</p>
$$
X_t|X_{t-1} = \mathcal{N}(\sqrt{1-\beta_t}X_{t-1}, \beta_t I), \quad X_t|X_0 = \mathcal{N}(\sqrt{\bar{\alpha}_t} X_0, (1-\bar{\alpha}_{t})I),
$$<p>
with $X_{0}\sim q_{0} = p_{data}$, $\alpha_{t} = 1-\beta_{t}$ and $\bar{\alpha}_{t} = \prod_{s=1}^t (1-\beta_{s})$.</p>
<p>The loss function is derived by minimizing the negative log-likelihood $-\log p_{\theta}(X_{0})$ with a variational lower bound $L_{VLB}$
</p>
$$
L_{VLB} = \mathbb{E}_{q}\left[ \log  \frac{q(x_{1:T}|x_{0})}{p_{\theta}(x_{0:T})}  \right]  = \sum_{t=0}^T L_{t}
$$<p>
with
</p>
$$
L_{t} = D_{KL}(q(x_{t}|x_{t+1},x_{0})\| p_{\theta}(x_{t}|x_{t+1})),\; 1\leq t\leq T-1. 
$$<p>
Since the conditional distribution $X_{t}|(X_{t-1}, X_{0})$ is Gaussian
</p>
$$
q(x_{t-1}|x_t, x_0) = \mathcal{N}(\tilde{\mu}_t(x_t, x_0), \tilde{\beta}_t I),
$$<p>
where
</p>
$$
\begin{equation}
\tilde{\mu}_t(x_t, x_0) := \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t +\frac{\bar{\alpha}_{t-1}\beta_t}{1-\bar{\alpha}_t}x_0;\quad \tilde{\beta}_t:= \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t.
\end{equation}
$$<p>
If we model the conditional distribution $X_{t-1}|X_{t}$ as a Gaussian $X_{t-1}|X_{t} \sim \mathcal{N}(\mu_\theta(X_t, t), \Sigma_\theta(X_t, t))$, the loss function is basically the KL-divergence between two Gaussian distributions, which has the closed form
</p>
$$
\begin{equation}
L_{t} = \frac{1}{2\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \|\tilde{\mu}(x_{t},x_{0}) - \mu_{\theta}(x_{t},t)\|^2 + C.
\end{equation}
$$<p>
Because $q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha})I)$ we can express $x_{t}|x_{0}$ as $x_{t}(x_{0}, \epsilon) = \sqrt{ \bar{\alpha}_{t} }x_{0} + \sqrt{ 1-\bar{\alpha}_{t} }\epsilon$ for $\epsilon\sim\mathcal{N}(0, I)$, therefore we can rewrite $\tilde{\mu}_{t}(x_{t}, x_{0})$ as
</p>
$$
\tilde{\mu}_{t} = \tilde{\mu}(x_{0}, \epsilon) = \frac{1}{\sqrt{ \alpha_{t} }}\left( x_{t} - \frac{1-\alpha_{t}}{\sqrt{ 1-\bar{\alpha}_{t} }} \epsilon \right).
$$<p>
Similarly, we can also model $\mu_{\theta}(x_{t}, t)$ as
</p>
$$
\mu_{\theta}(x_{t}, t) = \frac{1}{\sqrt{ \alpha_{t} }}\left( x_{t} - \frac{1-\alpha_{t}}{\sqrt{ 1-\bar{\alpha}_{t} }} \epsilon_{\theta}(x_{t,t}) \right),
$$<p>
where $\epsilon_{\theta}$ is an approximation of the noise $\epsilon$. Overall, the loss function $L_{t}$ becomes
$$</p>
<p>L_{t} = \mathbb{E}<em>{x</em>{0}, \epsilon} \left[\frac{(1-\alpha_{t})^2}{2 \alpha_{t}(1-\bar{\alpha}<em>{t})|\Sigma</em>{\theta}(x_{t},t)|<em>{2}^2} | \epsilon - \epsilon</em>{\theta}(\sqrt{ \bar{\alpha}<em>{t} }x</em>{0} + \sqrt{ 1-\bar{\alpha}_{t} }\epsilon,t)|^2 \right] + C.
$$</p>
<h2 id="understand-ddmp-from-score-matching">Understand DDMP from score matching</h2>
<p>Consider the random variable
</p>
$$
Y = X + \sigma Z, \quad X\sim P_{X}, \quad Z\sim \mathcal{N}(0,I).
$$<p>
By definition, $p_{Y|X}$ is Gaussian, but in general $p_{X|Y}$ is not Gaussian since we did not assume $p_{X}$ is Gaussian. But $p_{X|Y}$ is approximately Gaussian in the limit of $\sigma\to 0$.
</p>
$$
p_{X|Y}(x|y) \approx \mathcal{N}(y + \sigma^2\nabla \log p_{Y}(y), \sigma^2 I).
$$<p>
See proof in [[#^08dcf7|Approximation Gaussian]]. And for $Y = \gamma X + \sigma Z$ with $\gamma \neq 0$, then in the limit of $\sigma\to 0$,<br>
</p>
$$
p_{X|Y}(x|y) \approx \mathcal{N}\left( \frac{1}{\gamma}(y + \sigma^2\nabla \log p_{Y}(y)), \frac{\sigma^2}{\gamma^2} I \right).
$$<p>
In the setting of DDMP, we know the forward process is
</p>
$$
X_t|X_{t-1} = \mathcal{N}(\sqrt{1-\beta_t}X_{t-1}, \beta_t I), \quad X_t|X_0 = \mathcal{N}(\sqrt{\bar{\alpha}_t} X_0, (1-\bar{\alpha})I), \quad \bar{\alpha}_{t} = \prod_{s=1}^t(1-\beta_{s}).
$$<p>
If $\beta_{t}$ is small enough, approximately the backward process is
</p>
$$
p(x_{t-1}|x_{t}) \approx \mathcal{N}(\mu(x_{t},t), \beta_{t}I),
$$<p>
with $\mu(x_{t},t) = \frac{1}{\sqrt{ 1-\beta_{t} }}(x_{t} - \beta_{t}\nabla \log p_{t}(x_{t}))$. Similarly, we can a model $p_{\theta}(x_{t-1}|x_{t})$ to learn it
</p>
$$
p_{\theta}(x_{t-1}|x_{t}) = \mathcal{N}(\mu_{\theta}(x_{t}, t), \Sigma_{\theta}(x_{t},t)), \quad \mu_{\theta}(x_{t},t) = \frac{1}{\sqrt{ 1-\beta_{t} }}(x_{t} - \beta_{t}s_{\theta}(x_{t},t))
$$<p>
Essentially, we are using $s_{\theta}$ to learn the score function $\nabla_{x}\log p$. Naturally, we can think about using the loss function MSE of means or KL-divergence. There is not a big difference between them, so we take KL-divergence as an example
</p>
$$
\begin{aligned}
\mathcal{L}(\theta) &= \sum_{t=1}^T \lambda_{t} KL(p(x_{t-1}|x_{t}\|p_{\theta}(x_{t-1}|x_{t}))) \\
    &= \sum_{t=1}^T\lambda_{t} \frac{1}{2\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \|\mu(x_{t},x_{0}) - \mu_{\theta}(x_{t},t)\|^2 + C\\
    &= \sum_{t=1}^T\lambda_{t} \frac{(1-\alpha_{t})^2}{2 \alpha_{t}\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \| \nabla_{x}\log p_{t}(x_{t}) - s_{\theta}(x_{t},t)\|^2 + C
\end{aligned}
$$<p>
Here we see the form of score matching. However, the score function $\nabla_{x}\log p_{t}$ is inaccessible, so we cannot compute the above loss function. Recall that $X_{t}|X_{0}$ is a known Gaussian,  thus we can try to use $\nabla_{x}\log p_{t|0}(x_{t}|x_{0})$ in the loss function. Moreover, for Gaussian distribution $Y\sim \mathcal{N}(\mu, \sigma^2 I)$, the score function can be easily computed as
</p>
$$
\nabla_{y}\log p_{Y}(Y) \overset{\mathcal{D}}{=} - \frac{\epsilon}{\sigma}, \quad \epsilon \sim \mathcal{N}(0,I).
$$<p>
Like what we have done for DDPM VLB loss function, we use $\epsilon_{\theta} = -\sqrt{ 1-\bar{\alpha}_{t} }  s_{\theta}$. Now we can rewrite the loss function as
</p>
$$
\begin{aligned}
\mathcal{L}(\theta) &= \sum_{t=1}^T\lambda_{t} \frac{(1-\alpha_{t})^2}{2 \alpha_{t}\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \| \nabla_{x}\log p_{t}(x_{t}) - s_{\theta}(x_{t},t)\|^2 + C \\
    &= \sum_{t=1}^T\lambda_{t} \frac{(1-\alpha_{t})^2}{2 \alpha_{t}\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \mathbb{E}_{x_{0}\sim p_{0}} \left[\| \nabla_{x}\log p_{t|0}(x_{t}|x_{0}) - s_{\theta}(x_{t},t)\|^2 \right] + C \\
    &= \sum_{t=1}^T\lambda_{t} \frac{(1-\alpha_{t})^2}{2 \alpha_{t}\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \mathbb{E}_{x_{0}\sim p_{0}, \epsilon\sim\mathcal{N}(0,I)} \left[\| - \frac{\epsilon}{\sqrt{ 1-\bar{\alpha}_{t} }} - s_{\theta}(x_{t},t)\|^2 \right] + C \\
    &= \sum_{t=1}^T\lambda_{t} \frac{(1-\alpha_{t})^2}{2 \alpha_{t} (1-\bar{\alpha}_{t})\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \mathbb{E}_{x_{0}\sim p_{0}, \epsilon\sim\mathcal{N}(0,I)} \left[\| \epsilon- \epsilon_{\theta}(\sqrt{ \bar{\alpha}_{t} } x_{0} + \sqrt{ 1-\bar{\alpha}_{t} }\epsilon,t)\|^2 \right] + C, \\
\end{aligned}
$$<p>
which is exactly the VLB loss function of DDMP.</p>
<h2 id="ddmp-is-vp-sde-with-sde-sampling">DDMP is VP SDE with SDE sampling</h2>
<p>DDMP forward process in the limit $\beta_{t}\to 0$
</p>
$$
X_{t+1} = \sqrt{ 1-\beta_{t} } X_{t} + \sqrt{ \beta_{t} } Z_{t} \approx \left( 1-\frac{\beta_{t}}{2} \right)X_{t} + \sqrt{ \beta_{t} }Z_{t}.
$$<p>
Therefore when the timestep goes to zero, the DDMP forward process converges to the following VP SDE:
</p>
$$
dX_{t} = -\frac{1}{2}\beta_{t} X_{t} dt + \sqrt{ \beta_{t} }dW_{t}.
$$<p>DDMP sampling process is
</p>
$$
\bar{X}_{t-1} = \frac{1}{\sqrt{ 1-\beta_{t} }}\left( \bar{X}_{t} - \frac{\beta_{t}}{\sqrt{ 1-\bar{\alpha}_{t} }} \epsilon_{\theta}(\bar{X}_{t},t) \right) + \sigma_{t} Z_{t},
$$<p>
where $\sigma_{t}$ a unlearned constant used in $\Sigma_{\theta}(x_{t},t) = \sigma_{t}^2I$. We set $\sigma_{t}^2 = \beta_{t}$. If $\beta_{t}$ is slowly varying and $\beta_{t}\to 0$, we have
</p>
$$
\bar{\alpha}_{t} = \prod_{s=0}^T (1-\beta_{s}) \approx \prod_{s=0}^T e^{-\beta_{s}} \approx \exp\left( -\int_{0}^t \beta_{s} ds\right).
$$<p>
We the approximate the discrete DDMP sampling process as
</p>
$$
\bar{X}_{t-1} \approx \left( 1+\frac{\beta_{t}}{2} \right)\bar{X}_{t} - \frac{\beta_{t}}{\sqrt{ 1- \exp\left( -\int_{0}^t \beta_{s} ds\right)}} \epsilon_{\theta}(\bar{X}_{t}, t) + \sqrt{ \beta_{t} }Z_{t}
$$<p>
This implies the reverse-time SDE
</p>
$$
d\bar{X}_{t} = \left( -\frac{\beta_{t}}{2} \bar{X}_{t}  + \frac{\beta_{t}}{\sqrt{ 1- \exp\left( -\int_{0}^t \beta_{s} ds\right)}} \epsilon_{\theta}(\bar{X}_{t}, t)  \right)dt + \sqrt{ \beta_{t} } d \bar{W}_{t}.
$$<p>By the Anderson&rsquo;s theorem, we can get the reverse-time is
</p>
$$
\begin{aligned}
d \bar{X}_{t} &= \left( -\frac{\beta_{t}}{2}\bar{X}_{t} - \beta_{t}\nabla_{x}\log p_{t}(\bar{X}_{t}) \right)dt + \sqrt{ \beta_{t} }d\bar{W}_{t} \\
    &\approx \left( -\frac{\beta_{t}}{2}\bar{X}_{t} - \beta_{t}\nabla_{x}\log p_{t|0}(\bar{X}_{t}) \right)dt + \sqrt{ \beta_{t} }d\bar{W}_{t} \\
    &\approx \left( -\frac{\beta_{t}}{2}\bar{X}_{t} + \beta_{t} \frac{\epsilon_{\theta}(\bar{X}_{t},t)}{\sigma_{t}} \right)dt + \sqrt{ \beta_{t} }d\bar{W}_{t} ;\quad \text{by } \nabla_{x}\log p_{t|0}(x) = - \frac{\epsilon}{\sigma_{t}},\\
\end{aligned}
$$<p>
where $\sigma_{t}^2$ is the variance of $X_{t}|X_{0}$. We have stated in <a href="https://wangchxx.github.io/posts/diffusion_2_preliminary_sde/">Diffusion-models-2#Examples with O-U process#VP SDE</a> that the VP SDE has the property that $X_{t}|X_{0}$ follows a Gaussian distribution with
</p>
$$
X_{t}|X_{0} \sim \mathcal{N}\left(\exp\left( -\frac{1}{2}\int_{0}^t\beta_{s} ds\right) X_{0}, \left(1- \exp\left( -\int_{0}^t \beta_{s} ds\right)\right)I\right).
$$<p>
Therefore substituting $\sigma_{t} = \sqrt{ 1- \exp\left( -\int_{0}^t \beta_{s} ds\right)}$ we get
</p>
$$
d \bar{X}_{t} \approx \left( -\frac{\beta_{t}}{2}\bar{X}_{t} + \beta_{t} \frac{\epsilon_{\theta}(\bar{X}_{t},t)}{\sqrt{ 1- \exp\left( -\int_{0}^t \beta_{s} ds\right)}} \right)dt + \sqrt{ \beta_{t} }d\bar{W}_{t}
$$<p>
Therefore the reverse-time SDE derived from the sampling process of DDMP is consistent to the reverse-time SDE of VP SDE.</p>
<h2 id="ddim-is-vp-sde-with-ode-sampling">DDIM is VP SDE with ODE sampling</h2>
<p><img src="ddim_graph.png" alt=""></p>
<p>The loss function of DDPM depends on $q(x_{t}|x_{0})$, but not directly on the joint $q(x_{1:T}|x_{0})$. Thus DDIM explore alternative forward process that non-Markovian but with the same marginals $q(x_{t}|x_{0})$. Specifically,
</p>
$$
\begin{aligned}
q_{\rho}(x_{1},\dots x_{T}|x_{0}) &= q_{\rho}(x_{T}|x_{0}) \prod_{t=1}^{T-1}q_{\rho}(x_{t}|x_{t+1}, x_{0}) \\
q_{\rho}(x_{T}|x_{0}) &= \mathcal{N}(\sqrt{ \bar{\alpha}_{t} }x_{0}, (1-\bar{\alpha}_{T})I) \\
q_{\rho}(x_{t-1}|x_{t}, x_{0}) &= \mathcal{N}\left( \sqrt{\bar{\alpha}_{t-1} }x_{0} +  \sqrt{ 1-\bar{\alpha}_{t-1} - \rho_{t}^2 }\frac{x_{t}- \sqrt{ \bar{\alpha}_{t}}x_{0}}{\sqrt{ 1-\bar{\alpha}_{t} }} , \rho_{t}^2 I\right)
\end{aligned}
$$<p>
The transition kernel $X_{0}\mapsto X_{T}$ and $(X_{0},X_{t+1})\mapsto X_{t}$ are chosen to make sure that the marginals of DDIM match the marginals of DDMP:
</p>
$$
q(x_{t}|x_{0}) = \mathcal{N}(\sqrt{ \bar{\alpha}_{t} }x_{0}, (1-\bar{\alpha}_{t})I).
$$<p>
We can prove it by induction:
</p>
$$
\begin{align}
q_{\rho}(x_{t+1}|x_{0}) &= \mathcal{N}(\sqrt{ \bar{\alpha}_{t+1} }x_{0}, (1-\bar{\alpha}_{t+1})I)  \\
q_{\rho}(x_{t}|x_{0}) &= \int q_{\rho}(x_{t+1}|x_{0}) q_{\rho}(x_{t}|x_{t+1}, x_{0}) dx_{t+1}.
\end{align}
$$<p>
Since both $q_{\rho}(x_{t+1}|x_{0})$ and $q_{\rho}(x_{t}|x_{t+1}, x_{0})$ are Gaussian, we have that $q(x_{t}|x_{0})$ is Gaussian with mean $\sqrt{ \bar{\alpha}_{t} }x_{0}$ and variance $(1-\bar{\alpha}_{t})I$.</p>
<p>Since DDIM and DDPM have the same conditional marginals, their conditional and unconditional score functions are the same. Given a DDMP, it is unnecessary to retrain it. The contribution of DDIM is to find a faster sampling method.</p>
<p>The DDIM sampling is done with
</p>
$$
p_{\theta}(x_{t-1}|x_{t}) = q_{\rho}(x_{t-1}|x_{t}, x_{0}).
$$<p>
However, the $x_{0}$ is unknown. But it has an unbiased estimator through
</p>
$$
\begin{aligned}
X_{t} &= \sqrt{ \bar{\alpha}_{t} }X_{0} + \sqrt{ 1-\bar{\alpha}_{t} }\epsilon \\
\hat{X}_{0} &= \mathbb{E}[X_{0}|X_{t}] \approx  \frac{X_{t} - \sqrt{ 1-\bar{\alpha}_{t}}\epsilon_{\theta}(X_{t}, t)}{\sqrt{ \bar{\alpha}_{t} }}
\end{aligned},
$$<p>
where $\epsilon_{\theta}$ is our trained model that predicts the added noise from $X_{t}$. Then the sampling process is given via:
</p>
$$
\begin{align}
\bar{X}_{t-1} &= \sqrt{ \bar{\alpha}_{t-1} }\hat{X}_{0} + \sqrt{ 1-\bar{\alpha}_{t-1} - \rho_{t}^2 } \frac{\bar{X}_{t} - \sqrt{ \bar{\alpha}_{t} }X_{0}}{\sqrt{ 1-\bar{\alpha}_{t} }} + \rho_{t} Z_{t} \\ 
    &= \sqrt{ \bar{\alpha}_{t-1} }\hat{X}_{0} + \sqrt{ 1-\bar{\alpha}_{t-1} - \rho_{t}^2 } \frac{\bar{X}_{t} - \sqrt{ \bar{\alpha}_{t} }\hat{X}_{0}}{\sqrt{ 1-\bar{\alpha}_{t} }} + \rho_{t} Z_{t} \\ 
    &= \sqrt{ \bar{\alpha}_{t-1} }\hat{X}_{0} + \sqrt{ 1-\bar{\alpha}_{t-1} - \rho_{t}^2 } \epsilon_{\theta}(\bar{X}_{t}, t)+ \rho_{t} Z_{t} \\ 
\end{align}
$$<p>Recall that in DDMP, $q(x_{t-1}|x_{t}, x_{0}) = \mathcal{N}(\tilde{\mu}_{t}(x_{t},x_{0}), \tilde{\beta}_{t}I)$, therefore we have
</p>
$$
\tilde{\beta}_{t} = \rho_{t}^2 = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\beta_{t}.
$$<p>
It other words, when $\rho_{t}^2 =  \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\beta_{t}$ the sampling process becomes DDMP. The special case of $\rho_{t} = 0$ makes the sampling process deterministic, which is named the DDIM. If we rewrite the above sampling process in one-step form, we have
</p>
$$
\begin{align}
\bar{X}_{t-1} &= \frac{1}{\sqrt{ 1-\beta_{t} }} \bar{X}_{t} - \left( \frac{\sqrt{ 1-\bar{\alpha}_{t} }}{\sqrt{1-\beta_{t}}} - \sqrt{ 1-\bar{\alpha}_{t-1} } \right)\epsilon_{\theta}(\bar{X}_{t},t) \\
    &= \frac{1}{\sqrt{ 1-\beta_{t} }} \bar{X}_{t} - \left( \frac{\sqrt{ 1-\bar{\alpha}_{t} }}{\sqrt{1-\beta_{t}}} - \sqrt{ 1- \frac{\bar{\alpha}_{t}}{1-\beta_{t}} } \right)\epsilon_{\theta}(\bar{X}_{t},t)
\end{align}
$$<p>If $\beta_{t}$ is slowly varying and $\beta_{t}\to 0$, we have
</p>
$$
\begin{align}
\bar{X}_{t-1} &\approx \left( 1 + \frac{\beta_{t}}{2} \right)\bar{X}_{t} - \frac{\beta_{t}}{2\sqrt{ 1-\bar{\alpha}_{t} }}\epsilon_{\theta}(\bar{X}_{t},t)  \\
    &\approx \left( 1+\frac{\beta_{t}}{2} \right)\bar{X}_{t} - \frac{\beta_{t}}{2\sqrt{ 1- \exp\left( -\int_{0}^t \beta_{s} ds\right)}} \epsilon_{\theta}(\bar{X}_{t}, t)  \\
    &\approx  \left( 1+\frac{\beta_{t}}{2} \right)\bar{X}_{t} + \frac{\beta_{t}}{2}\nabla_{x}\log p_{t|0}(\bar{X}_{t})
\end{align}
$$<p>
It agrees with the reverse-time ODE of VP SDE:
</p>
$$
dX_{t} = - \frac{\beta_{t}}{2} X_{t} dt + \sqrt{ \beta_{t} }dW_{t}.
$$<h2 id="noise-conditional-score-network-ncsn-is-ve-sde-with-sde-sampling">Noise conditional score network (NCSN) is VE SDE with SDE sampling</h2>
<p>The forward-time process of NCSN is
</p>
$$
X_{t} = X_{t-1} + \sqrt{ \sigma_{t}^2 - \sigma_{t-1}^2} Z_{t},\quad X_{0}\sim p_{data}.
$$<p>
Since $\sqrt{ \sigma_{t}^2 - \sigma_{t-1}^2} = \sqrt{\frac{ \sigma_{t}^2 - \sigma_{t-1}^2}{\Delta_{t}}} \sqrt{ \Delta_{t} }$, it converges to
</p>
$$
dX_{t} = \sqrt{ \frac{d(\sigma_{t}^2)}{dt} }dW_{t}.
$$<p>
The sampling process of NCSN is defined as
</p>
$$
\begin{aligned}
\bar{X}_{t-1} &= \bar{X}_{t} +(\sigma_{t}^2 - \sigma_{t-1}^2)s_{\theta} + \sqrt{ \sigma_{t}^2 - \sigma_{t-1}^2}  Z_{t} \\
&\approx \bar{X}_{t} +(\sigma_{t}^2 - \sigma_{t-1}^2)\nabla_{x}\log p_{t}(\bar{X}_{t}) + \sqrt{ \sigma_{t}^2 - \sigma_{t-1}^2}  Z_{t} \\
\end{aligned}
$$<p>
It can be written as
</p>
$$
d\bar{X}_{t} = - \frac{d(\sigma_{t}^2)}{dt} \nabla_{x}\log p_{t}(\bar{X}_{t}) dt + \sqrt{\frac{d(\sigma_{t}^2)}{dt} }d\bar{W}_{t},
$$<p>
which is the reverse-time SDE of VE SDE.</p>
<h2 id="supplementary">Supplementary</h2>
<h3 id="tweedies-formula">Tweedie&rsquo;s formula</h3>
<p>Consider the random variable
</p>
$$
Y = X + \sigma Z, \quad X\sim p_{X}, \quad Z\sim\mathcal{N}(0,I),
$$<p>
where $p_{X}$ is not necessarily Gaussian, then
</p>
$$
\begin{align}
\mathbb{E}[X|Y] &= Y + \sigma^2 \nabla_{y}\log p_{y}(Y) \\
\mathrm{Var}(X|Y) &= \sigma^2I + \sigma^4 \nabla_{y}^2\log p_{y}(Y).
\end{align}
$$<p>
Easily, we can see that if $Y = \gamma X+\sigma Z$, $\gamma\neq 0$, then we have
</p>
$$
\begin{align}
\mathbb{E}[X|Y] &= \frac{1}{\gamma}(Y + \sigma^2 \nabla_{y}\log p_{y}(Y)) \\ \\
\mathrm{Var}(X|Y) &= \frac{\sigma^2}{\gamma^2}(I + \sigma^2\nabla_{y}^2\log p_{y}(Y)).
\end{align}
$$<p>
See proof <a href="https://efron.ckirby.su.domains/papers/2011TweediesFormula.pdf">here</a></p>
<h3 id="approximation-gaussian">Approximation Gaussian</h3>
<p>^08dcf7</p>
$$
\begin{aligned}
p_{X|Y}(x) &= \frac{p_{Y|X}(y) p_{X}(x)}{p_{Y}(y)} \\
    &= \frac{p_{Y|X}(y) p_{X}(x)}{\int p_{Y|X}(y)p_{X}(x)dx} \\ 
\end{aligned}
$$<p>
Note that $p_{Y|X}$ is Gaussian and
</p>
$$
\begin{aligned}
p_{Y|X} p_{X}(x) &= \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|y-x\|^2 \right) p_{X}(x) \\
    &= \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|y-x\|^2 \right) (p_{X}(y) + \langle \nabla p_{X}(y), x-y \rangle + O(\|x-y\|^2) )
\end{aligned}
$$<p>
Then the denominator can be calculated as $\text{denom} = p_{X}(y)  + O(\sigma^2)$ since
</p>
$$
\begin{aligned}
\int \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|y-x\|^2 \right) p_{X}(y) dx &= p_{X}(y) \\
\int \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|y-x\|^2 \right) (x-y) dx &= 0 \\
\int \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|y-x\|^2 \right) O(\|x-y\|^2) &= O(\sigma^2).
\end{aligned}
$$<p>It follows that
</p>
$$
\begin{aligned}
p_{X|Y}(x) &= \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|y-x\|^2 \right)  \frac{p_{X}(y) + \langle \nabla p_{X}(y), x-y \rangle + O(\|x-y\|^2)}{p_{X}(y)+ O(\sigma^2)} \\
 &=  \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|y-x\|^2 \right) (1 + \langle \nabla\log p_{X}(y), x-y \rangle + \text{h.o.t.} ) \\
 &=  \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|x-y\|^2 \right)  \exp(\langle \nabla \log p_{X}(y), x-y \rangle ) + \text{h.o.t.}\\
 &=  \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|x-y - \sigma^2\nabla\log p_{Y}(y)\|^2 + \text{h.o.t.} \right) + \text{h.o.t.} \\ 
 &\approx \mathcal{N}(y+\sigma^2\nabla\log p_{Y}(y), \sigma^2 I).
\end{aligned}
$$
	</article>
</main>
	
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '$', right: '$', display: false},  
      ],
      throwOnError : false
    });
  });
</script>

	

	

	



  <footer class="row row-mob al-c-mob col-rev-mob sm-2-mob  jc-bt mtb-2">
  <p>
    Â© Copyright notice |
    <a href="https://github.com/mivinci/hugo-theme-minima" target="_blank" rel="noopener noreferrer">Minima</a> theme on
    <a href="https://gohugo.io" target="_blank" rel="noopener noreferrer">Hugo</a>
  </p>
  <p class="row gap-0_5">
    
      <a class="icon" href="https://github.com/wangchxx" title="github">
      
        <svg fill="#63636f" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
      
      </a>
    
  </p>
</footer>
</body>
</html>