<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Reading | Posterior consistency of Gaussian process prior for nonparametric binary regression | My Notes</title>
<meta name="keywords" content="">
<meta name="description" content="
Ghosal, S., &amp; Roy, A. (2006). Posterior consistency of Gaussian process prior for nonparametric binary regression. The Annals of Statistics, 34(5). https://doi.org/10.1214/009053606000000795

The previous article discussed the methodology of GP priors for binary classification. This article would show the consistency results.
1. Problem setup
$$ 
 p(x) = P(Y=1|x) = H(\eta(x)).
$$
A GP prior with mean function $\mu(x)$ and covariance kernle $\sigma(x,x&#39;)$ is put on the function $\eta$. The covariance kernel is assumed to be of the form
">
<meta name="author" content="Chaohua Wang">
<link rel="canonical" href="https://wangchxx.github.io/posts/reading/gpconsistency_for_binary_classification/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://wangchxx.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://wangchxx.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://wangchxx.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://wangchxx.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://wangchxx.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://wangchxx.github.io/posts/reading/gpconsistency_for_binary_classification/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '$', right: '$', display: false},  
      ],
      throwOnError : false
    });
  });
</script>



<meta property="og:title" content="Reading | Posterior consistency of Gaussian process prior for nonparametric binary regression">
<meta property="og:description" content="
Ghosal, S., &amp; Roy, A. (2006). Posterior consistency of Gaussian process prior for nonparametric binary regression. The Annals of Statistics, 34(5). https://doi.org/10.1214/009053606000000795

The previous article discussed the methodology of GP priors for binary classification. This article would show the consistency results.
1. Problem setup
$$ 
 p(x) = P(Y=1|x) = H(\eta(x)).
$$
A GP prior with mean function $\mu(x)$ and covariance kernle $\sigma(x,x&#39;)$ is put on the function $\eta$. The covariance kernel is assumed to be of the form
">
<meta property="og:type" content="article">
<meta property="og:url" content="https://wangchxx.github.io/posts/reading/gpconsistency_for_binary_classification/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-08-07T18:24:27+02:00">
<meta property="article:modified_time" content="2021-08-07T18:24:27+02:00">


<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reading | Posterior consistency of Gaussian process prior for nonparametric binary regression">
<meta name="twitter:description" content="
Ghosal, S., &amp; Roy, A. (2006). Posterior consistency of Gaussian process prior for nonparametric binary regression. The Annals of Statistics, 34(5). https://doi.org/10.1214/009053606000000795

The previous article discussed the methodology of GP priors for binary classification. This article would show the consistency results.
1. Problem setup
$$ 
 p(x) = P(Y=1|x) = H(\eta(x)).
$$
A GP prior with mean function $\mu(x)$ and covariance kernle $\sigma(x,x&#39;)$ is put on the function $\eta$. The covariance kernel is assumed to be of the form
">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://wangchxx.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Reading | Posterior consistency of Gaussian process prior for nonparametric binary regression",
      "item": "https://wangchxx.github.io/posts/reading/gpconsistency_for_binary_classification/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Reading | Posterior consistency of Gaussian process prior for nonparametric binary regression",
  "name": "Reading | Posterior consistency of Gaussian process prior for nonparametric binary regression",
  "description": " Ghosal, S., \u0026amp; Roy, A. (2006). Posterior consistency of Gaussian process prior for nonparametric binary regression. The Annals of Statistics, 34(5). https://doi.org/10.1214/009053606000000795 The previous article discussed the methodology of GP priors for binary classification. This article would show the consistency results.\n1. Problem setup $$ p(x) = P(Y=1|x) = H(\\eta(x)). $$ A GP prior with mean function $\\mu(x)$ and covariance kernle $\\sigma(x,x')$ is put on the function $\\eta$. The covariance kernel is assumed to be of the form ",
  "keywords": [
    
  ],
  "articleBody": " Ghosal, S., \u0026 Roy, A. (2006). Posterior consistency of Gaussian process prior for nonparametric binary regression. The Annals of Statistics, 34(5). https://doi.org/10.1214/009053606000000795 The previous article discussed the methodology of GP priors for binary classification. This article would show the consistency results.\n1. Problem setup $$ p(x) = P(Y=1|x) = H(\\eta(x)). $$ A GP prior with mean function $\\mu(x)$ and covariance kernle $\\sigma(x,x')$ is put on the function $\\eta$. The covariance kernel is assumed to be of the form $$ \\sigma(x,x') = \\tau^{-1}\\sigma_0(\\lambda x, \\lambda x'), $$ where $\\sigma_0$ is a nonsingular covariance kernel and the hyper-parameters $\\tau\u003e0,\\lambda\u003e0$ play the roles of a scaling parameter and a bandwidth parameter, respectively. Let $\\tau\\sim \\Pi_\\tau, \\lambda\\sim \\Pi_\\lambda$, respectively.\nA popular method of prior construction on function is by expanding the function in a series $\\sum_j \\theta_j \\psi_j(x)$ and then putting independent $N(0,\\tau_j^2)$ priors on the coefficients. Such a prior leads to a GP prior on the function with covariance kernel $\\sigma(x,y) = \\sum_j \\tau_j^2 \\psi_j(x) \\psi_j(y)$.\n2. Main results Let $D^w \\eta = \\frac{\\partial \\eta(t_1,...,t_d)}{\\partial t_1 \\cdots \\partial t_d}$, $|w| = \\sum w_j$. Let sequences $\\lambda_n ,\\tau_n$ be s.t. $$ \\Pi_\\tau(\\tau\u003c\\tau_n) = e^{-cn} ,\\quad \\Pi_\\lambda(\\lambda\u003e\\lambda_n) = e^{-cn}, $$ for some constant $c$.\nDefine $$ \\Theta_n = \\Theta_{n,\\alpha} = \\{ p: P(x) = H(\\eta(x)), || D^2\\eta||_\\infty \u003c M_n, |w|\\leq \\alpha\\}. $$Assumptions\n(P). For every fixed $x$, the covariance function $\\sigma_0(x,\\cdot)$ has continuous partial derivatives up to order $2\\alpha+2$, where $\\alpha\u003e0$ is to be specified later.\n(C). The covariate space $\\mathcal{X}$ is a bounded subset of $\\mathbb{R}^d$.\n(T). The transformed true response function $\\eta_0 \\in \\bar{\\mathbb{H}}$, where $\\mathbb{H}$ is the RKHS of GP.\n(G). For every $b_1\u003e0, b_2\u003e0$, there exist sequences $M_n,\\tau_n, \\lambda_n$ s.t. $$ M_n^2 \\tau_n \\lambda_n^{-2} \\geq b_1 n , \\quad M_n^{d/\\alpha} \\leq b_2 n. $$ Theorem 1 Suppse $X$ is random covariate sampled from a prob dist $Q$. Supppse that assumptions (P), (C), (T) and (G) hold. Then for any $\\epsilon\u003e0$, $$ \\Pi(p: ||p - p_0||_{1,Q} \u003e\\epsilon| X^{(n)}, Y^{(n)}) \\to 0 $$ in $P_0^n$-probability. Theorem 2 Let $(x_i)$ be fixed design points, and $Q_n$ be empirical measure of the design points. Then under assumptions (P), (C), (T) and (G), for any $\\epsilon\u003e0$, $$ \\Pi(p: ||p - p_0||_{1,Q_n} \u003e\\epsilon| Y^{(n)}) \\to 0 $$ in $P_0^n$-probability. Let $\\mathcal{X}$ be one-dimensional. Let $S_{i,n} = X_{i+1,n} - x_{i,n}$ be the spacings between consecutive covariate values.\nAssumption\n(U). Given $\\delta\u003e0$, there exist a constant $K_1$ and an integer $N$ s.t., for $n\u003eN$, we have that $\\sum_{i:S_{i,n}\u003eK_1 n^{-1}} S_{i,n}\\leq \\delta$. This assumption means that the measure of the part of the design space where data is parse is small.\nTheorem 3 Suppose that the values of the covariate arise as design points satisfying assumption (U) and $\\mathcal{X}$ is a bounded interval of $\\mathbb{R}$. Assume that the prior satisfies assumption (P). The mapping $x\\mapsto \\eta_0(x)$ and the prior mean $\\mu$ are assumed to have two continuous derivatives and the covariance kernel is assumed to have continous partial derivatives up to order 6. Assume that $\\Pi_\\lambda ,\\Pi_\\tau$ are such that $\\tau_n^{-1} \\lambda_n^4 = O(n)$. Then for any $\\epsilon\u003e0$, $$ \\Pi(p: ||p - p_0||_{1} \u003e\\epsilon| Y^{(n)}) \\to 0 $$ in $P_0^n$-probability. Theorem 4 Assume that (W(t)) is a GP with continuous sample paths having mean function $\\mu$ and continuous covariance kernel $\\sigma(s,t)$. Assume that $\\mu$ and a function $w$ belong to the RKHS of the kernel $\\sigma(s,t)$. Then $$ P(\\sup_{t\\in T} |W(t) - w(t)| \u003c \\epsilon) \u003e0 \\quad \\text{for all } \\epsilon\u003e0. $$ proof W.l.o.g. we assume that $\\mu$ is the zero function. Let $\\sum_i \\sqrt{\\lambda_i} \\xi_i \\psi_i(t)$ be the Karhunen-Loeve expansion of $W(t)$, where $\\lambda_i's$ are the eigenvalues of the kernel $\\sigma(s,t)$, $\\psi_i$ are the corresponding eigenfunctions and $\\xi_i$ are independent $N(0,1)$. Represent $w(t)$ also as $\\sum_i \\sqrt{\\lambda_i} a_i \\psi_i(t)$, where $\\sum_ki \\lambda_i a_i^2 \u003c\\infty$. It follows from Mercer’s theorem that the series $\\sum_i \\sqrt{\\lambda_i} a_i \\psi_i(t)$ converges uniformly, and hence, the tail sum is uniformly small. Theorem 5 : Let $\\eta$ be a GP on $\\mathcal{X}$, a bouned subset of $\\mathbb{R}^d$. Assume that the mean function $\\mu \\in C^\\alpha(\\mathcal{X})$ and the covariance kernel $\\sigma$ has $2\\alpha+2$ mixed partial derivatives for some $\\alpha\\geq 1$. Then $\\eta$ has differentiable sample paths with mixed partial derivatives up to order $\\alpha$ and the successive derivative process $D^w \\eta$ are also Gaussian with continuous sample paths. Also the derivative processes are sub-Gaussian w.r.t. a constant multiple of the Euclidean distance. Further, there exits a constant $d_w$ s.t. $$ P(\\sup_x |D^2 \\eta (x)|\u003e M)\\leq K(\\eta)e^{-d_w M^2/\\sigma_w^2(\\eta)} $$ for $w = (w_1,...,w_d)$, $w_i\\in\\{0,1,2,...,\\alpha\\}$, $|w| \\leq \\alpha$ and $\\sigma_w^2(\\eta) = \\sup_x var(D^w\\eta(x)) \u003c\\infty$, $K(\\eta)$ is a polynomial in the supremum of the $(2\\alpha+2)$-order derivatives of $\\sigma$ and the covariance functions of the derivative processes $D^w\\eta(x)$ are functions of the derivatives of the covariance kernel $\\sigma$.\nproof . 3. Necessary lemmas Without proof we state some lemmas that would be used in the proof of main theorems.\nLemma 1 (prior mass) Under assumptions (P), (C) and (G), $\\Pi(\\Theta_n^\\complement)\\leq Ae^{-cn}$ for some constants $A$ and $c$. Lemma 2 (entropy bound) $\\log N(\\epsilon, \\Theta_n, ||\\cdot||_\\infty)\\leq KM_n^{d/\\alpha} \\epsilon^{-d/\\alpha}$ for some constant $K$. Lemma 3 Let $\\nu$ be a finite meaasure on $\\mathcal{X}$ and let $\\psi_1,\\psi_2$ be measurable functions s.t. $0\\leq \\psi_1,\\psi_2\\leq M$ and $\\int |\\psi_1 - \\psi_2| d\\nu \u003e (1 + \\nu(\\mathcal{X})\\epsilon$ for some $M,\\epsilon\u003e0$. Then $$ \\nu(x: |\\psi_1(x) - \\psi_2(x)|\u003e\\epsilon) \\geq \\epsilon/M. $$ Let $\\psi_1 = p, \\psi_2 = p_0$ and $\\nu = Q_n$, we see $$ \\begin{equation} I_{n,p} = \\#\\{x_i: |p(x_i) - p_0(x_i)|\u003e\\epsilon\\} \\geq K'n \\end{equation} $$ for some $K'$. $I_{n,p}$ contains those points s.t. either $p(x) \u003c p_0(x) - \\epsilon$ or $p(x) \u003e p_0(x) + \\epsilon$, and at least one case contains more than $K'n/2$ points. So we would not lose order of the number of indices to work with one case only.\nLemma 4 (tests) Let $Y_j$ be independent Bernoulli variable with $P(Y_j = 0) = \\mu_j$. Consider testing $H_0: u = u_0$ against $H_0: u \u003e u_0 + \\epsilon$. Then there exist tests $\\phi_n$ s.t. $$ P_0(\\phi_n) \\leq e^{-m\\epsilon^2/2}, \\quad P_1(1-\\phi_n)\\leq e^{-m\\epsilon^2/2}. $$ It can be extend to any $p_1 \\in \\Theta_n$. Consider the case $x_i$ s.t. $p(x_i) \u003e p_0(x_i) + \\epsilon$ and $||p^* - p||\u003c\\epsilon/2$, then $$ p^*(x_i) - p_0(x_i) \\geq p(x_i) - p_0(x_i) - ||p - p^*||_\\infty \u003e\\epsilon /2. $$With $\\epsilon$ replaced by $\\epsilon/2$, Lemma 4 implies the existence of tests $\\phi_{n,p}$ s.t. $$ P_0(\\phi_{n,p}) \\leq e^{-m\\epsilon^2/8}, \\quad P^*(1-\\phi_{n,p})\\leq e^{-m\\epsilon^2/8}. $$ Let $p_1,...,p_N \\in\\Theta_n$ be the maximal $\\epsilon/2$-separated points. Consider $\\phi_n = \\max_{j\\leq N} \\phi_{n,p_j}$, then $$ P_0\\phi_n \\leq \\sum_j^N P_0(\\phi_{n,p_{j}}) \\leq N e^{-m\\epsilon^2/8} = exp(\\log N - m\\epsilon^2/8). $$ For any $p\\in\\Theta_n$, we can find $p_j\\in\\Theta_n$ s.t. $||p - p_j||_\\infty \u003c \\epsilon/2$. Then $$ P(1-\\phi_n) \\leq P(1 - \\phi_{n,p_j}) \\leq e^{-m\\epsilon^2/8} $$With $M_n$ satisfying assumption (G), we have $\\log N \\leq b_2' n$, then $$ P_0 \\phi_n\\leq e^{-c'n}, \\quad \\sup_{||p- p_0|| \u003e \\epsilon/2,p\\in\\Theta_n} P(1-\\phi_n) \\leq e^{-c'n} $$ for some constant $c'$.\nLemma 5 (taylor’s expansion) Let $0\u003c \\epsilon_0 \u003c 1/2$ and $\\epsilon_0 \u003c \\alpha,b \u003c 1-\\epsilon_0$. Then there exists a constant $L_{\\epsilon_0}$ s.t. $$ \\alpha(\\log \\alpha/\\beta)^m + (1-\\alpha)(\\log \\frac{1-\\alpha}{1-\\beta}) \\leq L(\\alpha-\\beta)^2. $$ 4. Proof of the main theorems Proof of Thm 1 $Y_i|X_i \\sim Bin(1, p(X_i)), X_i \\sim Q$ i.i.d. then the joint density of $(X,Y)$ is given by $$ f(x,y) = p(x)^y(1-p(x))^{1-y}. $$ The corresponding true joint density is $f_0(x,y) = p_0(x)^y (1-p_0(x))^{1-y}$. Assumption (T) implies that $p_0$ is bounded s.t. $\\epsilon_0 \u003c p_0(x) \u003c 1-\\epsilon_0$ for some $\\epsilon_0\u003c1/2$. Thus $f_0(x,y) \u003e\\epsilon_0$. Note that $||f_1 - f_2||_{1} = 2||p_1 - p_2||_{1,Q} $ and $$ KL(f_0||f) = KL(p_0||p) + \\int(1-p_0)\\log \\frac{1-p_0}{1-p} dQ. $$ First, we show that $f_0\\in KL(\\Pi)$, or equivalently, $$ \\Pi(p: KL(p_0||p) + \\int(1-p_0)\\log \\frac{1-p_0}{1-p} dQ \u003c\\epsilon) \u003e0, $$ for any $\\epsilon\u003e0$. Lemma 5 implies that it suffices to show that $$ \\Pi(p:||p-p_0||_\\infty \u003c\\epsilon)\u003e0. $$ Because $p_0 = H(\\eta)$ and the link funciton $H$ is bounded and Lipschitz continuous, thus it is enough to show that $$ \\Pi(\\eta: ||\\eta- \\eta_0||_\\infty\u003c\\epsilon)\u003e0. $$ This is an immediate result of Theorem 4. Then we check the entropy condition. Consider the set $F_n = \\{f: p\\in\\Theta_n\\}$. Lemma 2, with $M_n = bn^{\\alpha/d}$, gives $$ \\log N(\\epsilon, F_n, ||\\cdot||_\\infty)\\leq K \\epsilon^{-d/\\alpha}b^{d/\\alpha}n. $$ Chose $b\u003c(\\beta/K)^{\\alpha/d}\\epsilon$, we have that $$ \\log N(\\epsilon, F_n, ||\\cdot||_\\infty)\\leq n\\beta. $$ Finally, we check the prior mass, which is given by Lemma 1.\nProof of Thm 2 Same. 5. Discussion This paper didn’t study the contraction rate. A more general result with contraction rates was studied in\nvan der Vaart, A. W., \u0026 van Zanten, J. H. (2008). Rates of contraction of posterior distributions based on Gaussian process priors. The Annals of Statistics, 36(3), 1435–1463. https://doi.org/10/ctnd8h ",
  "wordCount" : "1423",
  "inLanguage": "en",
  "datePublished": "2021-08-07T18:24:27+02:00",
  "dateModified": "2021-08-07T18:24:27+02:00",
  "author":{
    "@type": "Person",
    "name": "Chaohua Wang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://wangchxx.github.io/posts/reading/gpconsistency_for_binary_classification/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My Notes",
    "logo": {
      "@type": "ImageObject",
      "url": "https://wangchxx.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://wangchxx.github.io/" accesskey="h" title="My Notes (Alt + H)">My Notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://wangchxx.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/series/" title="Series">
                    <span>Series</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Reading | Posterior consistency of Gaussian process prior for nonparametric binary regression
    </h1>
    <div class="post-meta"><span title='2021-08-07 18:24:27 +0200 +0200'>August 7, 2021</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Chaohua Wang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-problem-setup" aria-label="1. Problem setup">1. Problem setup</a></li>
                <li>
                    <a href="#2-main-results" aria-label="2. Main results">2. Main results</a></li>
                <li>
                    <a href="#3-necessary-lemmas" aria-label="3. Necessary lemmas">3. Necessary lemmas</a></li>
                <li>
                    <a href="#4-proof-of-the-main-theorems" aria-label="4. Proof of the main theorems">4. Proof of the main theorems</a></li>
                <li>
                    <a href="#5-discussion" aria-label="5. Discussion">5. Discussion</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><ul>
<li>Ghosal, S., &amp; Roy, A. (2006). Posterior consistency of Gaussian process prior for nonparametric binary regression. The Annals of Statistics, 34(5). <a href="https://doi.org/10.1214/009053606000000795">https://doi.org/10.1214/009053606000000795</a></li>
</ul>
<p>The previous article discussed the methodology of GP priors for binary classification. This article would show the consistency results.</p>
<h2 id="1-problem-setup">1. Problem setup<a hidden class="anchor" aria-hidden="true" href="#1-problem-setup">#</a></h2>
$$ 
 p(x) = P(Y=1|x) = H(\eta(x)).
$$<p>
A GP prior with mean function $\mu(x)$ and covariance kernle $\sigma(x,x')$ is put on the function $\eta$. The covariance kernel is assumed to be of the form
</p>
$$ 
 \sigma(x,x') = \tau^{-1}\sigma_0(\lambda x, \lambda x'),
$$<p>
where $\sigma_0$ is a nonsingular covariance kernel and the hyper-parameters $\tau>0,\lambda>0$ play the roles of a scaling parameter and a bandwidth parameter, respectively. Let $\tau\sim \Pi_\tau, \lambda\sim \Pi_\lambda$, respectively.</p>
<p>A popular method of prior construction on function is by expanding the function in a series $\sum_j \theta_j \psi_j(x)$ and then putting independent $N(0,\tau_j^2)$ priors on the coefficients. Such a prior leads to a GP prior on the function with covariance kernel $\sigma(x,y) = \sum_j \tau_j^2 \psi_j(x) \psi_j(y)$.</p>
<h2 id="2-main-results">2. Main results<a hidden class="anchor" aria-hidden="true" href="#2-main-results">#</a></h2>
<p>Let $D^w \eta = \frac{\partial \eta(t_1,...,t_d)}{\partial t_1 \cdots \partial t_d}$, $|w| = \sum w_j$. Let sequences $\lambda_n ,\tau_n$ be s.t.
</p>
$$ 
 \Pi_\tau(\tau<\tau_n) = e^{-cn} ,\quad \Pi_\lambda(\lambda>\lambda_n) = e^{-cn},
$$<p>
for some constant $c$.</p>
<p>Define
</p>
$$ 
 \Theta_n = \Theta_{n,\alpha} = \{ p: P(x) = H(\eta(x)), || D^2\eta||_\infty < M_n, |w|\leq \alpha\}.
$$<p>Assumptions</p>
<ul>
<li>
<p>(P). For every fixed $x$, the covariance function $\sigma_0(x,\cdot)$ has continuous partial derivatives up to order $2\alpha+2$, where $\alpha>0$ is to be specified later.</p>
</li>
<li>
<p>(C). The covariate space $\mathcal{X}$ is a bounded subset of $\mathbb{R}^d$.</p>
</li>
<li>
<p>(T). The transformed true response function $\eta_0 \in \bar{\mathbb{H}}$, where $\mathbb{H}$ is the RKHS of GP.</p>
</li>
<li>
<p>(G). For every $b_1>0, b_2>0$, there exist sequences $M_n,\tau_n, \lambda_n$ s.t.
</p>
$$ 
 M_n^2 \tau_n \lambda_n^{-2} \geq b_1 n , \quad  M_n^{d/\alpha} \leq b_2 n.
$$</li>
</ul>
<dl>
<dt>Theorem 1</dt>
<dd>Suppse $X$ is random covariate sampled from a prob dist $Q$. Supppse that assumptions (P), (C), (T) and (G) hold. Then for any $\epsilon>0$,
$$ 
 \Pi(p: ||p - p_0||_{1,Q}  >\epsilon| X^{(n)}, Y^{(n)}) \to 0
$$
in $P_0^n$-probability.</dd>
<dt>Theorem 2</dt>
<dd>Let $(x_i)$ be fixed design points, and $Q_n$ be empirical measure of the design points. Then under assumptions (P), (C), (T) and (G), for any $\epsilon>0$,
$$ 
 \Pi(p: ||p - p_0||_{1,Q_n} >\epsilon| Y^{(n)}) \to 0
$$
in $P_0^n$-probability.</dd>
</dl>
<p>Let $\mathcal{X}$ be one-dimensional. Let $S_{i,n} = X_{i+1,n} - x_{i,n}$ be the spacings between consecutive covariate values.</p>
<p>Assumption</p>
<ul>
<li>(U). Given $\delta>0$, there exist a constant $K_1$ and an integer $N$ s.t., for $n>N$, we have that $\sum_{i:S_{i,n}>K_1 n^{-1}} S_{i,n}\leq \delta$.</li>
</ul>
<p>This assumption means that the measure of the part of the design space where data is parse is small.</p>
<dl>
<dt>Theorem 3</dt>
<dd>Suppose that the values of the covariate arise as design points satisfying assumption (U) and $\mathcal{X}$ is a bounded interval of $\mathbb{R}$. Assume that the prior satisfies assumption (P). The mapping $x\mapsto \eta_0(x)$ and the prior mean $\mu$ are assumed to have two continuous derivatives and the covariance kernel is assumed to have continous partial derivatives up to order 6. Assume that $\Pi_\lambda ,\Pi_\tau$ are such that $\tau_n^{-1} \lambda_n^4 = O(n)$. Then for any $\epsilon>0$,
$$ 
 \Pi(p: ||p - p_0||_{1} >\epsilon| Y^{(n)}) \to 0
$$
in $P_0^n$-probability.</dd>
<dt>Theorem 4</dt>
<dd>Assume that (W(t)) is a GP with continuous sample paths having mean  function $\mu$ and continuous covariance kernel $\sigma(s,t)$. Assume that $\mu$ and a function $w$ belong to the RKHS of the kernel $\sigma(s,t)$. Then
$$ 
 P(\sup_{t\in T} |W(t) - w(t)| < \epsilon) >0 \quad \text{for all } \epsilon>0.
$$</dd>
<dt>proof</dt>
<dd>W.l.o.g. we assume that $\mu$ is the zero function. Let $\sum_i \sqrt{\lambda_i} \xi_i \psi_i(t)$ be the Karhunen-Loeve expansion of $W(t)$, where $\lambda_i's$ are the eigenvalues of the kernel $\sigma(s,t)$, $\psi_i$ are the corresponding eigenfunctions and $\xi_i$ are independent $N(0,1)$. Represent $w(t)$ also as $\sum_i \sqrt{\lambda_i} a_i \psi_i(t)$, where $\sum_ki \lambda_i a_i^2 <\infty$. It follows from Mercer&rsquo;s theorem that the series $\sum_i \sqrt{\lambda_i} a_i \psi_i(t)$ converges uniformly, and hence, the tail sum is uniformly small.</dd>
</dl>
<p>Theorem 5
: Let $\eta$ be a GP on $\mathcal{X}$, a bouned subset of $\mathbb{R}^d$. Assume that the mean function $\mu \in C^\alpha(\mathcal{X})$ and the covariance kernel $\sigma$ has $2\alpha+2$ mixed partial derivatives for some $\alpha\geq 1$. Then $\eta$ has differentiable sample paths with mixed partial derivatives up to order $\alpha$ and the successive derivative process $D^w \eta$ are also Gaussian with continuous sample paths. Also the derivative processes are sub-Gaussian w.r.t. a constant multiple of the Euclidean distance. Further, there exits a constant $d_w$ s.t.
</p>
$$ 
  P(\sup_x |D^2 \eta (x)|> M)\leq K(\eta)e^{-d_w M^2/\sigma_w^2(\eta)}
 $$<p>
for $w = (w_1,...,w_d)$, $w_i\in\{0,1,2,...,\alpha\}$, $|w| \leq \alpha$ and $\sigma_w^2(\eta) = \sup_x var(D^w\eta(x)) <\infty$, $K(\eta)$ is a polynomial in the supremum of the $(2\alpha+2)$-order derivatives of $\sigma$ and the covariance functions of the derivative processes $D^w\eta(x)$ are functions of the derivatives of the covariance kernel $\sigma$.</p>
<dl>
<dt>proof</dt>
<dd>.</dd>
</dl>
<h2 id="3-necessary-lemmas">3. Necessary lemmas<a hidden class="anchor" aria-hidden="true" href="#3-necessary-lemmas">#</a></h2>
<p>Without proof we state some lemmas that would be used in the proof of main theorems.</p>
<dl>
<dt>Lemma 1 (prior mass)</dt>
<dd>Under assumptions (P), (C) and (G), $\Pi(\Theta_n^\complement)\leq Ae^{-cn}$ for some constants $A$ and $c$.</dd>
<dt>Lemma 2 (entropy bound)</dt>
<dd>$\log N(\epsilon, \Theta_n, ||\cdot||_\infty)\leq KM_n^{d/\alpha} \epsilon^{-d/\alpha}$ for some constant $K$.</dd>
<dt>Lemma 3</dt>
<dd>Let $\nu$ be a finite meaasure on $\mathcal{X}$ and let $\psi_1,\psi_2$ be measurable functions s.t. $0\leq \psi_1,\psi_2\leq M$ and $\int |\psi_1 - \psi_2| d\nu > (1 + \nu(\mathcal{X})\epsilon$ for some $M,\epsilon>0$. Then
$$ 
 \nu(x: |\psi_1(x) - \psi_2(x)|>\epsilon) \geq \epsilon/M.
$$</dd>
</dl>
<p>Let $\psi_1 = p, \psi_2 = p_0$ and $\nu = Q_n$, we see
</p>
$$ 
\begin{equation}
 I_{n,p} = \#\{x_i: |p(x_i) - p_0(x_i)|>\epsilon\} \geq K'n
\end{equation}
$$<p>
for some $K'$. $I_{n,p}$ contains those points s.t. either $p(x) < p_0(x) - \epsilon$ or $p(x) > p_0(x) + \epsilon$, and at least one case contains more than $K'n/2$ points. So we would not lose order of the number of indices to work with one case only.</p>
<dl>
<dt>Lemma 4 (tests)</dt>
<dd>Let $Y_j$ be independent Bernoulli variable with $P(Y_j = 0) = \mu_j$. Consider testing $H_0: u = u_0$ against $H_0: u > u_0 + \epsilon$. Then there exist tests $\phi_n$ s.t.
$$ 
 P_0(\phi_n) \leq e^{-m\epsilon^2/2}, \quad P_1(1-\phi_n)\leq e^{-m\epsilon^2/2}.
$$</dd>
</dl>
<p>It can be extend to any $p_1 \in \Theta_n$. Consider the case $x_i$ s.t. $p(x_i) > p_0(x_i) + \epsilon$ and $||p^* - p||<\epsilon/2$, then
</p>
$$ 
 p^*(x_i) - p_0(x_i) \geq p(x_i) - p_0(x_i) - ||p - p^*||_\infty >\epsilon /2.
$$<p>With $\epsilon$ replaced by $\epsilon/2$, Lemma 4 implies the existence of tests $\phi_{n,p}$ s.t.
</p>
$$ 
 P_0(\phi_{n,p}) \leq e^{-m\epsilon^2/8}, \quad P^*(1-\phi_{n,p})\leq e^{-m\epsilon^2/8}.
$$<p>
Let $p_1,...,p_N \in\Theta_n$ be the maximal $\epsilon/2$-separated points. Consider $\phi_n = \max_{j\leq N} \phi_{n,p_j}$, then
</p>
$$ 
 P_0\phi_n \leq \sum_j^N P_0(\phi_{n,p_{j}}) \leq N e^{-m\epsilon^2/8} = exp(\log N - m\epsilon^2/8).
$$<p>
For any $p\in\Theta_n$, we can find $p_j\in\Theta_n$ s.t. $||p - p_j||_\infty < \epsilon/2$. Then
</p>
$$ 
 P(1-\phi_n) \leq P(1 - \phi_{n,p_j})  \leq e^{-m\epsilon^2/8}
$$<p>With $M_n$ satisfying assumption (G), we have $\log N \leq b_2' n$, then
</p>
$$ 
 P_0 \phi_n\leq e^{-c'n}, \quad \sup_{||p- p_0|| > \epsilon/2,p\in\Theta_n} P(1-\phi_n)  \leq e^{-c'n}
$$<p>
for some constant $c'$.</p>
<dl>
<dt>Lemma 5 (taylor&rsquo;s expansion)</dt>
<dd>Let $0< \epsilon_0 < 1/2$ and $\epsilon_0 < \alpha,b < 1-\epsilon_0$. Then there exists a constant $L_{\epsilon_0}$ s.t.
$$ 
 \alpha(\log \alpha/\beta)^m + (1-\alpha)(\log \frac{1-\alpha}{1-\beta}) \leq L(\alpha-\beta)^2.
$$</dd>
</dl>
<h2 id="4-proof-of-the-main-theorems">4. Proof of the main theorems<a hidden class="anchor" aria-hidden="true" href="#4-proof-of-the-main-theorems">#</a></h2>
<dl>
<dt>Proof of Thm 1</dt>
<dd>$Y_i|X_i \sim Bin(1, p(X_i)), X_i \sim Q$ i.i.d. then the joint density of $(X,Y)$ is given by
$$ 
 f(x,y) = p(x)^y(1-p(x))^{1-y}.
$$
The corresponding true joint density is $f_0(x,y) = p_0(x)^y (1-p_0(x))^{1-y}$. Assumption (T) implies that $p_0$ is bounded s.t. $\epsilon_0 < p_0(x) < 1-\epsilon_0$ for some $\epsilon_0<1/2$. Thus $f_0(x,y) >\epsilon_0$. Note that $||f_1 - f_2||_{1} =  2||p_1 - p_2||_{1,Q} $ and
$$ 
 KL(f_0||f) = KL(p_0||p) + \int(1-p_0)\log \frac{1-p_0}{1-p} dQ.
$$
First, we show that $f_0\in KL(\Pi)$, or equivalently,
$$
 \Pi(p: KL(p_0||p) +  \int(1-p_0)\log \frac{1-p_0}{1-p} dQ <\epsilon) >0,
$$
for any $\epsilon>0$. Lemma 5 implies that it suffices to show that
$$
 \Pi(p:||p-p_0||_\infty <\epsilon)>0.
$$
Because $p_0 = H(\eta)$ and the link funciton $H$ is bounded and Lipschitz continuous, thus it is enough to show that
$$ 
 \Pi(\eta: ||\eta- \eta_0||_\infty<\epsilon)>0.
$$
This is an immediate result of Theorem 4.
<p>Then we check the entropy condition. Consider the set $F_n = \{f: p\in\Theta_n\}$. Lemma 2, with $M_n = bn^{\alpha/d}$, gives
</p>
$$ 
   \log N(\epsilon, F_n, ||\cdot||_\infty)\leq K \epsilon^{-d/\alpha}b^{d/\alpha}n.
  $$<p>
Chose $b<(\beta/K)^{\alpha/d}\epsilon$, we have that
</p>
$$ 
     \log N(\epsilon, F_n, ||\cdot||_\infty)\leq n\beta.
  $$<p>
Finally, we check the prior mass, which is given by Lemma 1.</p>
</dd>
<dt>Proof of Thm 2</dt>
<dd>Same.</dd>
</dl>
<h2 id="5-discussion">5. Discussion<a hidden class="anchor" aria-hidden="true" href="#5-discussion">#</a></h2>
<p>This paper didn&rsquo;t study the contraction rate. A more general result with contraction rates was studied in</p>
<ul>
<li>van der Vaart, A. W., &amp; van Zanten, J. H. (2008). Rates of contraction of posterior distributions based on Gaussian process priors. The Annals of Statistics, 36(3), 1435–1463. <a href="https://doi.org/10/ctnd8h">https://doi.org/10/ctnd8h</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://wangchxx.github.io/posts/reading/gp_multiclass_classification/">
    <span class="title">« Prev</span>
    <br>
    <span>Reading | Bayesian Classification of Multiclass Functional Data</span>
  </a>
  <a class="next" href="https://wangchxx.github.io/posts/reading/gppriors_for_binary_classification/">
    <span class="title">Next »</span>
    <br>
    <span>Reading | Nonparametric binary regression using a Gaussian process prior</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://wangchxx.github.io/">My Notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
