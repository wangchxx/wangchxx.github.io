<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta property="og:title" content="RL | Dynamics Approximation for Model-Based RL" />
<meta property="og:description" content="One shortcoming to DP methods is that they assume the knowledge of the dynamics. To overcome it, we introduced two model-free methods: MC and TP. They are capable of learning a wide range of tasks. However, such methods suffer from very high sample complexity. In this chapter, we consider a model-based method with dynamics approximation.
1. Dynamics Approximation Let $p(s&#39;|s,a)$ be the unknown dynamics function, and $f_\theta(s_t, a_t)$ be the learned dynamics function parameterized by $\theta$." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://wangchxx.github.io/posts/rl/rl_08_dynamics_approximation/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-09-09T13:26:05+02:00" />
<meta property="article:modified_time" content="2021-09-09T13:26:05+02:00" />
<meta property="og:see_also" content="https://wangchxx.github.io/posts/rl/rl_07_policy_gradient/" /><meta property="og:see_also" content="https://wangchxx.github.io/posts/rl/rl_06_value_function_approximation/" /><meta property="og:see_also" content="https://wangchxx.github.io/posts/rl/rl_05_temporal_difference/" /><meta property="og:see_also" content="https://wangchxx.github.io/posts/rl/rl_04_monte_carlo/" /><meta property="og:see_also" content="https://wangchxx.github.io/posts/rl/rl_03_dynamic_programming/" />


  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="RL | Dynamics Approximation for Model-Based RL"/>
<meta name="twitter:description" content="One shortcoming to DP methods is that they assume the knowledge of the dynamics. To overcome it, we introduced two model-free methods: MC and TP. They are capable of learning a wide range of tasks. However, such methods suffer from very high sample complexity. In this chapter, we consider a model-based method with dynamics approximation.
1. Dynamics Approximation Let $p(s&#39;|s,a)$ be the unknown dynamics function, and $f_\theta(s_t, a_t)$ be the learned dynamics function parameterized by $\theta$."/>

  
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#262d33">
  <title>
    
    My Math Notes - RL | Dynamics Approximation for Model-Based RL
    
  </title>
  
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;600;700&display=swap"
    rel="stylesheet">
  <link rel="stylesheet" href="https://unpkg.com/normalize.css">
  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="/css/md.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="/css/syntax.css" />
  <script src="/js/main.js"></script>
</head>
<script>
  try {
    if (!('theme' in localStorage)) {
      localStorage.theme = window.matchMedia('(prefer-color-scheme: dark)').matches ? 'dark' : 'light';
    }
    document.querySelector('html').classList.add(localStorage.theme);
  } catch (e) {
    console.error(e);
  }
</script>
<body>
  <header>
  <h1 class="row gap-1">
    <div id="theme-switcher" class="btn lg-1"></div>
    My Math Notes
  </h1>
  <nav class="row gap-1">
  
    <a href="/">Home</a>
  
    <a href="/categories">Categories</a>
  
    <a href="/series">Series</a>
  
    <a href="/about">About</a>
  
  </nav>
  <hr>
</header>
  
  
<main>
	<h1>RL | Dynamics Approximation for Model-Based RL</h1>
	<div class="sm-1 mtb-1">
		Posted at &mdash; Sep 9, 2021
		
	</div>
	<p></p>
	<article class="md">
		<p>One shortcoming to DP methods is that they assume the knowledge of the dynamics. To overcome it, we introduced two model-free methods: MC and TP. They are capable of learning a wide range of tasks. However, such methods suffer from very high sample complexity. In this chapter, we consider a model-based method with dynamics approximation.</p>
<h2 id="1-dynamics-approximation">1. Dynamics Approximation</h2>
<p>Let $p(s'|s,a)$ be the unknown dynamics function, and $f_\theta(s_t, a_t)$ be the learned dynamics function parameterized by $\theta$. A straightforward parameterization for $f_\theta(s_t, a_t)$ would take as input the current state-action pair and ouput the predicted next state $\hat{s}_{t+1}$. However, this function can be difficult to learn as $s_t$ and $s_{t+1}$ can be too similar and the action has seemingly little effect on the ouput. Instead predict the next state directly, we predict the change in state $s_t$ over the time step
$$
f_\theta(s_t, a_t) = \hat{\Delta}_{t+1}:= \hat{s}_{t+1} - s_t.
$$</p>
<p>Thus, the predicted next state is $\hat{s}<em>{t+1} = s_t + f</em>\theta(s_t, a_t)$.</p>
<p>Given a dataset $D$ of trajectories $\tau = (s_0,a_0, s1,&hellip;,s_{T-1})$, we can train $f_\theta$ by performing SGD on the following object:
$$
J(\theta) = \sum_{(s_t,a_t,s_{t+1})\in D} || (s_{t+1} - s_t) - f_\theta(s_t, a_t) ||_2^2.
$$
In practice, it&rsquo;s helpful to normalize the target $s_{t+1} - s_t$ as it increases model robustness.</p>
<p>Note that we can collect training data $D$ by initializing a starting state $s_0$ and executing actions from a random policy $\pi_0$ at each timestep. It implies that the model-based methods can learn from off-policy data.</p>
<h2 id="2-model-based-control">2. Model-Based Control</h2>
<p>In this section, we consider how to make decisions given the learned dynamics model. The goal at each time step is to take the action that maximizes the future return, given by
$$
Q_\theta(a_t,&hellip;) = \sum_{t' = t}^\infty \gamma^{t'- t} R(s_{t'}, a_{t'}),
$$
where $R(s,a)$ is the rewrad obtained when executing action $a$ at state $s$ and $s_{t+1} = s_t + f_\theta(s_t,a_t)$.</p>
<p>It is impossible to plan over an infinite sequence of actions, so we optimize the sequence of actions $\mathbf{A}_t = (a_t,&hellip;,a_{t+H-1})$ over a finite horizon $H$.</p>
<p>$$
\begin{equation}
\mathbf{A}_t = \arg\max_{A_t'}\sum_{t'=t}^{t+H-1}\gamma^{t'- t} R(s_{t'}, a_{t'}), \quad s_{t+1} = s_t + f_\theta (s_t,a_t).
\end{equation}
$$</p>
<p>Note that the learned dynamics model is imperfect, so it is desirable to plan $H$ steps since the accumulating errors make planing far into the future inaccurate. But this approach is also limited becaue it may not be sufficient for solving long-horizon tasks.</p>
<p>Solving the Equation (1) is diffifult due to the dynamics and reward functions being nonlinear. The simplest technique to solve this problem is <code>random shooting method</code></p>
<hr>
<p>Random Shooting.</p>
<ol>
<li>Pick $K$ candidate action sequences $\mathbf{A}$ from a distribution $p(\mathbf{A})$.</li>
<li>Evaluate the performace $Q_\theta(\mathbf{A})$ for each candidate.</li>
<li>Choose the elite with highest $Q_\theta(\mathbf{A})$.</li>
</ol>
<hr>
<p>Because of the inaccuracies in  the learned dynamics model, rather than have the policy execute this action sequence, we use <code>model predictive control (MPC)</code>: the policy executes only the first action $a_t$, receives updated state information $s_{t+1}$, and recalculates the optimal action sequence at the next timestep.</p>
<p>So far we have obtained the model-based reinforcement learning algorithm.</p>
<hr>
<table>
<thead>
<tr>
<th>Model-Based RL (version 0.5)</th>
</tr>
</thead>
</table>
<ul>
<li>Gather dataset $D$ from a random policy $\pi_0$</li>
<li>Train $f_\theta$ using $D$ by performing gradient descent.</li>
<li>Plan through $f_\theta(s,a)$ to choose actions.</li>
</ul>
<hr>
<h2 id="3-improving-model-based-rl">3. Improving Model-Based RL</h2>
<p>Although model-based RL is in theory off-policy, in practice it will perform poorly because of the distribution dismatch problem:
$$
p_{\pi_f}(s_t) \neq p_{\pi_0}(s_t),
$$
where $p_\pi(s)$ is the distribution of states generated by policy $\pi$. One solution is on-policy data aggregation.</p>
<hr>
<table>
<thead>
<tr>
<th>Model-Based RL with On-Policy Data (version 1.0)</th>
</tr>
</thead>
</table>
<ul>
<li>gather dataset $D$ from a random policy $\pi_0$</li>
<li>initialize model $f_\theta$ randomly.</li>
<li><code>for</code> each iter <code>do</code>
<ul>
<li>train $f_\theta$ using $D$ by performing gradient descent.</li>
<li><code>for</code> each timestep $t = 1:T$ <code>do</code>
<ul>
<li>plan through $f_\theta(s,a)$ to obtain the optimal action sequence $\mathbf{A_t}$.</li>
<li>MPC: execute first action $a_t$ from selected action sequene, and observe next state $s_{t+1}$.</li>
<li>add $(s_t, a_t, s_{t+1})$ to dataset $D$.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>Our policy depends on the model $f_\theta$, but we are not certain about the model. One solution is to model the uncertainty, e.g. apply a prior to $\theta$. An alternative idea is model ensembles: rather than training one model $f_\theta$, we train $N$ independent models $\{f_{\theta_i}\}$ and average their predictions to get the final predictions
$$
f(s_t,a_t) = \frac{1}{N} \sum_{n=1}^N f_{\theta_n}(s_t, a_t).
$$</p>
<hr>
<table>
<thead>
<tr>
<th>Model-Based RL with Ensemble (version 2.0)</th>
</tr>
</thead>
</table>
<ul>
<li>gather dataset $D$ from a random policy $\pi_0$</li>
<li>initialize models $\{f_{\theta_n}\}$ randomly.</li>
<li><code>for</code> each iter <code>do</code>
<ul>
<li>train each model $f_{\theta_n}$ using $D$ by performing SGD, $n = 1,&hellip;,N$.</li>
<li><code>for</code> each timestep $t = 1:T$ <code>do</code>
<ul>
<li>plan through the averaged model $f(s,a)$ to obtain the optimal action sequence $\mathbf{A_t}$.</li>
<li>MPC: execute first action $a_t$ from selected action sequene, and observe next state $s_{t+1}$.</li>
<li>add $(s_t, a_t, s_{t+1})$ to dataset $D$.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>

	</article>
</main>
	
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css"
  integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js"
  integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js"
  integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      
      
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
        { left: '\\(', right: '\\)', display: false },
        { left: '\\[', right: '\\]', display: true },
        
      ],
      
      throwOnError: false
    });
  });
</script>
	

	

	



  <footer class="row row-mob al-c-mob col-rev-mob sm-2-mob  jc-bt mtb-2">
  <p>
    © Copyright notice |
    <a href="https://github.com/mivinci/hugo-theme-minima" target="_blank" rel="noopener noreferrer">Minima</a> theme on
    <a href="https://gohugo.io" target="_blank" rel="noopener noreferrer">Hugo</a>
  </p>
  <p class="row gap-0_5">
    
      <a class="icon" href="https://github.com/wangchxx" title="github">
      
        <svg fill="#63636f" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
      
      </a>
    
  </p>
</footer>
</body>
</html>