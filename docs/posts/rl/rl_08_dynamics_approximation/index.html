<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>RL | Dynamics Approximation for Model-Based RL | My Notes</title>
<meta name="keywords" content="">
<meta name="description" content="One shortcoming to DP methods is that they assume the knowledge of the dynamics. To overcome it, we introduced two model-free methods: MC and TP. They are capable of learning a wide range of tasks. However, such methods suffer from very high sample complexity. In this chapter, we consider a model-based method with dynamics approximation.
1. Dynamics Approximation
Let $p(s&#39;|s,a)$ be the unknown dynamics function, and $f_\theta(s_t, a_t)$ be the learned dynamics function parameterized by $\theta$. A straightforward parameterization for $f_\theta(s_t, a_t)$ would take as input the current state-action pair and ouput the predicted next state $\hat{s}_{t&#43;1}$. However, this function can be difficult to learn as $s_t$ and $s_{t&#43;1}$ can be too similar and the action has seemingly little effect on the ouput. Instead predict the next state directly, we predict the change in state $s_t$ over the time step
">
<meta name="author" content="Chaohua Wang">
<link rel="canonical" href="https://wangchxx.github.io/posts/rl/rl_08_dynamics_approximation/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://wangchxx.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://wangchxx.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://wangchxx.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://wangchxx.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://wangchxx.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://wangchxx.github.io/posts/rl/rl_08_dynamics_approximation/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '$', right: '$', display: false},  
      ],
      throwOnError : false
    });
  });
</script>



<meta property="og:title" content="RL | Dynamics Approximation for Model-Based RL">
<meta property="og:description" content="One shortcoming to DP methods is that they assume the knowledge of the dynamics. To overcome it, we introduced two model-free methods: MC and TP. They are capable of learning a wide range of tasks. However, such methods suffer from very high sample complexity. In this chapter, we consider a model-based method with dynamics approximation.
1. Dynamics Approximation
Let $p(s&#39;|s,a)$ be the unknown dynamics function, and $f_\theta(s_t, a_t)$ be the learned dynamics function parameterized by $\theta$. A straightforward parameterization for $f_\theta(s_t, a_t)$ would take as input the current state-action pair and ouput the predicted next state $\hat{s}_{t&#43;1}$. However, this function can be difficult to learn as $s_t$ and $s_{t&#43;1}$ can be too similar and the action has seemingly little effect on the ouput. Instead predict the next state directly, we predict the change in state $s_t$ over the time step
">
<meta property="og:type" content="article">
<meta property="og:url" content="https://wangchxx.github.io/posts/rl/rl_08_dynamics_approximation/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-09-09T13:26:05+02:00">
<meta property="article:modified_time" content="2021-09-09T13:26:05+02:00">


<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RL | Dynamics Approximation for Model-Based RL">
<meta name="twitter:description" content="One shortcoming to DP methods is that they assume the knowledge of the dynamics. To overcome it, we introduced two model-free methods: MC and TP. They are capable of learning a wide range of tasks. However, such methods suffer from very high sample complexity. In this chapter, we consider a model-based method with dynamics approximation.
1. Dynamics Approximation
Let $p(s&#39;|s,a)$ be the unknown dynamics function, and $f_\theta(s_t, a_t)$ be the learned dynamics function parameterized by $\theta$. A straightforward parameterization for $f_\theta(s_t, a_t)$ would take as input the current state-action pair and ouput the predicted next state $\hat{s}_{t&#43;1}$. However, this function can be difficult to learn as $s_t$ and $s_{t&#43;1}$ can be too similar and the action has seemingly little effect on the ouput. Instead predict the next state directly, we predict the change in state $s_t$ over the time step
">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://wangchxx.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "RL | Dynamics Approximation for Model-Based RL",
      "item": "https://wangchxx.github.io/posts/rl/rl_08_dynamics_approximation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RL | Dynamics Approximation for Model-Based RL",
  "name": "RL | Dynamics Approximation for Model-Based RL",
  "description": "One shortcoming to DP methods is that they assume the knowledge of the dynamics. To overcome it, we introduced two model-free methods: MC and TP. They are capable of learning a wide range of tasks. However, such methods suffer from very high sample complexity. In this chapter, we consider a model-based method with dynamics approximation.\n1. Dynamics Approximation Let $p(s'|s,a)$ be the unknown dynamics function, and $f_\\theta(s_t, a_t)$ be the learned dynamics function parameterized by $\\theta$. A straightforward parameterization for $f_\\theta(s_t, a_t)$ would take as input the current state-action pair and ouput the predicted next state $\\hat{s}_{t+1}$. However, this function can be difficult to learn as $s_t$ and $s_{t+1}$ can be too similar and the action has seemingly little effect on the ouput. Instead predict the next state directly, we predict the change in state $s_t$ over the time step ",
  "keywords": [
    
  ],
  "articleBody": "One shortcoming to DP methods is that they assume the knowledge of the dynamics. To overcome it, we introduced two model-free methods: MC and TP. They are capable of learning a wide range of tasks. However, such methods suffer from very high sample complexity. In this chapter, we consider a model-based method with dynamics approximation.\n1. Dynamics Approximation Let $p(s'|s,a)$ be the unknown dynamics function, and $f_\\theta(s_t, a_t)$ be the learned dynamics function parameterized by $\\theta$. A straightforward parameterization for $f_\\theta(s_t, a_t)$ would take as input the current state-action pair and ouput the predicted next state $\\hat{s}_{t+1}$. However, this function can be difficult to learn as $s_t$ and $s_{t+1}$ can be too similar and the action has seemingly little effect on the ouput. Instead predict the next state directly, we predict the change in state $s_t$ over the time step $$ f_\\theta(s_t, a_t) = \\hat{\\Delta}_{t+1}:= \\hat{s}_{t+1} - s_t. $$Thus, the predicted next state is $\\hat{s}_{t+1} = s_t + f_\\theta(s_t, a_t)$.\nGiven a dataset $D$ of trajectories $\\tau = (s_0,a_0, s1,...,s_{T-1})$, we can train $f_\\theta$ by performing SGD on the following object: $$ J(\\theta) = \\sum_{(s_t,a_t,s_{t+1})\\in D} || (s_{t+1} - s_t) - f_\\theta(s_t, a_t) ||_2^2. $$ In practice, itâ€™s helpful to normalize the target $s_{t+1} - s_t$ as it increases model robustness.\nNote that we can collect training data $D$ by initializing a starting state $s_0$ and executing actions from a random policy $\\pi_0$ at each timestep. It implies that the model-based methods can learn from off-policy data.\n2. Model-Based Control In this section, we consider how to make decisions given the learned dynamics model. The goal at each time step is to take the action that maximizes the future return, given by $$ Q_\\theta(a_t,...) = \\sum_{t' = t}^\\infty \\gamma^{t'- t} R(s_{t'}, a_{t'}), $$ where $R(s,a)$ is the rewrad obtained when executing action $a$ at state $s$ and $s_{t+1} = s_t + f_\\theta(s_t,a_t)$.\nIt is impossible to plan over an infinite sequence of actions, so we optimize the sequence of actions $\\mathbf{A}_t = (a_t,...,a_{t+H-1})$ over a finite horizon $H$.\n$$ \\begin{equation} \\mathbf{A}_t = \\arg\\max_{A_t'}\\sum_{t'=t}^{t+H-1}\\gamma^{t'- t} R(s_{t'}, a_{t'}), \\quad s_{t+1} = s_t + f_\\theta (s_t,a_t). \\end{equation} $$Note that the learned dynamics model is imperfect, so it is desirable to plan $H$ steps since the accumulating errors make planing far into the future inaccurate. But this approach is also limited becaue it may not be sufficient for solving long-horizon tasks.\nSolving the Equation (1) is diffifult due to the dynamics and reward functions being nonlinear. The simplest technique to solve this problem is random shooting method\nRandom Shooting.\nPick $K$ candidate action sequences $\\mathbf{A}$ from a distribution $p(\\mathbf{A})$. Evaluate the performace $Q_\\theta(\\mathbf{A})$ for each candidate. Choose the elite with highest $Q_\\theta(\\mathbf{A})$. Because of the inaccuracies in the learned dynamics model, rather than have the policy execute this action sequence, we use model predictive control (MPC): the policy executes only the first action $a_t$, receives updated state information $s_{t+1}$, and recalculates the optimal action sequence at the next timestep.\nSo far we have obtained the model-based reinforcement learning algorithm.\nModel-Based RL (version 0.5) Gather dataset $D$ from a random policy $\\pi_0$ Train $f_\\theta$ using $D$ by performing gradient descent. Plan through $f_\\theta(s,a)$ to choose actions. 3. Improving Model-Based RL Although model-based RL is in theory off-policy, in practice it will perform poorly because of the distribution dismatch problem: $$ p_{\\pi_f}(s_t) \\neq p_{\\pi_0}(s_t), $$ where $p_\\pi(s)$ is the distribution of states generated by policy $\\pi$. One solution is on-policy data aggregation.\nModel-Based RL with On-Policy Data (version 1.0) gather dataset $D$ from a random policy $\\pi_0$ initialize model $f_\\theta$ randomly. for each iter do train $f_\\theta$ using $D$ by performing gradient descent. for each timestep $t = 1:T$ do plan through $f_\\theta(s,a)$ to obtain the optimal action sequence $\\mathbf{A_t}$. MPC: execute first action $a_t$ from selected action sequene, and observe next state $s_{t+1}$. add $(s_t, a_t, s_{t+1})$ to dataset $D$. Our policy depends on the model $f_\\theta$, but we are not certain about the model. One solution is to model the uncertainty, e.g. apply a prior to $\\theta$. An alternative idea is model ensembles: rather than training one model $f_\\theta$, we train $N$ independent models $\\{f_{\\theta_i}\\}$ and average their predictions to get the final predictions $$ f(s_t,a_t) = \\frac{1}{N} \\sum_{n=1}^N f_{\\theta_n}(s_t, a_t). $$ Model-Based RL with Ensemble (version 2.0) gather dataset $D$ from a random policy $\\pi_0$ initialize models $\\{f_{\\theta_n}\\}$ randomly. for each iter do train each model $f_{\\theta_n}$ using $D$ by performing SGD, $n = 1,...,N$. for each timestep $t = 1:T$ do plan through the averaged model $f(s,a)$ to obtain the optimal action sequence $\\mathbf{A_t}$. MPC: execute first action $a_t$ from selected action sequene, and observe next state $s_{t+1}$. add $(s_t, a_t, s_{t+1})$ to dataset $D$. ",
  "wordCount" : "783",
  "inLanguage": "en",
  "datePublished": "2021-09-09T13:26:05+02:00",
  "dateModified": "2021-09-09T13:26:05+02:00",
  "author":{
    "@type": "Person",
    "name": "Chaohua Wang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://wangchxx.github.io/posts/rl/rl_08_dynamics_approximation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My Notes",
    "logo": {
      "@type": "ImageObject",
      "url": "https://wangchxx.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://wangchxx.github.io/" accesskey="h" title="My Notes (Alt + H)">My Notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://wangchxx.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/series/" title="Series">
                    <span>Series</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      RL | Dynamics Approximation for Model-Based RL
    </h1>
    <div class="post-meta"><span title='2021-09-09 13:26:05 +0200 +0200'>September 9, 2021</span>&nbsp;Â·&nbsp;4 min&nbsp;Â·&nbsp;Chaohua Wang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-dynamics-approximation" aria-label="1. Dynamics Approximation">1. Dynamics Approximation</a></li>
                <li>
                    <a href="#2-model-based-control" aria-label="2. Model-Based Control">2. Model-Based Control</a></li>
                <li>
                    <a href="#3-improving-model-based-rl" aria-label="3. Improving Model-Based RL">3. Improving Model-Based RL</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>One shortcoming to DP methods is that they assume the knowledge of the dynamics. To overcome it, we introduced two model-free methods: MC and TP. They are capable of learning a wide range of tasks. However, such methods suffer from very high sample complexity. In this chapter, we consider a model-based method with dynamics approximation.</p>
<h2 id="1-dynamics-approximation">1. Dynamics Approximation<a hidden class="anchor" aria-hidden="true" href="#1-dynamics-approximation">#</a></h2>
<p>Let $p(s'|s,a)$ be the unknown dynamics function, and $f_\theta(s_t, a_t)$ be the learned dynamics function parameterized by $\theta$. A straightforward parameterization for $f_\theta(s_t, a_t)$ would take as input the current state-action pair and ouput the predicted next state $\hat{s}_{t+1}$. However, this function can be difficult to learn as $s_t$ and $s_{t+1}$ can be too similar and the action has seemingly little effect on the ouput. Instead predict the next state directly, we predict the change in state $s_t$ over the time step
</p>
$$
f_\theta(s_t, a_t) = \hat{\Delta}_{t+1}:= \hat{s}_{t+1} - s_t.
$$<p>Thus, the predicted next state is $\hat{s}_{t+1} = s_t + f_\theta(s_t, a_t)$.</p>
<p>Given a dataset $D$ of trajectories $\tau = (s_0,a_0, s1,...,s_{T-1})$, we can train $f_\theta$ by performing SGD on the following object:
</p>
$$ 
 J(\theta) = \sum_{(s_t,a_t,s_{t+1})\in D} || (s_{t+1} - s_t) - f_\theta(s_t, a_t) ||_2^2.
$$<p>
In practice, it&rsquo;s helpful to normalize the target $s_{t+1} - s_t$ as it increases model robustness.</p>
<p>Note that we can collect training data $D$ by initializing a starting state $s_0$ and executing actions from a random policy $\pi_0$ at each timestep. It implies that the model-based methods can learn from off-policy data.</p>
<h2 id="2-model-based-control">2. Model-Based Control<a hidden class="anchor" aria-hidden="true" href="#2-model-based-control">#</a></h2>
<p>In this section, we consider how to make decisions given the learned dynamics model. The goal at each time step is to take the action that maximizes the future return, given by
</p>
$$ 
Q_\theta(a_t,...) = \sum_{t' = t}^\infty \gamma^{t'- t} R(s_{t'}, a_{t'}),
$$<p>
where $R(s,a)$ is the rewrad obtained when executing action $a$ at state $s$ and $s_{t+1} = s_t + f_\theta(s_t,a_t)$.</p>
<p>It is impossible to plan over an infinite sequence of actions, so we optimize the sequence of actions $\mathbf{A}_t = (a_t,...,a_{t+H-1})$ over a finite horizon $H$.</p>
$$
\begin{equation}
 \mathbf{A}_t = \arg\max_{A_t'}\sum_{t'=t}^{t+H-1}\gamma^{t'- t} R(s_{t'}, a_{t'}), \quad s_{t+1} = s_t + f_\theta (s_t,a_t).
\end{equation}
$$<p>Note that the learned dynamics model is imperfect, so it is desirable to plan $H$ steps since the accumulating errors make planing far into the future inaccurate. But this approach is also limited becaue it may not be sufficient for solving long-horizon tasks.</p>
<p>Solving the Equation (1) is diffifult due to the dynamics and reward functions being nonlinear. The simplest technique to solve this problem is <code>random shooting method</code></p>
<hr>
<p>Random Shooting.</p>
<ol>
<li>Pick $K$ candidate action sequences $\mathbf{A}$ from a distribution $p(\mathbf{A})$.</li>
<li>Evaluate the performace $Q_\theta(\mathbf{A})$ for each candidate.</li>
<li>Choose the elite with highest $Q_\theta(\mathbf{A})$.</li>
</ol>
<hr>
<p>Because of the inaccuracies in  the learned dynamics model, rather than have the policy execute this action sequence, we use <code>model predictive control (MPC)</code>: the policy executes only the first action $a_t$, receives updated state information $s_{t+1}$, and recalculates the optimal action sequence at the next timestep.</p>
<p>So far we have obtained the model-based reinforcement learning algorithm.</p>
<hr>
<table>
  <thead>
      <tr>
          <th>Model-Based RL (version 0.5)</th>
      </tr>
  </thead>
  <tbody>
  </tbody>
</table>
<ul>
<li>Gather dataset $D$ from a random policy $\pi_0$</li>
<li>Train $f_\theta$ using $D$ by performing gradient descent.</li>
<li>Plan through $f_\theta(s,a)$ to choose actions.</li>
</ul>
<hr>
<h2 id="3-improving-model-based-rl">3. Improving Model-Based RL<a hidden class="anchor" aria-hidden="true" href="#3-improving-model-based-rl">#</a></h2>
<p>Although model-based RL is in theory off-policy, in practice it will perform poorly because of the distribution dismatch problem:
</p>
$$ 
   p_{\pi_f}(s_t) \neq p_{\pi_0}(s_t),
$$<p>
where $p_\pi(s)$ is the distribution of states generated by policy $\pi$. One solution is on-policy data aggregation.</p>
<hr>
<table>
  <thead>
      <tr>
          <th>Model-Based RL with On-Policy Data (version 1.0)</th>
      </tr>
  </thead>
  <tbody>
  </tbody>
</table>
<ul>
<li>gather dataset $D$ from a random policy $\pi_0$</li>
<li>initialize model $f_\theta$ randomly.</li>
<li><code>for</code> each iter <code>do</code>
<ul>
<li>train $f_\theta$ using $D$ by performing gradient descent.</li>
<li><code>for</code> each timestep $t = 1:T$ <code>do</code>
<ul>
<li>plan through $f_\theta(s,a)$ to obtain the optimal action sequence $\mathbf{A_t}$.</li>
<li>MPC: execute first action $a_t$ from selected action sequene, and observe next state $s_{t+1}$.</li>
<li>add $(s_t, a_t, s_{t+1})$ to dataset $D$.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>Our policy depends on the model $f_\theta$, but we are not certain about the model. One solution is to model the uncertainty, e.g. apply a prior to $\theta$. An alternative idea is model ensembles: rather than training one model $f_\theta$, we train $N$ independent models $\{f_{\theta_i}\}$ and average their predictions to get the final predictions
</p>
$$ 
 f(s_t,a_t) = \frac{1}{N} \sum_{n=1}^N f_{\theta_n}(s_t, a_t). 
$$<hr>
<table>
  <thead>
      <tr>
          <th>Model-Based RL with Ensemble (version 2.0)</th>
      </tr>
  </thead>
  <tbody>
  </tbody>
</table>
<ul>
<li>gather dataset $D$ from a random policy $\pi_0$</li>
<li>initialize models $\{f_{\theta_n}\}$ randomly.</li>
<li><code>for</code> each iter <code>do</code>
<ul>
<li>train each model $f_{\theta_n}$ using $D$ by performing SGD, $n = 1,...,N$.</li>
<li><code>for</code> each timestep $t = 1:T$ <code>do</code>
<ul>
<li>plan through the averaged model $f(s,a)$ to obtain the optimal action sequence $\mathbf{A_t}$.</li>
<li>MPC: execute first action $a_t$ from selected action sequene, and observe next state $s_{t+1}$.</li>
<li>add $(s_t, a_t, s_{t+1})$ to dataset $D$.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://wangchxx.github.io/posts/rl/rl_09_mcts/">
    <span class="title">Â« Prev</span>
    <br>
    <span>RL | Multiplayer Monte Carlo Tree Search</span>
  </a>
  <a class="next" href="https://wangchxx.github.io/posts/rl/rl_07_policy_gradient/">
    <span class="title">Next Â»</span>
    <br>
    <span>RL | Policy Gradient Methods</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://wangchxx.github.io/">My Notes</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
