<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>RL | Dynamic Programming | My Notes</title>
<meta name="keywords" content="">
<meta name="description" content="There are two common tasks in an MDP

prediction: estimating the valued function $\pi_\pi$ given a MDP and a policy $\pi$.
control: finding optimal policy $\pi_*$ and corresponding optimal value function $v_*$ given a MDP.

In this chapter we will show how to use DP to solve the prediction and control problems in an MDP. DP is of limited utility in RL both because of their assumption of a perfect model (environment&rsquo;s dynamics are completely known) and because of their great computational expense, but they are important theoretically.">
<meta name="author" content="Chaohua Wang">
<link rel="canonical" href="https://wangchxx.github.io/posts/rl/rl_03_dynamic_programming/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.14b508198d6c32523f8895e6c6606da34de25e588fe390ef44ad07a0cc7dad33.css" integrity="sha256-FLUIGY1sMlI/iJXmxmBto03iXliP45DvRK0HoMx9rTM=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://wangchxx.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://wangchxx.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://wangchxx.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://wangchxx.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://wangchxx.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://wangchxx.github.io/posts/rl/rl_03_dynamic_programming/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '$', right: '$', display: false},  
      ],
      throwOnError : false
    });
  });
</script>



<meta property="og:title" content="RL | Dynamic Programming">
<meta property="og:description" content="There are two common tasks in an MDP

prediction: estimating the valued function $\pi_\pi$ given a MDP and a policy $\pi$.
control: finding optimal policy $\pi_*$ and corresponding optimal value function $v_*$ given a MDP.

In this chapter we will show how to use DP to solve the prediction and control problems in an MDP. DP is of limited utility in RL both because of their assumption of a perfect model (environment&rsquo;s dynamics are completely known) and because of their great computational expense, but they are important theoretically.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://wangchxx.github.io/posts/rl/rl_03_dynamic_programming/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-08-24T03:00:31+02:00">
<meta property="article:modified_time" content="2021-08-24T03:00:31+02:00">


<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RL | Dynamic Programming">
<meta name="twitter:description" content="There are two common tasks in an MDP

prediction: estimating the valued function $\pi_\pi$ given a MDP and a policy $\pi$.
control: finding optimal policy $\pi_*$ and corresponding optimal value function $v_*$ given a MDP.

In this chapter we will show how to use DP to solve the prediction and control problems in an MDP. DP is of limited utility in RL both because of their assumption of a perfect model (environment&rsquo;s dynamics are completely known) and because of their great computational expense, but they are important theoretically.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://wangchxx.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "RL | Dynamic Programming",
      "item": "https://wangchxx.github.io/posts/rl/rl_03_dynamic_programming/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RL | Dynamic Programming",
  "name": "RL | Dynamic Programming",
  "description": "There are two common tasks in an MDP\nprediction: estimating the valued function $\\pi_\\pi$ given a MDP and a policy $\\pi$. control: finding optimal policy $\\pi_*$ and corresponding optimal value function $v_*$ given a MDP. In this chapter we will show how to use DP to solve the prediction and control problems in an MDP. DP is of limited utility in RL both because of their assumption of a perfect model (environment\u0026rsquo;s dynamics are completely known) and because of their great computational expense, but they are important theoretically.\n",
  "keywords": [
    
  ],
  "articleBody": "There are two common tasks in an MDP\nprediction: estimating the valued function $\\pi_\\pi$ given a MDP and a policy $\\pi$. control: finding optimal policy $\\pi_*$ and corresponding optimal value function $v_*$ given a MDP. In this chapter we will show how to use DP to solve the prediction and control problems in an MDP. DP is of limited utility in RL both because of their assumption of a perfect model (environment’s dynamics are completely known) and because of their great computational expense, but they are important theoretically.\n1. Policy Evaluation Recall the Bellman equation for value functions that, for any $s\\in\\mathcal{S}$, $$ \\begin{equation} v_\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a) (r + \\gamma v_\\pi(s')). \\end{equation} $$ Define\n$R_\\pi(s) = \\sum_a \\pi(a|s) r(s,a)$. $P_\\pi(s'|s) = \\sum_{a} \\pi(a|s) p(s'|s,a)$. Then we can represent (1) in the matrix form: $$ v_\\pi = R_\\pi + \\gamma P_\\pi v_\\pi. $$If the environment’s dynamics are completely known, then (1) is a system of $|\\mathcal{S}|$ linear equations with direct solution $$ v_\\pi = (I - \\gamma P_\\pi)^{-1}R_\\pi. $$Alternatively, it can be solved by iterative methods.\nGiven an initial approximation $v_0$, by the Bellman equation (1), the successive approximation can be obtained by $$ v_{k+1}(s):= \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a) (r + \\gamma v_k(s')). $$ If the $v_\\pi$ exits ($\\gamma\u003c1$), the sequence $v_k$ would converge to $v_\\pi$ as $k\\to\\infty$.\nMoreover, in view of the special form of Bellman optimality equation for $v_*$,\n$$ v_*(s) = \\max_a \\sum_{s'}\\sum_r p(s',r|s,a)[r+ \\gamma v_*(s')] = , $$ that it does not involve the factor $\\pi_*(a|s)$. It implies that we can estimate $v_*$ even though we do not know what the optimal policy is. In particular, we can estimate $v_*$ by solving $$ v_*(s) = \\max_a \\left(r(s,a) + \\gamma \\sum_{s'}p(s'|s,a) v_*(s') \\right) $$ for using the iterative relationship $$ v_{k+1}(s) = \\max_{a} \\sum_{s',r} p(s',r|s,a)(r +\\gamma v_k(s')). $$2. Policy Iteration For simplicity, we take the deterministic policy $\\pi$ as an example. Suppose we have solved the value function $v_\\pi$ for an arbitrary deterministic policy $\\pi$, we want to improve it.\nTheorem 3.1 (policy improvement theorem) Let $\\pi,\\pi'$ be any pair of deterministic policies s.t., for all $s\\in\\mathcal{S}$, $$ q_\\pi(s, \\pi'(s)) \\geq v_\\pi(s), $$ then $v_{\\pi'}(s) \\geq v_\\pi(s)$ for all $s\\in\\mathcal{S}$. Proof By the Bellman equation for $q_\\pi$, we have $$ q_\\pi(s,a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma v_\\pi(s')]. $$ Then by the given condition, we have $$ \\begin{align*} v_\\pi(s) \u0026\\leq q_\\pi(s,\\pi'(s)) \\cr \u0026= \\mathbb{E}[R_{t+1}+ \\gamma v_\\pi(S_{t+1})| S_t = s, A_t = \\pi'(s)] \\cr \u0026= \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma v_\\pi(S_{t+1})| S_t = s]\\cr \u0026\\leq \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma q_{\\pi}(S_{t+1}, \\pi'(S_{t+1}))| S_t = s]\\cr \u0026= \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma \\mathbb{E}[R_{t+2} + \\gamma v_\\pi(S_{t+2})|S_{t+1}, A_{t+1} = \\pi'(S_{t+1})]| S_t = s]\\cr \u0026= \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma R_{t+2} + \\gamma^2 v_\\pi(S_{t+2})| S_t = s] \\cr \u0026\\vdots \\cr \u0026\\leq \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots| S_t = s] = v_{\\pi'}(s). \\end{align*} $$ $\\Box$\nThis theorem implies that we can obtain an improved policy $\\pi'$ by $$ \\begin{align*} \\pi'(s) \u0026:= \\arg\\max_{a} q_\\pi(s,a) \\cr \u0026=\\arg\\max_a \\sum_{s',r}p(s',r|s,a) [ r+ \\gamma v_\\pi(s')]. \\end{align*} $$Suppose that we have got the new policy $\\pi'$ s.t. $v_\\pi = v_{\\pi'}$, then it follows that $$ v_{\\pi'}(s) = \\max_a \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_{\\pi'}(s')]. $$ This is the same as the Bellman equation for $v_{*}$, and hence $\\pi'$ must be the optimal policy. So far we have shown how to get an optimal deterministic policy. In the general case, a stochastic policy $\\pi$ can be improved by the same way.\nSo far we have studied how to improve a policy $\\pi$ using its value function $v_\\pi$. We can thus improve it iteratively: given an initial policy $\\pi_0$, $$ \\pi_0 \\xrightarrow{E} v_{\\pi_0} \\xrightarrow{I} \\pi_1 \\xrightarrow{E} v_{\\pi_1} \\to \\cdots \\pi_* \\xrightarrow{E} v_*, $$ where $\\xrightarrow{E}$ denotes a policy evaluation and $\\xrightarrow{I}$ denotes a policy improvement. This way of finding an optimal policy is called policy iteration.\n3. Value Iteration One drawback of policy iteration is that each step involves policy evaluation, which may itself be an iterative computation. One may consider truncating the policy evaluation in some ways without losing convergence.\nThe idea of the value iteration algorithm is to estimate the optimal value function $v_*$ first, and then do the policy improvement. In other words, $$ v_0 \\to v_1 \\to \\cdots \\to v_* \\xrightarrow{I} \\pi_{*}, $$ where the policy evaluation steps are given by $$ v_{k+1}(s) = \\max_{a} \\sum_{s',r} p(s',r|s,a)(r +\\gamma v_k(s')). $$Note that the value iteration requires the maximum to be taken over all actions.\n4. Convergence of Policy Evaluation In the policy evaluation step, we use the iterative relationship $$ v_{k+1}(s):= \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a) (r + \\gamma v_k(s')). $$But how do we know the policy evaluation converges to $v_\\pi$? First we define the Bellman expectation backup operator $T_\\pi$ by $$ T_\\pi(v) = R_\\pi + \\gamma P_\\pi v. $$ We say that the operator $T_\\pi$ is a $\\gamma$-contraction, i.e. it makes value function closer by at least $\\gamma$, $$ \\begin{align*} ||T_\\pi(u) - T_\\pi(v)||_\\infty \u0026= || \\gamma P_\\pi(u -v)||_\\infty \\cr \u0026\\leq \\gamma ||u - v||_\\infty. \\end{align*} $$ Theorem 3.2 (contraction mapping theorem) For any metric space $\\mathcal{V}$ that is complete under an operator $T(v)$, if $T$ is a $\\gamma$-contraction, then $T$ converges to a unique fixed point in $\\mathcal{V}$ at a linear rate of $\\gamma$. By the contraction mapping theorem, the Bellman expectation operator $T_\\pi$ has a unique fixed point. Furthermore, by the bellman equation, $v_\\pi$ is a fixed of $T_\\pi$. Hence the iterative policy evaluation converges to $v_\\pi$ and then policy iteration converges to $v_*$.\nFor the value iteration, we use the iterative relationship $$ v_{k+1}(s) = \\max_{a} \\sum_{s',r} p(s',r|s,a)(r +\\gamma v_k(s')). $$ We can define the Bellman optimality backup operator $T_*$ by $$ T_*(v) = \\max_a( r(\\cdot, a) + \\gamma P_{*,a} v), $$ which is also a $\\gamma$-contraction. It follows that the value iteration converges to the unique fix point $v_*$.\n",
  "wordCount" : "957",
  "inLanguage": "en",
  "datePublished": "2021-08-24T03:00:31+02:00",
  "dateModified": "2021-08-24T03:00:31+02:00",
  "author":{
    "@type": "Person",
    "name": "Chaohua Wang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://wangchxx.github.io/posts/rl/rl_03_dynamic_programming/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My Notes",
    "logo": {
      "@type": "ImageObject",
      "url": "https://wangchxx.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://wangchxx.github.io/" accesskey="h" title="My Notes (Alt + H)">My Notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://wangchxx.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/series/" title="Series">
                    <span>Series</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      RL | Dynamic Programming
    </h1>
    <div class="post-meta"><span title='2021-08-24 03:00:31 +0200 +0200'>August 24, 2021</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Chaohua Wang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-policy-evaluation" aria-label="1. Policy Evaluation">1. Policy Evaluation</a></li>
                <li>
                    <a href="#2-policy-iteration" aria-label="2. Policy Iteration">2. Policy Iteration</a></li>
                <li>
                    <a href="#3-value-iteration" aria-label="3. Value Iteration">3. Value Iteration</a></li>
                <li>
                    <a href="#4-convergence-of-policy-evaluation" aria-label="4. Convergence of Policy Evaluation">4. Convergence of Policy Evaluation</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>There are two common tasks in an MDP</p>
<ol>
<li>prediction: estimating the valued function $\pi_\pi$ given a MDP and a policy $\pi$.</li>
<li>control: finding optimal policy $\pi_*$ and corresponding optimal value function $v_*$ given a MDP.</li>
</ol>
<p>In this chapter we will show how to use DP to solve the prediction and control problems in an MDP. DP is of limited utility in RL both because of their assumption of a perfect model (environment&rsquo;s dynamics are completely known) and because of their great computational expense, but they are important theoretically.</p>
<h2 id="1-policy-evaluation">1. Policy Evaluation<a hidden class="anchor" aria-hidden="true" href="#1-policy-evaluation">#</a></h2>
<p>Recall the Bellman equation for value functions that, for any $s\in\mathcal{S}$,
</p>
$$ 
 \begin{equation}
 v_\pi(s) = \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a) (r + \gamma v_\pi(s')).
 \end{equation}
$$<p>
Define</p>
<ul>
<li>$R_\pi(s) = \sum_a \pi(a|s) r(s,a)$.</li>
<li>$P_\pi(s'|s) = \sum_{a} \pi(a|s) p(s'|s,a)$.</li>
</ul>
<p>Then we can represent (1) in the matrix form:
</p>
$$ 
 v_\pi = R_\pi + \gamma P_\pi v_\pi.
$$<p>If the environment&rsquo;s dynamics are completely known, then (1) is a system of $|\mathcal{S}|$ linear equations with direct solution
</p>
$$ 
 v_\pi = (I - \gamma P_\pi)^{-1}R_\pi.
$$<p>Alternatively, it can be solved by iterative methods.</p>
<p>Given an initial approximation $v_0$, by the Bellman equation (1), the successive approximation can be obtained by
</p>
$$ 
 v_{k+1}(s):=   \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a) (r + \gamma v_k(s')).
$$<p>
If the $v_\pi$ exits ($\gamma<1$), the sequence $v_k$ would converge to $v_\pi$ as $k\to\infty$.</p>
<p>Moreover, in view of the special form of Bellman optimality equation for $v_*$,<br>
</p>
$$ 
 v_*(s) = \max_a \sum_{s'}\sum_r p(s',r|s,a)[r+ \gamma v_*(s')] = ,
$$<p>
that it does not involve the factor $\pi_*(a|s)$. It implies that we can estimate $v_*$ even though we do not know what the optimal policy is. In particular, we can  estimate $v_*$ by solving
</p>
$$ 
   v_*(s) = \max_a \left(r(s,a) + \gamma \sum_{s'}p(s'|s,a) v_*(s') \right)
$$<p>
for using the iterative relationship
</p>
$$ 
 v_{k+1}(s) = \max_{a} \sum_{s',r} p(s',r|s,a)(r +\gamma v_k(s')).
$$<h2 id="2-policy-iteration">2. Policy Iteration<a hidden class="anchor" aria-hidden="true" href="#2-policy-iteration">#</a></h2>
<p>For simplicity, we take the deterministic policy $\pi$ as an example. Suppose we have solved the value function $v_\pi$ for an arbitrary deterministic policy $\pi$, we want to improve it.</p>
<dl>
<dt><em>Theorem 3.1 (policy improvement theorem)</em></dt>
<dd>Let $\pi,\pi'$ be any pair of deterministic policies s.t., for all $s\in\mathcal{S}$,
$$ 
 q_\pi(s, \pi'(s)) \geq v_\pi(s),
$$
then $v_{\pi'}(s) \geq v_\pi(s)$ for all $s\in\mathcal{S}$.</dd>
<dt><em>Proof</em></dt>
<dd>By the Bellman equation for $q_\pi$, we have
$$ 
 q_\pi(s,a) = \sum_{s',r} p(s',r|s,a)[r + \gamma v_\pi(s')].
$$
Then by the given condition, we have
$$ 
 \begin{align*}
 v_\pi(s) &\leq q_\pi(s,\pi'(s)) \cr
    &= \mathbb{E}[R_{t+1}+ \gamma v_\pi(S_{t+1})| S_t = s, A_t = \pi'(s)] \cr
    &= \mathbb{E}_{\pi'}[R_{t+1}+ \gamma v_\pi(S_{t+1})| S_t = s]\cr
    &\leq \mathbb{E}_{\pi'}[R_{t+1}+ \gamma q_{\pi}(S_{t+1}, \pi'(S_{t+1}))| S_t = s]\cr
    &= \mathbb{E}_{\pi'}[R_{t+1}+ \gamma \mathbb{E}[R_{t+2} + \gamma v_\pi(S_{t+2})|S_{t+1}, A_{t+1} = \pi'(S_{t+1})]| S_t = s]\cr
    &= \mathbb{E}_{\pi'}[R_{t+1}+ \gamma R_{t+2} + \gamma^2 v_\pi(S_{t+2})| S_t = s] \cr
    &\vdots \cr
    &\leq \mathbb{E}_{\pi'}[R_{t+1}+ \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots| S_t = s] = v_{\pi'}(s).
 \end{align*}
$$</dd>
</dl>
<p>$\Box$</p>
<p>This theorem implies that we can obtain an improved policy $\pi'$ by
</p>
$$ 
 \begin{align*}
 \pi'(s) &:= \arg\max_{a} q_\pi(s,a) \cr
    &=\arg\max_a \sum_{s',r}p(s',r|s,a) [ r+ \gamma v_\pi(s')].
 \end{align*}
$$<p>Suppose that we have got the new policy $\pi'$ s.t. $v_\pi = v_{\pi'}$, then it follows that
</p>
$$ 
 v_{\pi'}(s)  = \max_a  \sum_{s',r} p(s',r|s,a) [r + \gamma v_{\pi'}(s')].
$$<p>
This is the same as the Bellman equation for $v_{*}$, and hence $\pi'$ must be the optimal policy. So far we have shown how to get an optimal deterministic policy. In the general case, a stochastic policy $\pi$ can be improved by the same way.</p>
<p>So far we have studied how to improve a policy $\pi$ using its value function $v_\pi$. We can thus improve it iteratively: given an initial policy $\pi_0$,
</p>
$$ 
 \pi_0 \xrightarrow{E} v_{\pi_0} \xrightarrow{I} \pi_1  \xrightarrow{E} v_{\pi_1} \to \cdots \pi_* \xrightarrow{E} v_*, 
$$<p>
where $\xrightarrow{E}$ denotes a policy evaluation and $\xrightarrow{I}$ denotes a policy improvement. This way of finding an optimal policy is called <em>policy iteration</em>.</p>
<h2 id="3-value-iteration">3. Value Iteration<a hidden class="anchor" aria-hidden="true" href="#3-value-iteration">#</a></h2>
<p>One drawback of policy iteration is that each step involves policy evaluation, which may itself be an iterative computation. One may consider truncating the policy evaluation in some ways without losing convergence.</p>
<p>The idea of the <em>value iteration</em> algorithm is to estimate the optimal value function $v_*$ first, and then do the policy improvement. In other words,
</p>
$$ 
 v_0 \to v_1 \to \cdots \to v_*   \xrightarrow{I} \pi_{*},
$$<p>
where the policy evaluation steps are given by
</p>
$$ 
 v_{k+1}(s) = \max_{a} \sum_{s',r} p(s',r|s,a)(r +\gamma v_k(s')).
$$<p>Note that the value iteration requires the maximum to be taken over all actions.</p>
<h2 id="4-convergence-of-policy-evaluation">4. Convergence of Policy Evaluation<a hidden class="anchor" aria-hidden="true" href="#4-convergence-of-policy-evaluation">#</a></h2>
<p>In the policy evaluation step, we use the iterative relationship
</p>
$$ 
 v_{k+1}(s):=   \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a) (r + \gamma v_k(s')).
$$<p>But how do we know the policy evaluation converges to $v_\pi$?  First we define the Bellman expectation backup operator $T_\pi$ by
</p>
$$ 
 T_\pi(v) = R_\pi + \gamma P_\pi v.
$$<p>
We say that the operator $T_\pi$ is a $\gamma$-contraction, i.e. it makes value function closer by at least $\gamma$,
</p>
$$ 
\begin{align*}
 ||T_\pi(u) - T_\pi(v)||_\infty &= || \gamma P_\pi(u -v)||_\infty  \cr
   &\leq \gamma ||u - v||_\infty.
\end{align*}
$$<dl>
<dt><em>Theorem 3.2 (contraction mapping theorem)</em></dt>
<dd>For any metric space $\mathcal{V}$ that is complete under an operator $T(v)$, if $T$ is a $\gamma$-contraction, then $T$ converges to a unique fixed point in $\mathcal{V}$ at a linear rate of $\gamma$.</dd>
</dl>
<p>By the contraction mapping theorem, the Bellman expectation operator $T_\pi$ has a unique fixed point. Furthermore, by the bellman equation, $v_\pi$ is a fixed of $T_\pi$. Hence the iterative policy evaluation converges to $v_\pi$ and then policy iteration converges to $v_*$.</p>
<p>For the value iteration, we use the iterative relationship
</p>
$$ 
 v_{k+1}(s) = \max_{a} \sum_{s',r} p(s',r|s,a)(r +\gamma v_k(s')).
$$<p>
We can define the <em>Bellman optimality backup operator</em> $T_*$ by
</p>
$$ 
 T_*(v) = \max_a( r(\cdot, a) +  \gamma P_{*,a} v),
$$<p>
which is also a $\gamma$-contraction. It follows that the value iteration converges to the unique fix point $v_*$.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://wangchxx.github.io/posts/rl/rl_04_monte_carlo/">
    <span class="title">« Prev</span>
    <br>
    <span>RL | Model-Free: Monte Carlo Methods</span>
  </a>
  <a class="next" href="https://wangchxx.github.io/posts/rl/rl_02_markov_decision_process/">
    <span class="title">Next »</span>
    <br>
    <span>RL | Markov Decision Processes</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://wangchxx.github.io/">My Notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
