<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta property="og:title" content="RL | Value Function Approximation" />
<meta property="og:description" content="So far we have represented value function by a lookup table. But sometimes we may have large MDPs where there are too many states or actions to store in memory. One solution to this problem is estimating value function with function approximation
$$ \hat{v}(s,w) \approx v_\pi(s), \quad \hat{q}(s,a,w) \approx q_\pi(s,a). $$
For the simplicity of notation, we denote the function approximator by $f$. With a differentiable loss function $J_w$, e.g. $$ J_w = \mathbb{E}_\pi (f(s,w) - v_\pi(s))^2, $$ we can approximate $v$ or $q$ by GD, i." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://wangchxx.github.io/posts/rl/rl_06_value_function_approximation/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-08-25T22:17:58+02:00" />
<meta property="article:modified_time" content="2021-08-25T22:17:58+02:00" />
<meta property="og:see_also" content="https://wangchxx.github.io/posts/rl/rl_07_policy_gradient/" /><meta property="og:see_also" content="https://wangchxx.github.io/posts/rl/rl_05_temporal_difference/" /><meta property="og:see_also" content="https://wangchxx.github.io/posts/rl/rl_04_monte_carlo/" /><meta property="og:see_also" content="https://wangchxx.github.io/posts/rl/rl_03_dynamic_programming/" /><meta property="og:see_also" content="https://wangchxx.github.io/posts/rl/rl_02_markov_decision_process/" />


  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="RL | Value Function Approximation"/>
<meta name="twitter:description" content="So far we have represented value function by a lookup table. But sometimes we may have large MDPs where there are too many states or actions to store in memory. One solution to this problem is estimating value function with function approximation
$$ \hat{v}(s,w) \approx v_\pi(s), \quad \hat{q}(s,a,w) \approx q_\pi(s,a). $$
For the simplicity of notation, we denote the function approximator by $f$. With a differentiable loss function $J_w$, e.g. $$ J_w = \mathbb{E}_\pi (f(s,w) - v_\pi(s))^2, $$ we can approximate $v$ or $q$ by GD, i."/>

  
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#262d33">
  <title>
    
    My Math Notes - RL | Value Function Approximation
    
  </title>
  
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;600;700&display=swap"
    rel="stylesheet">
  <link rel="stylesheet" href="https://unpkg.com/normalize.css">
  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="/css/md.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="/css/syntax.css" />
  <script src="/js/main.js"></script>
</head>
<script>
  try {
    if (!('theme' in localStorage)) {
      localStorage.theme = window.matchMedia('(prefer-color-scheme: dark)').matches ? 'dark' : 'light';
    }
    document.querySelector('html').classList.add(localStorage.theme);
  } catch (e) {
    console.error(e);
  }
</script>
<body>
  <header>
  <h1 class="row gap-1">
    <div id="theme-switcher" class="btn lg-1"></div>
    My Math Notes
  </h1>
  <nav class="row gap-1">
  
    <a href="/">Home</a>
  
    <a href="/categories">Categories</a>
  
    <a href="/series">Series</a>
  
    <a href="/about">About</a>
  
  </nav>
  <hr>
</header>
  
  
<main>
	<h1>RL | Value Function Approximation</h1>
	<div class="sm-1 mtb-1">
		Posted at &mdash; Aug 25, 2021
		
	</div>
	<p></p>
	<article class="md">
		<p>So far we have represented value function by a lookup table. But sometimes we may have large MDPs where there are too many states or actions to store in memory. One solution to this problem is estimating value function with function approximation</p>
<p>$$
\hat{v}(s,w) \approx v_\pi(s), \quad \hat{q}(s,a,w) \approx q_\pi(s,a).
$$</p>
<p>For the simplicity of notation, we denote the function approximator by $f$. With a differentiable loss function $J_w$, e.g.
$$
J_w = \mathbb{E}_\pi (f(s,w) - v_\pi(s))^2,
$$
we can approximate $v$ or $q$ by GD, i.e.
$$
w \leftarrow w - \frac{1}{2}\alpha \nabla_w J_w,
$$
where $\nabla_w J_w = 2 \mathbb{E}_{\pi} [(f(s,w) - v_\pi(s)) \nabla_w f(s,w)]$. We assume that samples are generated by policy $\pi$, then we can estimate the gradient by sampling,
$$
\begin{equation}
\nabla_w J_w = 2(f(s,w) - v_\pi(s)) \nabla_w f(s,w).
\end{equation}
$$
This is known as SGD.</p>
<p>The function approximator can be of various forms, e.g. linear function, neural network, decision tree, etc.</p>
<h2 id="1-mc-and-td-with-value-function-approximation">1. MC and TD with Value Function Approximation</h2>
<p>So far we have assumed that the target function $v_\pi$ or $q_\pi$ is known, but in RL where is no supervisor, on rewards. So, in practice, we do not approximate $v_\pi$ or $q_\pi$ directly, but substitute a target for it.</p>
<ul>
<li>
<p>For MC, the target is the return $G_t$
$$
\begin{align*}
\nabla_w J_w &amp;= 2(f(S_t,(A_t),w) - G_t) \nabla_w f(S_t,(A_t),w), \cr
\end{align*}
$$</p>
</li>
<li>
<p>For TD($0$), the target is the TD target $R_{t+1} + \gamma f(S_{t+1},(A_{t+1}), w)$
$$
\nabla_w J_w = 2(f(S_t,(A_t),w) - (R_{t+1} + \gamma f(S_{t+1},(A_{t+1}), w))) \nabla_w f(S_t,(A_t),w),
$$</p>
</li>
<li>
<p>For TD($\lambda$), the target is the $\lambda$-return $G_t^\lambda$ or $q_t^\lambda$
$$
\nabla_w J_w = 2(f(S_t,(A_t),w) - G_t^\lambda) \nabla_w f(S_t,(A_t),w).
$$</p>
</li>
</ul>
<p>When the argument $A_t$ if function $f$ exists, it means that $f$ is an approximator for $q_\pi(s,a)$.</p>
<p>The backward view of TD($\lambda$) becomes
$$
\begin{align*}
E_0(s,(a)) &amp;= 0 \cr
E_t(s,(a)) &amp;= \gamma \lambda E_{t-1}(s,(a)) + \nabla_w f(S_t,(A_t),w)\cr
\delta_t &amp;= R_{t+1} - \gamma f(S_{t+1},(A_{t+1}), w) - f(S_t,(A_t),w) \cr
w &amp;\leftarrow w + \alpha \delta_t E_t.
\end{align*}
$$</p>
<h2 id="2batch-methods">2.Batch Methods</h2>
<p>All the methods we have discussed in the previous section required computation per state transition. In this section we present a method for batch computation.</p>
<p>Take approximating state-value function $v_\pi$ as an example, and the case for $q_\pi$ is essentially the same.  Assume that we have a training date set $D$ consisting of state-value pairs
$$
D = \{ (s_1, v_{\pi,1}),&hellip;,(s_n, v_{\pi,n}) \}.
$$
Define the loss function by
$$
J_w = \frac{1}{n}\sum_{i=1}^n (v_{\pi,i} - f(s_i,w))^2 = \mathbb{E}_D [v_\pi - f(s,w)]^2.
$$</p>
<p>The SGD yields
$$
w \leftarrow w - \frac{1}{2} \alpha \nabla_w J_w.
$$</p>
<p>In particular, if the approximator $f$ is a linear function, i.e. $f(s,w) = x(s)^T w$, we can solve $w$ analytically
$$
w = (X^T X)^{-1}X^T v_\pi.
$$</p>
<p>However, the training data $D$ can be generated from many policies. To evaluate $q_\pi$, we must learn off-policy. We use the same idea as $Q$-learning:</p>
<hr>
<table>
<thead>
<tr>
<th>Least Square TD Q-learning (LSTDQ)</th>
</tr>
</thead>
</table>
<ul>
<li>
<p>Initialize parameters $w$ and target policy $\pi_{0}$, behavior policy $\pi$.</p>
</li>
<li>
<p>Given dataset $D$</p>
</li>
<li>
<p>Repeat:</p>
<ul>
<li>
<p>Generate transitions $(S_t, A_t, R_{t+1}, S_{t+1})$ from policy $\pi$ and store in replay memory $D$.</p>
</li>
<li>
<p>Consider alternative successor action $A' = \pi_0(S_{t+1})$.</p>
</li>
<li>
<p>Sample mini-batch from $D$ and update $w$ by SGD
$$
w \leftarrow w  - \alpha(f(S_t, A_t,w) - (R_{t+1} + \gamma f(S_{t+1}, A', w)))\nabla_w f
$$</p>
</li>
<li>
<p>update the policy $\pi_0$ w.r.t. the function $f$ with new $w$.</p>
</li>
</ul>
</li>
</ul>
<hr>
<p>With the policy evaluation, we can solve the control problem by policy iteration.</p>
<hr>
<table>
<thead>
<tr>
<th>Least Square Policy Iteration</th>
</tr>
</thead>
</table>
<ul>
<li>Initialize parameters $w$, and derive a policy $\pi$ w.r.t. $w$.</li>
<li>Given the training dataset $D$</li>
<li>Repeat
<ul>
<li>policy evaluation LSTDQ$(\pi, D)$.</li>
<li>policy improvement, update $\pi$.</li>
</ul>
</li>
</ul>
<hr>
<dl>
<dt><em>Example 6.1 (Experience replay in deep Q-networks)</em></dt>
<dd>Deep Q-networks (DQN) uses deep neural networks as the approximator for the action-value function $q_\pi(s,a)$.
<ul>
<li>Initialize parameters $w$ and a target action-value function $Q$.</li>
<li>Given states $s$, take actions $a$ by $\epsilon$-greedy w.r.t. $f(s,a,w)$.</li>
<li>Store transitions $(S_t, A_t, R_{t+1}, S_{t+1})$ in replay memory $D$.</li>
<li>Sample random mini-batch, of size $n$, of transitions $(s,a,r,s')$ from $D$.</li>
<li>Compute Q-learning targets w.r.t network $f(s,a,w)$.</li>
<li>Compute MSE
$$
J_w = \frac{1}{n} \sum_{i=1}^n \left( r + \gamma \max_{a'} Q(s',a') - f(s,a,w) \right)^2.
$$</li>
<li>Compute $w$ by SGD, and update the target $Q$ using $f(s,a,w^{old}).$</li>
</ul>
</dd>
</dl>

	</article>
</main>
	
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css"
  integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js"
  integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js"
  integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      
      
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
        { left: '\\(', right: '\\)', display: false },
        { left: '\\[', right: '\\]', display: true },
        
      ],
      
      throwOnError: false
    });
  });
</script>
	

	

	



  <footer class="row row-mob al-c-mob col-rev-mob sm-2-mob  jc-bt mtb-2">
  <p>
    © Copyright notice |
    <a href="https://github.com/mivinci/hugo-theme-minima" target="_blank" rel="noopener noreferrer">Minima</a> theme on
    <a href="https://gohugo.io" target="_blank" rel="noopener noreferrer">Hugo</a>
  </p>
  <p class="row gap-0_5">
    
      <a class="icon" href="https://github.com/wangchxx" title="github">
      
        <svg fill="#63636f" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
      
      </a>
    
  </p>
</footer>
</body>
</html>