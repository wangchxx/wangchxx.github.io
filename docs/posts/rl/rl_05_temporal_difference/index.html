<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta property="og:url" content="https://wangchxx.github.io/posts/rl/rl_05_temporal_difference/">
  <meta property="og:site_name" content="My Math Notes">
  <meta property="og:title" content="RL | Model-Free: Temporal Difference Learning">
  <meta property="og:description" content="Temporal-difference (TD) learning is a combination of Monte Carlo ideas and DP ideas. Like MC methods, TD can learn from raw experience without knowledge of the environment’s dynamics. Like DP, TD updates estimates based in part on other learned estimates, without waiting for a final output ($G_t$).
1. TD Prediction Recall that a simple MC methods for nonstationary environment updates $V(S_t)$ by $$ V(S_t) \leftarrow V(S_t) &#43; \alpha [ G_t - V(S_t)], $$ where $G_t$ can only be known when a visit to $S_t$ occurs. The simplest TD method updates $V$ by $$ \begin{equation} V(S_t) \leftarrow V(S_t) &#43; \alpha \big[ R_{t&#43;1} &#43; \gamma V(S_{t&#43;1}) - V(S_t) \big]. \end{equation} $$">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2021-08-24T18:28:15+02:00">
    <meta property="article:modified_time" content="2021-08-24T18:28:15+02:00">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/rl/rl_09_mcts/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/rl/rl_08_dynamics_approximation/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/rl/rl_07_policy_gradient/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/rl/rl_06_value_function_approximation/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/rl/rl_04_monte_carlo/">
      <meta property="og:see_also" content="https://wangchxx.github.io/posts/rl/rl_03_dynamic_programming/">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="RL | Model-Free: Temporal Difference Learning">
  <meta name="twitter:description" content="Temporal-difference (TD) learning is a combination of Monte Carlo ideas and DP ideas. Like MC methods, TD can learn from raw experience without knowledge of the environment’s dynamics. Like DP, TD updates estimates based in part on other learned estimates, without waiting for a final output ($G_t$).
1. TD Prediction Recall that a simple MC methods for nonstationary environment updates $V(S_t)$ by $$ V(S_t) \leftarrow V(S_t) &#43; \alpha [ G_t - V(S_t)], $$ where $G_t$ can only be known when a visit to $S_t$ occurs. The simplest TD method updates $V$ by $$ \begin{equation} V(S_t) \leftarrow V(S_t) &#43; \alpha \big[ R_{t&#43;1} &#43; \gamma V(S_{t&#43;1}) - V(S_t) \big]. \end{equation} $$">

  
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#262d33">
  <title>
    
    My Math Notes - RL | Model-Free: Temporal Difference Learning
    
  </title>
  
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;600;700&display=swap"
    rel="stylesheet">
  <link rel="stylesheet" href="https://unpkg.com/normalize.css">
  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="/css/md.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="/css/syntax.css" />
  <script src="/js/main.js"></script>
</head>
<script>
  try {
    if (!('theme' in localStorage)) {
      localStorage.theme = window.matchMedia('(prefer-color-scheme: dark)').matches ? 'dark' : 'light';
    }
    document.querySelector('html').classList.add(localStorage.theme);
  } catch (e) {
    console.error(e);
  }
</script>
<body>
  <header>
  <h1 class="row gap-1">
    <div id="theme-switcher" class="btn lg-1"></div>
    My Math Notes
  </h1>
  <nav class="row gap-1">
  
    <a href="/">Home</a>
  
    <a href="/categories">Categories</a>
  
    <a href="/series">Series</a>
  
    <a href="/about">About</a>
  
  </nav>
  <hr>
</header>
  
  
<main>
	<h1>RL | Model-Free: Temporal Difference Learning</h1>
	<div class="sm-1 mtb-1">
		Posted at &mdash; Aug 24, 2021
		
	</div>
	<p></p>
	<article class="md">
		<p>Temporal-difference (TD) learning is a combination of Monte Carlo ideas and DP ideas. Like MC methods, TD can learn from raw experience without knowledge of the environment&rsquo;s dynamics. Like DP, TD updates estimates based in part on other learned estimates, without waiting for a final output ($G_t$).</p>
<h2 id="1-td-prediction">1. TD Prediction</h2>
<p>Recall that a simple MC methods for nonstationary environment updates $V(S_t)$ by
$$
V(S_t) \leftarrow V(S_t) + \alpha [ G_t - V(S_t)],
$$
where $G_t$ can only be known when a visit to $S_t$ occurs. The simplest TD method updates $V$ by
$$
\begin{equation}
V(S_t) \leftarrow V(S_t) + \alpha \big[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \big].
\end{equation}
$$</p>
<p>It implies that TD can update $V$ immediately on transition to $S_{t+1}$ and receiving $R_{t+1}$. This TD method is called TD(0), or <em>one-step TD</em>, because it is a special case of the $TD(\lambda)$, $n$-step TD.</p>
<p>Because TD(0) update $V(S_t)$ using an existing estimate $V(S_{t+1})$, we say that is is a bootstrapping method, like DP. We know that
$$
\begin{align}
v_\pi(s) &amp;= \mathbb{E}<em>\pi [G_t|S_t = s] \cr
&amp;= \mathbb{E}</em>\pi [R_{t+1} + \gamma v_\pi(S_{t+1})|S_t = s].
\end{align}
$$
MC methods use an estimate of (2) as a target, whereas DP methods use an estimate of (3) as a target. The MC methods estimate the expectation in (2) by sampling and TD methods estimate $v_\pi$ using existing estimate $V(S_{t+1})$. The TD methods combine the sampling of MC with the bootstrapping of DP.</p>
<h2 id="2-tdlambda-and-n-step-td">2. TD($\lambda$) and $n$-step TD</h2>
<ul>
<li>$n$-step returns: $G_t^{(n)} := R_{t+1} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n  V(S_{t+n})$ for $n=1,2,&hellip;,\infty$.</li>
</ul>
<p>Then the $n$-step TD learning is
$$
V(S_t) \leftarrow V(S_t) + \alpha [ G_t^{(n)} - V(S_t)].
$$</p>
<ul>
<li>$\lambda$-returns: $G_t^\lambda:= (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1}G_t^{(n)}$.</li>
</ul>
<p>Then the $TD(\lambda)$ learning is
$$
V(S_t) \leftarrow V(S_t) + \alpha [ G_t^{\lambda} - V(S_t)].
$$</p>
<p>Like MC, the $\lambda$-returns can only be computed from complete episodes.</p>
<dl>
<dt><em>Definition 5.1 (TD error)</em></dt>
<dd>The TD error is defined as $\delta_t:= R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$.</dd>
<dt><em>Definition 5.2 (Eligibility trace)</em></dt>
<dd>The Eligibility trace is defined as
$$
\begin{align*}
E_0(s) &amp;= 0 \cr
E_t(s) &amp;= \gamma \lambda E_{t-1}(s) + 1_{S_t = s}.
\end{align*}
$$</dd>
</dl>
<p>If we view   $TD(\lambda)$ backward, keeping $E_t(s)$ for every $s$ and $\delta_t$ when updating $V(s)$, the $TD(\lambda)$ can be  written as an online learning algorithm
$$
V(s) \leftarrow V(s) + \alpha \delta_t E_t(s).
$$</p>
<p>Note that if the array $V$ does not change during the episode (episodic offline), then the MC error can be written as a sum of TD errors:
$$
\begin{align}
G_t - V(S_t) &amp;= \delta_t + \gamma \delta_{t+1} + \cdots + \gamma_{T-t-1}\delta_{T-1} + \gamma^{T-t}(G_T - V(S_T))  \cr
&amp;= \delta_t + \gamma \delta_{t+1} + \cdots + \gamma_{T-t-1}\delta_{T-1} + \gamma^{T-t}(0 - 0)  \cr
&amp;= \sum_{k=t}^{T-1} \gamma^{k-t} \delta_k.
\end{align}
$$</p>
<dl>
<dt><em>Theorem 5.3</em></dt>
<dd>The sum of offline updates is identical for forward-view and backward-view $TD(\lambda)$, i.e.
$$
\sum_{t=1}^T \alpha \delta_t E_t(s) = \sum_{t=1}^T \alpha(G_t^\lambda - V(S_t)) 1_{S_t = s}.
$$</dd>
</dl>
<p>Consider an episode where state $s$ is visited once at time-step $k$. The eligibility trace of TD(1) is
$$
E_t(s) = \gamma^{t-k} 1_{t\geq k}.
$$</p>
<p>By the Theorem 5.3, we see that
$$
\sum_{t=1}^{T-1} \alpha \delta_t E_t(s) = \alpha  \sum_{k=t}^{T-1} \gamma^{k-t} \delta_k = \alpha ( G_t - V(S_t)),
$$
which is exactly the total update for MC. It implies that TD(1) is roughly equivalent to every-visit MC.</p>
<h2 id="3-on-policy-td-control-sarsa">3. On-policy TD Control :SARSA</h2>
<p>We now turn to the use of TD prediction methods for the control problem. The first step is to learn an action-value function $q_\pi$. In the first section, we considered transitions from state to state and learned the value function $v_\pi$. The idea of estimating $q_\pi$ is essentially identical to estimating $v_\pi$. Apply TD(0) to updating $Q(S_t, A_t)$ by
$$
\begin{equation}
Q(S_t, A_t) \leftarrow  Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) -  Q(S_t, A_t)].
\end{equation}
$$
This rules uses the tuple $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$, that makes up a transition from one state-action pair to the next, $(S_t, A_t)\to R_{t+1} \to (S_{t+1}, A_{t+1})$. This tuple gives rise to the name <em>SARSA</em> for the algorithm.</p>
<hr>
<table>
  <thead>
      <tr>
          <th>SARSA for On-policy Control</th>
      </tr>
  </thead>
  <tbody>
  </tbody>
</table>
<ul>
<li>
<p>Initialize $Q(s,a)$ arbitrarily for all $a,s$, and $Q(S_T,\cdot) = 0$.</p>
</li>
<li>
<p>initialize a policy $\pi$ w.r.t. $Q$.</p>
</li>
<li>
<p>For each episode:</p>
<ul>
<li>Initialize $S_0$</li>
<li>Choose $A_0$ using the policy $\pi$.</li>
<li>Loop for each step $t = 0,&hellip;,T-1$:
<ul>
<li>take action $A_t$, observe $R_{t+1}, S_{t+1}$.</li>
<li>choose action $A_{t+1}$ using policy derived from $Q$ (e.g., $\epsilon$-greedy).</li>
<li>incremental update $Q$ by
$$
Q(S_t, A_t) \leftarrow  Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) -  Q(S_t, A_t)].
$$</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<dl>
<dt><em>Theorem 5.4 (convergence of SARSA)</em></dt>
<dd>SARSA converges to the optimal action-value function $q_*$, under the following conditions:
<ol>
<li>GLIE sequence of policies $\pi_t(a|s)$.</li>
<li>Robbins-Monro sequence of $\alpha_t$,
$$
\sum_{t=1}^\infty \alpha_t = \infty, \quad   \sum_{t=1}^\infty \alpha_t^2&lt;\infty.
$$</li>
</ol>
</dd>
</dl>
<p>We can also apply the TD($\lambda$) idea or $n$-step TD learning to estimating $q_*$.</p>
<ul>
<li>$n$-step SARSA: $q_t^{(n)} := R_{t+1} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n  Q(S_{t+n}, A_{t+n})$ for $n=1,2,&hellip;,\infty$.</li>
</ul>
<p>Then the $n$-step SARSA updates
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [ q_t^{(n)} - Q(S_t, A_t)].
$$</p>
<ul>
<li>$q^\lambda$-return: $q_t^\lambda:= (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1}q_t^{(n)}$.</li>
</ul>
<p>Then the forward SARSA($\lambda$) updates
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [ q_t^{\lambda} - Q(S_t, A_t)].
$$</p>
<p>Note that $q_t^\lambda$ is unknown until the terminal state. Just like TD($\lambda$), we use eligibility traces in an online algorithm. We need the eligibility trace for each state-action pair,
$$
\begin{align*}
E_0(s,a) &amp;= 0 \cr
E_t(s,a) &amp;= \gamma \lambda E_{t-1}(s,a) + 1_{(S_t = s, A_t = a)}.
\end{align*}
$$</p>
<p>And the TD error $\delta_t$ now becomes
$$
\delta_t =   R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t),
$$
and hence we update $Q(s,a)$ by
$$
Q(s,a) \leftarrow Q(s,a) + \alpha \delta_t E_t(s,a).
$$</p>
<hr>
<table>
  <thead>
      <tr>
          <th>SARSA($\lambda$) for On-policy Control</th>
      </tr>
  </thead>
  <tbody>
  </tbody>
</table>
<ul>
<li>
<p>Initialize $Q(s,a)$ arbitrarily for all $a,s$ and $Q(S_T,\cdot) = 0$.</p>
</li>
<li>
<p>initialize a policy $\pi$ w.r.t. $Q$.</p>
</li>
<li>
<p>For each episode:</p>
<ul>
<li>Initialize $E(s,a) = 0$ for all $a,s$.</li>
<li>Initialize $S_0$</li>
<li>Choose $A_0$ using the policy $\pi$.</li>
<li>Loop for each step $t = 0,&hellip;,T-1$:
<ul>
<li>take action $A_t$, observe $R_{t+1}, S_{t+1}$.</li>
<li>choose action $A_{t+1}$ using policy derived from $Q$ (e.g., $\epsilon$-greedy).</li>
<li>compute $\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$, update $E(S_t, A_t) += 1 $.</li>
<li>incremental update $Q, E$ for all $s,a$ by
$$
\begin{align*}
Q(s,a) &amp;\leftarrow Q(s,a) + \alpha \delta_t E(s,a), \cr
E(s,a) &amp;\leftarrow \gamma \lambda E(s,a).
\end{align*}
$$</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="4-off-policy-td-control-q-learning">4. Off-policy TD Control: Q-learning</h2>
<p>We now apply TD methods to off-policy control. Given the a policy $\pi$ and a behavior policy $\mu$, when choosing next action $A_{t+1}$ conditioned on state $S_{t+1}$ using the policy $\mu$, we also consider alternative successor action $A&rsquo;$ using the target policy $\pi$. Then we can update $Q$ by
$$
Q(S_t, A_t) \leftarrow  Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A&rsquo;) -  Q(S_t, A_t)],
$$
where reward $R_{t+1}$ is a result of action $A_{t+1}$.</p>
<p>Now we allow both behavior and target policies to improve:</p>
<ul>
<li>the target policy $\pi$ is greedy w.r.t. $Q(s,a)$.</li>
<li>the behavior policy $\mu$ is e.g. $\epsilon$-greedy w.r.t. $Q(s,a)$.</li>
</ul>
<p>The Q-learning target then simplifies:
$$
\begin{align*}
R_{t+1} + \gamma Q(S_{t+1}, A&rsquo;) &amp;= R_{t+1} + \gamma Q(S_{t+1}, \arg\max_{a&rsquo;} Q(S_{t+1}, a&rsquo;)) \cr
&amp;= R_{t+1} + \max_{a&rsquo;}\gamma Q(S_{t+1}, a&rsquo;).
\end{align*}
$$</p>
<p>Hence Q-learning becomes
$$
\begin{equation}
Q(S_t, A_t) \leftarrow  Q(S_t, A_t) + \alpha [R_{t+1} + \max_{a&rsquo;}\gamma Q(S_{t+1}, a&rsquo;) -  Q(S_t, A_t)]
\end{equation}
$$</p>
<dl>
<dt><em>Theorem 5.5</em></dt>
<dd>Q-learning control converges to the optimal action-value function $q_*$.</dd>
</dl>
<hr>
<table>
  <thead>
      <tr>
          <th>Q-learning for Off-policy Control</th>
      </tr>
  </thead>
  <tbody>
  </tbody>
</table>
<ul>
<li>
<p>Initialize $Q(s,a)$ arbitrarily for all $a,s$ and $Q(S_T,\cdot) = 0$.</p>
</li>
<li>
<p>initialize the behavior policy $\mu$ w.r.t. $Q$.</p>
</li>
<li>
<p>For each episode:</p>
<ul>
<li>
<p>Initialize $S_0$</p>
</li>
<li>
<p>Choose $A_0$ using the policy $\mu$ (derived from $Q$).</p>
</li>
<li>
<p>Loop for each step $t = 0,&hellip;,T-1$:</p>
<ul>
<li>take action $A_t$, observe $R_{t+1}, S_{t+1}$.</li>
<li>choose action $A_{t+1}$ using policy $\mu$ derived from $Q$ (e.g., $\epsilon$-greedy).</li>
<li>incremental update $Q$  by
$$
Q(S_t, A_t) \leftarrow  Q(S_t, A_t) + \alpha [R_{t+1} + \max_{a&rsquo;}\gamma Q(S_{t+1}, a&rsquo;) -  Q(S_t, A_t)].
$$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>$Q(s,a)\to q_<em>(s,a)$ by theorem 5.5, then we obtain $\pi_</em>$ from $q_*$.</p>
<hr>

	</article>
</main>
	
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css"
  integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js"
  integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js"
  integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      
      
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
        { left: '\\(', right: '\\)', display: false },
        { left: '\\[', right: '\\]', display: true },
        
      ],
      
      throwOnError: false
    });
  });
</script>
	

	

	



  <footer class="row row-mob al-c-mob col-rev-mob sm-2-mob  jc-bt mtb-2">
  <p>
    © Copyright notice |
    <a href="https://github.com/mivinci/hugo-theme-minima" target="_blank" rel="noopener noreferrer">Minima</a> theme on
    <a href="https://gohugo.io" target="_blank" rel="noopener noreferrer">Hugo</a>
  </p>
  <p class="row gap-0_5">
    
      <a class="icon" href="https://github.com/wangchxx" title="github">
      
        <svg fill="#63636f" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
      
      </a>
    
  </p>
</footer>
</body>
</html>