[{"content":"Conditional diffusion models Given $Y$, the forward-time SDE generates $X_{t}$ conditioned on $Y$ and has $X_{t}\\sim p_{t}(\\cdot|Y)$: $$ d X_{t} = fdt + gdW_{t}, \\quad X_{0} \\sim q_{0} = p_{data}(\\cdot|Y). $$ The reverse-time SDE generates $\\bar{X}_{0}\\sim p_{0}(\\cdot|Y)=p_{data}(\\cdot|Y)$: $$ d\\bar{X}_{t} = (f-g^2 \\nabla_{x}\\log p_{t}(\\bar{X}_{t}|Y))dt + gd\\bar{W}_{t}, \\quad \\bar{X}_{T}\\sim p_{T}(\\cdot|Y)\\approx \\mathcal{N}(0, \\sigma^2_{T} I). $$ So we need to learn the conditional score $\\nabla_{x} \\log p_{t}(X_{t}|Y)$. But how to do it?\nClassifier guidance By Bayes\u0026rsquo; rule $$ p_{t}(X_{t}|Y)\\propto p_{t}(X_{t}) p(Y|X_{t}); \\quad \\nabla_{x}\\log p_{t}(X_{t}|Y) = \\nabla_{x} \\log p_{t}(X_{t}) + \\nabla_{x}\\log p_{t}(Y|X_{t}). $$Thus we can generate conditional data given $Y$ in 3 steps:\nTrain a regular score function $s_{\\theta}(X_{t}, t)\\approx \\nabla_{x}\\log p_{t}(X_{t})$ by disregarding the label $Y$. Train a time-dependent classifier $p_{\\phi}(Y|X_{t})\\approx p_{t}(Y|X_{t})$ that predicts the label $Y$ given corrupted data $X_{t}$. Use the $\\color{red}s_{\\theta}$ and $\\color{blue}p_{\\phi}$ to approximate the reverse-time conditional SDE $$ d\\bar{X}_{t} = (f - g^2({\\color{red}s_{\\theta}(X_{t},t)} +{\\color{blue} \\nabla_{x}p_{\\phi}(Y|X_{t})})) dt + g d\\bar{W}_{t}, \\quad \\bar{X}_{T}\\sim p_{T}. $$ To control the strength of the classifier guidance, we can add a weight $w$ to the SDE, and experimentally, it helps to scale the classifier gradients by $w\u003e1$. $$ d\\bar{X}_{t} = (f - g^2({\\color{red}s_{\\theta}(X_{t},t)} + w{\\color{blue} \\nabla_{x}p_{\\phi}(Y|X_{t})})) dt + g d\\bar{W}_{t}, \\quad \\bar{X}_{T}\\sim p_{T}. $$ (Dhariwal and Nichol, 2021)\nClassifier-free guidance The problem with classifier guidance is that we must train a separate classifier. Classifier-free guidance relies on $$ p(y|x)\\propto \\frac{p(x|y)}{p(x)} $$ to get $$ \\begin{aligned} d\\bar{X}_{t} \u0026= (f - g^2( \\nabla_{x}\\log p_{t}(X_{t}) + w\\nabla_{x}\\log p_{t}(Y|X_{t}) )) dt + g d\\bar{W}_{t} \\\\ \u0026= \\left(f - g^2\\left( \\nabla_{x}\\log p_{t}(X_{t}) + w(\\nabla_{x}\\log p_{t}(X_{t}|Y) - \\nabla_{x}\\log p_{t}(X_{t}) ) \\right) \\right) dt + g d\\bar{W}_{t} \\\\ \u0026= \\left(f - g^2\\left( (1-w) \\nabla_{x}\\log p_{t}(X_{t}) + w\\nabla_{x}\\log p_{t}(X_{t}|Y) \\right) \\right) dt + g d\\bar{W}_{t} \\quad \\bar{X}_{T}\\sim p_{T}.\\\\ \\end{aligned} $$ Instead of a separate classifier, we train one score network to represent both the unconditional score score $\\nabla_{x}\\log p_{t}(X_{t}) = s_{\\theta}(\\bar{X}_{t},t,\\emptyset)$ and conditional score $\\nabla_{x}\\log p_{t}(X_{t}|Y) = s_{\\theta}(\\bar{X}_{t},t,Y)$.\nInpainting Let $X_{0}\\in \\mathbb{R}^{C\\times W\\times H}$ be the true image and $\\Omega:\\mathbb{R}^{C\\times W\\times H} \\mapsto \\mathbb{R}^K, K\\leq CWH$, which subsamples pixel/channel. Our observed data is $Y = \\Omega(X_{0})$. In image inpainting, we reconstruct $Y^C$ based on $\\Omega(X_{0})$. Let $X_{t}^\\Omega = \\Omega(X_{t})$ and $X_{t}^C = \\Omega^C(X_{t})$. Assume $f$ and $g$ are defined elementwise. $$ dX_{t} = f(X_{t}, t)dt + g(t)dW_{t}, \\quad X_{0}\\sim p_{0} $$ can be decomposed into $$ (dX_{t})_{i} = f_{i}((X_{t})_{i}, t) dt + g_{i}(t)(dW_{t})_{i}, \\quad \\forall i. $$ Then the forwrad-time corruption is independent across pixels and channels. $$ d X_{t}^C = f^C(X_{t}^C, t) dt + g^C(t) dW_{t}^C, \\quad X_{0}^C = \\Omega^C(X_{0}), $$ where $f^C(\\cdot) = \\Omega^C(f(\\cdot))$, $g^C(\\cdot) = \\Omega^C(g(\\cdot))$ and $W_{t}^C = \\Omega^C(W_{t})$.\nHowever, the reverse-time generation is not independent across pixels and channels, because the score function $\\nabla_{x}\\log p_{t}(X_{t})$ does not split elementwise. So we cannot apply the sampling process of diffusion process directly to unobserved data $X_{T}^C \\to X_{0}^C$.\nWe have $$ \\begin{aligned} p_{t}(X_{t}^C|X_{0}^\\Omega) \u0026= \\int p_{t}(X_{t}^C| X_{t}^\\Omega, X_{0}^\\Omega) p_{t}(X_{t}^\\Omega|X_{0}^\\Omega) dX_{t}^\\Omega \\\\ \u0026= \\mathbb{E}_{X_{t}^\\Omega \\sim p_{t}(X_{t}^\\Omega|X_{0}^\\Omega)} [p_{t}(X_{t}^C|X_{t}^\\Omega, X_{0}^\\Omega)] \\\\ \u0026\\approx \\mathbb{E}_{X_{t}^\\Omega \\sim p_{t}(X_{t}^\\Omega|X_{0}^\\Omega)} [p_{t}(X_{t}^C|X_{t}^\\Omega)] \\\\ \u0026\\approx p_{t}(X_{t}^C| \\hat{X}_{t}^\\Omega), \\end{aligned} $$ where $\\hat{X}_{t}^\\Omega$ is a forward noise sample given $X_{0}^\\Omega$.\nThe first approximation is based on argument that $X_{0}^\\Omega$ does not additional provide much information about $X_{t}^C$ given $X_{t}^\\Omega$. For small $t$, $X_{t}^\\Omega \\approx X_{0}^\\Omega$ so the approximation holds. For large $t$, $X_{0}^\\Omega$ becomes further away from $X_{t}^C$ in the Markov process, and thus have smaller impact on $X_{t}^C$. The second approximation is a single-sample estimate of the expected value. Finally, we perform the SDE sampling via $$ \\begin{aligned} \\nabla_{x^C} \\log p_{t}(X_{t}^C|X_{0}^\\Omega) \u0026\\approx \\nabla_{x^C}\\log p_{t}(X_{t}^C|\\hat{X}_{t}^\\Omega) \\\\ \u0026= \\nabla_{x^C}\\log p_{t}([X_{t}^C;\\hat{X}_{t}^\\Omega]) \\\\ \u0026\\approx \\Omega^C(s_{\\theta}([X_{t}^C;\\hat{X}_{t}^\\Omega], t)) \\end{aligned} $$DALL-E 2 DALL-E 2\nThe DALL-E 2 model consists of 4 neural networks:\nImage encoder $f_{\\theta}$: the image encoder of a CLIP model, $Z_{img} = f_{\\theta}(X)$. Text encoder $g_{\\phi}$: the text encoder of a CLIP model, $Z_{text} = g_{\\phi}(C)$. Image decoder $h_{\\psi}$: generate samples from image embedding and text embedding (optional), $X = h_{\\psi}(Z_{img}, Z_{text})$. It can be a diffusion model or any image decoder. Prior $p_{\\omega}$: generate image embeddings given text embeddings, $Z_{img} = p_{\\omega}(Z_{text})$. It can be a diffusion model or a autoregressive model. Latent diffusion model (LDM) (Rombach et al., 2021)\nStandard diffusion model directly operates on image, which is very high-dimensional. LDM loosely decomposes the perceptual compression and semantic compression with generative modeling learning by first trimming off pixel-level redundancy with autoencoder and then generating semantic concepts with diffusion process on learned latent.\nEncoder $q_{\\phi}(Z_{0}|X)$ compress image $X$ to latent expression $Z_{0}$. Decoder $p_{\\psi}(X|Z_{0})$generates images from latent expression $Z_{0}$. Diffusion model that corrupts $Z_{0}\\to Z_{T}$ and sample $Z_{T}\\to Z_{0}$ with prior $p_{\\theta}(Z_{T}) = \\mathcal{N}(0, I)$. The VLB can be decomposed into three terms $$ \\begin{aligned} \\text{VLB}_{\\phi,\\psi, \\theta}(X) \u0026= \\mathbb{E}_{Z_{0}\\sim q_{\\phi}(\\cdot|X)} [-\\log p_{\\psi}(X|Z_{0})] + D_{KL}(q_{\\phi}(\\cdot|X)\\|p_{\\theta}(\\cdot)) \\\\ \u0026= \\mathbb{E}_{Z_{0}\\sim q_{\\phi}(\\cdot|X)} [-\\log p_{\\psi}(X|Z_{0})] + \\mathbb{E}_{Z_{0}\\sim q_{\\phi}(\\cdot|X)} [\\log q_{\\phi}(Z_{0}|X)] + \\mathbb{E}_{Z_{0}\\sim q_{\\phi}(\\cdot|X)} [-\\log p_{\\theta}(Z_{0})] \\end{aligned} $$ The first term is the reconstruction term, the second one is the negative encoder entropy and the third one is cross-entropy. Under the standard VAE setup, $q_{\\phi}(\\cdot|X)$ and $p_{\\psi}(\\cdot|Z_{0})$ are Gaussian. So sampling $Z_{0}\\sim q_{\\phi}(\\cdot|X)$ and forming the first two with the reparameterization trick and backprop is straightforward.\nFor the third one, we can deal with the cross-entropy via score matching. $$ \\text{CE}(q_{\\phi}(\\cdot|X)\\|p_{\\theta}(\\cdot)) = \\mathbb{E}_{t\\sim U[0,1]}\\left[ \\frac{w(t)}{2} \\mathbb{E}_{Z_{0}\\sim q_{\\phi}, \\epsilon\\sim\\mathcal{N}(0,I), Z_{t} = \\mu_{t}(Z_{0}) + \\sigma_{t}\\epsilon } \\right][\\|\\epsilon - \\epsilon_{\\theta}(Z_{t}, t)\\|^2] + \\frac{dZ}{2}\\log (2\\pi e \\sigma_{0}), $$ where $\\mu_{t}(Z_{0})$ is the mean of $Z_{t}$ conditioned on $Z_{0}$ under the SDE $dZ_{t} = f(t)Z_{t}dt + g(t)dW_{t}$.\nApplication: unconditional image generation Training:\nEncoder $\\mathcal{E}$: compress the original image $x$ to the latent features $z$. Prior $p_{\\theta}$: can generate latent features $z$, e.g. diffusion model or transformer. Decoder $\\mathcal{D}$: generate images $\\hat{x}$ from latent features $z$. Sampling:\nSampling latent features $z$ from the prior model $p_{\\theta}$. Generating samples through decoder $\\mathcal{D}$. Application: conditional image generation Let the prior model $p_{\\theta}$ can take the conditional input $c$ by adding $z+c$ or concate $[z;c]$.\nApplication: super resolution Let $x_{\\text{high}}$ be the high-resolution image, and $x_{\\text{low}}$ be the low-resolution image. To generate high-resolution images given low-resolution ones, we let the prior model $p_{\\theta}$ takes the low-resolution latent features $z_{\\text{low}}$ and images $z_{\\text{low}}$ as the input and generate $z_{\\text{high}}$. Then we can generate high-resolution images though the decoder given $z_{\\text{high}}$.\nTraining:\nEncoder: generate $z_{\\text{high}}$ from $x_{\\text{high}}$. Then degrade $x_{\\text{high}}$ to $x_{\\text{low}}$, and get $z_{\\text{low}}$. Prior: let the prior model to learn $z_{\\text{high}}$ given $z_{\\text{low}}$ and $x_{\\text{low}}$. Decoder: generate $x_{\\text{high}}$ with $z_{\\text{high}}$. ","permalink":"http://localhost:1313/posts/diffusion_4_conditional_df/","summary":"\u003ch1 id=\"conditional-diffusion-models\"\u003eConditional diffusion models\u003c/h1\u003e\n\u003cp\u003eGiven $Y$, the forward-time SDE generates $X_{t}$ conditioned on $Y$ and has $X_{t}\\sim p_{t}(\\cdot|Y)$:\n\u003c/p\u003e\n$$\nd X_{t} = fdt + gdW_{t}, \\quad X_{0} \\sim q_{0} = p_{data}(\\cdot|Y).\n$$\u003cp\u003e\nThe reverse-time SDE generates $\\bar{X}_{0}\\sim p_{0}(\\cdot|Y)=p_{data}(\\cdot|Y)$:\n\u003c/p\u003e\n$$\nd\\bar{X}_{t} = (f-g^2 \\nabla_{x}\\log p_{t}(\\bar{X}_{t}|Y))dt + gd\\bar{W}_{t}, \\quad \\bar{X}_{T}\\sim p_{T}(\\cdot|Y)\\approx \\mathcal{N}(0, \\sigma^2_{T} I).\n$$\u003cp\u003e\nSo we need to learn the conditional score $\\nabla_{x} \\log p_{t}(X_{t}|Y)$. But how to do it?\u003c/p\u003e\n\u003ch2 id=\"classifier-guidance\"\u003eClassifier guidance\u003c/h2\u003e\n\u003cp\u003eBy Bayes\u0026rsquo; rule\n\u003c/p\u003e","title":"DL | Diffusion Models 4 - Conditional Diffusion Models"},{"content":"Understand DDMP from VLB In the Diffusion-models-1 we have introduced the DDMP model $$ X_t|X_{t-1} = \\mathcal{N}(\\sqrt{1-\\beta_t}X_{t-1}, \\beta_t I), \\quad X_t|X_0 = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t} X_0, (1-\\bar{\\alpha}_{t})I), $$ with $X_{0}\\sim q_{0} = p_{data}$, $\\alpha_{t} = 1-\\beta_{t}$ and $\\bar{\\alpha}_{t} = \\prod_{s=1}^t (1-\\beta_{s})$.\nThe loss function is derived by minimizing the negative log-likelihood $-\\log p_{\\theta}(X_{0})$ with a variational lower bound $L_{VLB}$ $$ L_{VLB} = \\mathbb{E}_{q}\\left[ \\log \\frac{q(x_{1:T}|x_{0})}{p_{\\theta}(x_{0:T})} \\right] = \\sum_{t=0}^T L_{t} $$ with $$ L_{t} = D_{KL}(q(x_{t}|x_{t+1},x_{0})\\| p_{\\theta}(x_{t}|x_{t+1})),\\; 1\\leq t\\leq T-1. $$ Since the conditional distribution $X_{t}|(X_{t-1}, X_{0})$ is Gaussian $$ q(x_{t-1}|x_t, x_0) = \\mathcal{N}(\\tilde{\\mu}_t(x_t, x_0), \\tilde{\\beta}_t I), $$ where $$ \\begin{equation} \\tilde{\\mu}_t(x_t, x_0) := \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}x_t +\\frac{\\bar{\\alpha}_{t-1}\\beta_t}{1-\\bar{\\alpha}_t}x_0;\\quad \\tilde{\\beta}_t:= \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}\\beta_t. \\end{equation} $$ If we model the conditional distribution $X_{t-1}|X_{t}$ as a Gaussian $X_{t-1}|X_{t} \\sim \\mathcal{N}(\\mu_\\theta(X_t, t), \\Sigma_\\theta(X_t, t))$, the loss function is basically the KL-divergence between two Gaussian distributions, which has the closed form $$ \\begin{equation} L_{t} = \\frac{1}{2\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\|\\tilde{\\mu}(x_{t},x_{0}) - \\mu_{\\theta}(x_{t},t)\\|^2 + C. \\end{equation} $$ Because $q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha})I)$ we can express $x_{t}|x_{0}$ as $x_{t}(x_{0}, \\epsilon) = \\sqrt{ \\bar{\\alpha}_{t} }x_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t} }\\epsilon$ for $\\epsilon\\sim\\mathcal{N}(0, I)$, therefore we can rewrite $\\tilde{\\mu}_{t}(x_{t}, x_{0})$ as $$ \\tilde{\\mu}_{t} = \\tilde{\\mu}(x_{0}, \\epsilon) = \\frac{1}{\\sqrt{ \\alpha_{t} }}\\left( x_{t} - \\frac{1-\\alpha_{t}}{\\sqrt{ 1-\\bar{\\alpha}_{t} }} \\epsilon \\right). $$ Similarly, we can also model $\\mu_{\\theta}(x_{t}, t)$ as $$ \\mu_{\\theta}(x_{t}, t) = \\frac{1}{\\sqrt{ \\alpha_{t} }}\\left( x_{t} - \\frac{1-\\alpha_{t}}{\\sqrt{ 1-\\bar{\\alpha}_{t} }} \\epsilon_{\\theta}(x_{t,t}) \\right), $$ where $\\epsilon_{\\theta}$ is an approximation of the noise $\\epsilon$. Overall, the loss function $L_{t}$ becomes $$ L_{t} = \\mathbb{E}_{x_{0}, \\epsilon} \\left[\\frac{(1-\\alpha_{t})^2}{2 \\alpha_{t}(1-\\bar{\\alpha}_{t})\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\| \\epsilon - \\epsilon_{\\theta}(\\sqrt{ \\bar{\\alpha}_{t} }x_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t} }\\epsilon,t)\\|^2 \\right] + C. $$Understand DDMP from score matching Consider the random variable $$ Y = X + \\sigma Z, \\quad X\\sim P_{X}, \\quad Z\\sim \\mathcal{N}(0,I). $$ By definition, $p_{Y|X}$ is Gaussian, but in general $p_{X|Y}$ is not Gaussian since we did not assume $p_{X}$ is Gaussian. But $p_{X|Y}$ is approximately Gaussian in the limit of $\\sigma\\to 0$. $$ p_{X|Y}(x|y) \\approx \\mathcal{N}(y + \\sigma^2\\nabla \\log p_{Y}(y), \\sigma^2 I). $$ See Proof of Gaussian. And for $Y = \\gamma X + \\sigma Z$ with $\\gamma \\neq 0$, then in the limit of $\\sigma\\to 0$,\n$$ p_{X|Y}(x|y) \\approx \\mathcal{N}\\left( \\frac{1}{\\gamma}(y + \\sigma^2\\nabla \\log p_{Y}(y)), \\frac{\\sigma^2}{\\gamma^2} I \\right). $$ In the setting of DDMP, we know the forward process is $$ X_t|X_{t-1} = \\mathcal{N}(\\sqrt{1-\\beta_t}X_{t-1}, \\beta_t I), \\quad X_t|X_0 = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t} X_0, (1-\\bar{\\alpha})I), \\quad \\bar{\\alpha}_{t} = \\prod_{s=1}^t(1-\\beta_{s}). $$ If $\\beta_{t}$ is small enough, approximately the backward process is $$ p(x_{t-1}|x_{t}) \\approx \\mathcal{N}(\\mu(x_{t},t), \\beta_{t}I), $$ with $\\mu(x_{t},t) = \\frac{1}{\\sqrt{ 1-\\beta_{t} }}(x_{t} - \\beta_{t}\\nabla \\log p_{t}(x_{t}))$. Similarly, we can a model $p_{\\theta}(x_{t-1}|x_{t})$ to learn it $$ p_{\\theta}(x_{t-1}|x_{t}) = \\mathcal{N}(\\mu_{\\theta}(x_{t}, t), \\Sigma_{\\theta}(x_{t},t)), \\quad \\mu_{\\theta}(x_{t},t) = \\frac{1}{\\sqrt{ 1-\\beta_{t} }}(x_{t} - \\beta_{t}s_{\\theta}(x_{t},t)) $$ Essentially, we are using $s_{\\theta}$ to learn the score function $\\nabla_{x}\\log p$. Naturally, we can think about using the loss function MSE of means or KL-divergence. There is not a big difference between them, so we take KL-divergence as an example $$ \\begin{aligned} \\mathcal{L}(\\theta) \u0026= \\sum_{t=1}^T \\lambda_{t} KL(p(x_{t-1}|x_{t}\\|p_{\\theta}(x_{t-1}|x_{t}))) \\\\ \u0026= \\sum_{t=1}^T\\lambda_{t} \\frac{1}{2\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\|\\mu(x_{t},x_{0}) - \\mu_{\\theta}(x_{t},t)\\|^2 + C\\\\ \u0026= \\sum_{t=1}^T\\lambda_{t} \\frac{(1-\\alpha_{t})^2}{2 \\alpha_{t}\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\| \\nabla_{x}\\log p_{t}(x_{t}) - s_{\\theta}(x_{t},t)\\|^2 + C \\end{aligned} $$ Here we see the form of score matching. However, the score function $\\nabla_{x}\\log p_{t}$ is inaccessible, so we cannot compute the above loss function. Recall that $X_{t}|X_{0}$ is a known Gaussian, thus we can try to use $\\nabla_{x}\\log p_{t|0}(x_{t}|x_{0})$ in the loss function. Moreover, for Gaussian distribution $Y\\sim \\mathcal{N}(\\mu, \\sigma^2 I)$, the score function can be easily computed as $$ \\nabla_{y}\\log p_{Y}(Y) \\overset{\\mathcal{D}}{=} - \\frac{\\epsilon}{\\sigma}, \\quad \\epsilon \\sim \\mathcal{N}(0,I). $$ Like what we have done for DDPM VLB loss function, we use $\\epsilon_{\\theta} = -\\sqrt{ 1-\\bar{\\alpha}_{t} } s_{\\theta}$. Now we can rewrite the loss function as $$ \\begin{aligned} \\mathcal{L}(\\theta) \u0026= \\sum_{t=1}^T\\lambda_{t} \\frac{(1-\\alpha_{t})^2}{2 \\alpha_{t}\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\| \\nabla_{x}\\log p_{t}(x_{t}) - s_{\\theta}(x_{t},t)\\|^2 + C \\\\ \u0026= \\sum_{t=1}^T\\lambda_{t} \\frac{(1-\\alpha_{t})^2}{2 \\alpha_{t}\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\mathbb{E}_{x_{0}\\sim p_{0}} \\left[\\| \\nabla_{x}\\log p_{t|0}(x_{t}|x_{0}) - s_{\\theta}(x_{t},t)\\|^2 \\right] + C \\\\ \u0026= \\sum_{t=1}^T\\lambda_{t} \\frac{(1-\\alpha_{t})^2}{2 \\alpha_{t}\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\mathbb{E}_{x_{0}\\sim p_{0}, \\epsilon\\sim\\mathcal{N}(0,I)} \\left[\\| - \\frac{\\epsilon}{\\sqrt{ 1-\\bar{\\alpha}_{t} }} - s_{\\theta}(x_{t},t)\\|^2 \\right] + C \\\\ \u0026= \\sum_{t=1}^T\\lambda_{t} \\frac{(1-\\alpha_{t})^2}{2 \\alpha_{t} (1-\\bar{\\alpha}_{t})\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\mathbb{E}_{x_{0}\\sim p_{0}, \\epsilon\\sim\\mathcal{N}(0,I)} \\left[\\| \\epsilon- \\epsilon_{\\theta}(\\sqrt{ \\bar{\\alpha}_{t} } x_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t} }\\epsilon,t)\\|^2 \\right] + C, \\\\ \\end{aligned} $$ which is exactly the VLB loss function of DDMP.\nDDMP is VP SDE with SDE sampling DDMP forward process in the limit $\\beta_{t}\\to 0$ $$ X_{t+1} = \\sqrt{ 1-\\beta_{t} } X_{t} + \\sqrt{ \\beta_{t} } Z_{t} \\approx \\left( 1-\\frac{\\beta_{t}}{2} \\right)X_{t} + \\sqrt{ \\beta_{t} }Z_{t}. $$ Therefore when the timestep goes to zero, the DDMP forward process converges to the following VP SDE: $$ dX_{t} = -\\frac{1}{2}\\beta_{t} X_{t} dt + \\sqrt{ \\beta_{t} }dW_{t}. $$DDMP sampling process is $$ \\bar{X}_{t-1} = \\frac{1}{\\sqrt{ 1-\\beta_{t} }}\\left( \\bar{X}_{t} - \\frac{\\beta_{t}}{\\sqrt{ 1-\\bar{\\alpha}_{t} }} \\epsilon_{\\theta}(\\bar{X}_{t},t) \\right) + \\sigma_{t} Z_{t}, $$ where $\\sigma_{t}$ a unlearned constant used in $\\Sigma_{\\theta}(x_{t},t) = \\sigma_{t}^2I$. We set $\\sigma_{t}^2 = \\beta_{t}$. If $\\beta_{t}$ is slowly varying and $\\beta_{t}\\to 0$, we have $$ \\bar{\\alpha}_{t} = \\prod_{s=0}^T (1-\\beta_{s}) \\approx \\prod_{s=0}^T e^{-\\beta_{s}} \\approx \\exp\\left( -\\int_{0}^t \\beta_{s} ds\\right). $$ We the approximate the discrete DDMP sampling process as $$ \\bar{X}_{t-1} \\approx \\left( 1+\\frac{\\beta_{t}}{2} \\right)\\bar{X}_{t} - \\frac{\\beta_{t}}{\\sqrt{ 1- \\exp\\left( -\\int_{0}^t \\beta_{s} ds\\right)}} \\epsilon_{\\theta}(\\bar{X}_{t}, t) + \\sqrt{ \\beta_{t} }Z_{t} $$ This implies the reverse-time SDE $$ d\\bar{X}_{t} = \\left( -\\frac{\\beta_{t}}{2} \\bar{X}_{t} + \\frac{\\beta_{t}}{\\sqrt{ 1- \\exp\\left( -\\int_{0}^t \\beta_{s} ds\\right)}} \\epsilon_{\\theta}(\\bar{X}_{t}, t) \\right)dt + \\sqrt{ \\beta_{t} } d \\bar{W}_{t}. $$By the Anderson\u0026rsquo;s theorem, we can get the reverse-time is $$ \\begin{aligned} d \\bar{X}_{t} \u0026= \\left( -\\frac{\\beta_{t}}{2}\\bar{X}_{t} - \\beta_{t}\\nabla_{x}\\log p_{t}(\\bar{X}_{t}) \\right)dt + \\sqrt{ \\beta_{t} }d\\bar{W}_{t} \\\\ \u0026\\approx \\left( -\\frac{\\beta_{t}}{2}\\bar{X}_{t} - \\beta_{t}\\nabla_{x}\\log p_{t|0}(\\bar{X}_{t}) \\right)dt + \\sqrt{ \\beta_{t} }d\\bar{W}_{t} \\\\ \u0026\\approx \\left( -\\frac{\\beta_{t}}{2}\\bar{X}_{t} + \\beta_{t} \\frac{\\epsilon_{\\theta}(\\bar{X}_{t},t)}{\\sigma_{t}} \\right)dt + \\sqrt{ \\beta_{t} }d\\bar{W}_{t} ;\\quad \\text{by } \\nabla_{x}\\log p_{t|0}(x) = - \\frac{\\epsilon}{\\sigma_{t}},\\\\ \\end{aligned} $$ where $\\sigma_{t}^2$ is the variance of $X_{t}|X_{0}$. We have stated in Diffusion-models-2#Examples with O-U process#VP SDE that the VP SDE has the property that $X_{t}|X_{0}$ follows a Gaussian distribution with $$ X_{t}|X_{0} \\sim \\mathcal{N}\\left(\\exp\\left( -\\frac{1}{2}\\int_{0}^t\\beta_{s} ds\\right) X_{0}, \\left(1- \\exp\\left( -\\int_{0}^t \\beta_{s} ds\\right)\\right)I\\right). $$ Therefore substituting $\\sigma_{t} = \\sqrt{ 1- \\exp\\left( -\\int_{0}^t \\beta_{s} ds\\right)}$ we get $$ d \\bar{X}_{t} \\approx \\left( -\\frac{\\beta_{t}}{2}\\bar{X}_{t} + \\beta_{t} \\frac{\\epsilon_{\\theta}(\\bar{X}_{t},t)}{\\sqrt{ 1- \\exp\\left( -\\int_{0}^t \\beta_{s} ds\\right)}} \\right)dt + \\sqrt{ \\beta_{t} }d\\bar{W}_{t} $$ Therefore the reverse-time SDE derived from the sampling process of DDMP is consistent to the reverse-time SDE of VP SDE.\nDDIM is VP SDE with ODE sampling The loss function of DDPM depends on $q(x_{t}|x_{0})$, but not directly on the joint $q(x_{1:T}|x_{0})$. Thus DDIM explore alternative forward process that non-Markovian but with the same marginals $q(x_{t}|x_{0})$. Specifically, $$ \\begin{aligned} q_{\\rho}(x_{1},\\dots x_{T}|x_{0}) \u0026= q_{\\rho}(x_{T}|x_{0}) \\prod_{t=1}^{T-1}q_{\\rho}(x_{t}|x_{t+1}, x_{0}) \\\\ q_{\\rho}(x_{T}|x_{0}) \u0026= \\mathcal{N}(\\sqrt{ \\bar{\\alpha}_{t} }x_{0}, (1-\\bar{\\alpha}_{T})I) \\\\ q_{\\rho}(x_{t-1}|x_{t}, x_{0}) \u0026= \\mathcal{N}\\left( \\sqrt{\\bar{\\alpha}_{t-1} }x_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t-1} - \\rho_{t}^2 }\\frac{x_{t}- \\sqrt{ \\bar{\\alpha}_{t}}x_{0}}{\\sqrt{ 1-\\bar{\\alpha}_{t} }} , \\rho_{t}^2 I\\right) \\end{aligned} $$ The transition kernel $X_{0}\\mapsto X_{T}$ and $(X_{0},X_{t+1})\\mapsto X_{t}$ are chosen to make sure that the marginals of DDIM match the marginals of DDMP: $$ q(x_{t}|x_{0}) = \\mathcal{N}(\\sqrt{ \\bar{\\alpha}_{t} }x_{0}, (1-\\bar{\\alpha}_{t})I). $$ We can prove it by induction: $$ \\begin{align} q_{\\rho}(x_{t+1}|x_{0}) \u0026= \\mathcal{N}(\\sqrt{ \\bar{\\alpha}_{t+1} }x_{0}, (1-\\bar{\\alpha}_{t+1})I) \\\\ q_{\\rho}(x_{t}|x_{0}) \u0026= \\int q_{\\rho}(x_{t+1}|x_{0}) q_{\\rho}(x_{t}|x_{t+1}, x_{0}) dx_{t+1}. \\end{align} $$ Since both $q_{\\rho}(x_{t+1}|x_{0})$ and $q_{\\rho}(x_{t}|x_{t+1}, x_{0})$ are Gaussian, we have that $q(x_{t}|x_{0})$ is Gaussian with mean $\\sqrt{ \\bar{\\alpha}_{t} }x_{0}$ and variance $(1-\\bar{\\alpha}_{t})I$.\nSince DDIM and DDPM have the same conditional marginals, their conditional and unconditional score functions are the same. Given a DDMP, it is unnecessary to retrain it. The contribution of DDIM is to find a faster sampling method.\nThe DDIM sampling is done with $$ p_{\\theta}(x_{t-1}|x_{t}) = q_{\\rho}(x_{t-1}|x_{t}, x_{0}). $$ However, the $x_{0}$ is unknown. But it has an unbiased estimator through $$ \\begin{aligned} X_{t} \u0026= \\sqrt{ \\bar{\\alpha}_{t} }X_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t} }\\epsilon \\\\ \\hat{X}_{0} \u0026= \\mathbb{E}[X_{0}|X_{t}] \\approx \\frac{X_{t} - \\sqrt{ 1-\\bar{\\alpha}_{t}}\\epsilon_{\\theta}(X_{t}, t)}{\\sqrt{ \\bar{\\alpha}_{t} }} \\end{aligned}, $$ where $\\epsilon_{\\theta}$ is our trained model that predicts the added noise from $X_{t}$. Then the sampling process is given via: $$ \\begin{align} \\bar{X}_{t-1} \u0026= \\sqrt{ \\bar{\\alpha}_{t-1} }\\hat{X}_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t-1} - \\rho_{t}^2 } \\frac{\\bar{X}_{t} - \\sqrt{ \\bar{\\alpha}_{t} }X_{0}}{\\sqrt{ 1-\\bar{\\alpha}_{t} }} + \\rho_{t} Z_{t} \\\\ \u0026= \\sqrt{ \\bar{\\alpha}_{t-1} }\\hat{X}_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t-1} - \\rho_{t}^2 } \\frac{\\bar{X}_{t} - \\sqrt{ \\bar{\\alpha}_{t} }\\hat{X}_{0}}{\\sqrt{ 1-\\bar{\\alpha}_{t} }} + \\rho_{t} Z_{t} \\\\ \u0026= \\sqrt{ \\bar{\\alpha}_{t-1} }\\hat{X}_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t-1} - \\rho_{t}^2 } \\epsilon_{\\theta}(\\bar{X}_{t}, t)+ \\rho_{t} Z_{t} \\\\ \\end{align} $$Recall that in DDMP, $q(x_{t-1}|x_{t}, x_{0}) = \\mathcal{N}(\\tilde{\\mu}_{t}(x_{t},x_{0}), \\tilde{\\beta}_{t}I)$, therefore we have $$ \\tilde{\\beta}_{t} = \\rho_{t}^2 = \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_{t}}\\beta_{t}. $$ It other words, when $\\rho_{t}^2 = \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_{t}}\\beta_{t}$ the sampling process becomes DDMP. The special case of $\\rho_{t} = 0$ makes the sampling process deterministic, which is named the DDIM. If we rewrite the above sampling process in one-step form, we have $$ \\begin{align} \\bar{X}_{t-1} \u0026= \\frac{1}{\\sqrt{ 1-\\beta_{t} }} \\bar{X}_{t} - \\left( \\frac{\\sqrt{ 1-\\bar{\\alpha}_{t} }}{\\sqrt{1-\\beta_{t}}} - \\sqrt{ 1-\\bar{\\alpha}_{t-1} } \\right)\\epsilon_{\\theta}(\\bar{X}_{t},t) \\\\ \u0026= \\frac{1}{\\sqrt{ 1-\\beta_{t} }} \\bar{X}_{t} - \\left( \\frac{\\sqrt{ 1-\\bar{\\alpha}_{t} }}{\\sqrt{1-\\beta_{t}}} - \\sqrt{ 1- \\frac{\\bar{\\alpha}_{t}}{1-\\beta_{t}} } \\right)\\epsilon_{\\theta}(\\bar{X}_{t},t) \\end{align} $$If $\\beta_{t}$ is slowly varying and $\\beta_{t}\\to 0$, we have $$ \\begin{align} \\bar{X}_{t-1} \u0026\\approx \\left( 1 + \\frac{\\beta_{t}}{2} \\right)\\bar{X}_{t} - \\frac{\\beta_{t}}{2\\sqrt{ 1-\\bar{\\alpha}_{t} }}\\epsilon_{\\theta}(\\bar{X}_{t},t) \\\\ \u0026\\approx \\left( 1+\\frac{\\beta_{t}}{2} \\right)\\bar{X}_{t} - \\frac{\\beta_{t}}{2\\sqrt{ 1- \\exp\\left( -\\int_{0}^t \\beta_{s} ds\\right)}} \\epsilon_{\\theta}(\\bar{X}_{t}, t) \\\\ \u0026\\approx \\left( 1+\\frac{\\beta_{t}}{2} \\right)\\bar{X}_{t} + \\frac{\\beta_{t}}{2}\\nabla_{x}\\log p_{t|0}(\\bar{X}_{t}) \\end{align} $$ It agrees with the reverse-time ODE of VP SDE: $$ dX_{t} = - \\frac{\\beta_{t}}{2} X_{t} dt + \\sqrt{ \\beta_{t} }dW_{t}. $$Noise conditional score network (NCSN) is VE SDE with SDE sampling The forward-time process of NCSN is $$ X_{t} = X_{t-1} + \\sqrt{ \\sigma_{t}^2 - \\sigma_{t-1}^2} Z_{t},\\quad X_{0}\\sim p_{data}. $$ Since $\\sqrt{ \\sigma_{t}^2 - \\sigma_{t-1}^2} = \\sqrt{\\frac{ \\sigma_{t}^2 - \\sigma_{t-1}^2}{\\Delta_{t}}} \\sqrt{ \\Delta_{t} }$, it converges to $$ dX_{t} = \\sqrt{ \\frac{d(\\sigma_{t}^2)}{dt} }dW_{t}. $$ The sampling process of NCSN is defined as $$ \\begin{aligned} \\bar{X}_{t-1} \u0026= \\bar{X}_{t} +(\\sigma_{t}^2 - \\sigma_{t-1}^2)s_{\\theta} + \\sqrt{ \\sigma_{t}^2 - \\sigma_{t-1}^2} Z_{t} \\\\ \u0026\\approx \\bar{X}_{t} +(\\sigma_{t}^2 - \\sigma_{t-1}^2)\\nabla_{x}\\log p_{t}(\\bar{X}_{t}) + \\sqrt{ \\sigma_{t}^2 - \\sigma_{t-1}^2} Z_{t} \\\\ \\end{aligned} $$ It can be written as $$ d\\bar{X}_{t} = - \\frac{d(\\sigma_{t}^2)}{dt} \\nabla_{x}\\log p_{t}(\\bar{X}_{t}) dt + \\sqrt{\\frac{d(\\sigma_{t}^2)}{dt} }d\\bar{W}_{t}, $$ which is the reverse-time SDE of VE SDE.\nSupplementary Tweedie\u0026rsquo;s formula Consider the random variable $$ Y = X + \\sigma Z, \\quad X\\sim p_{X}, \\quad Z\\sim\\mathcal{N}(0,I), $$ where $p_{X}$ is not necessarily Gaussian, then $$ \\begin{align} \\mathbb{E}[X|Y] \u0026= Y + \\sigma^2 \\nabla_{y}\\log p_{y}(Y) \\\\ \\mathrm{Var}(X|Y) \u0026= \\sigma^2I + \\sigma^4 \\nabla_{y}^2\\log p_{y}(Y). \\end{align} $$ Easily, we can see that if $Y = \\gamma X+\\sigma Z$, $\\gamma\\neq 0$, then we have $$ \\begin{align} \\mathbb{E}[X|Y] \u0026= \\frac{1}{\\gamma}(Y + \\sigma^2 \\nabla_{y}\\log p_{y}(Y)) \\\\ \\\\ \\mathrm{Var}(X|Y) \u0026= \\frac{\\sigma^2}{\\gamma^2}(I + \\sigma^2\\nabla_{y}^2\\log p_{y}(Y)). \\end{align} $$ See proof here\nApproximation Gaussian ^08dcf7 $$ \\begin{aligned} p_{X|Y}(x) \u0026= \\frac{p_{Y|X}(y) p_{X}(x)}{p_{Y}(y)} \\\\ \u0026= \\frac{p_{Y|X}(y) p_{X}(x)}{\\int p_{Y|X}(y)p_{X}(x)dx} \\\\ \\end{aligned} $$ Note that $p_{Y|X}$ is Gaussian and $$ \\begin{aligned} p_{Y|X} p_{X}(x) \u0026= \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|y-x\\|^2 \\right) p_{X}(x) \\\\ \u0026= \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|y-x\\|^2 \\right) (p_{X}(y) + \\langle \\nabla p_{X}(y), x-y \\rangle + O(\\|x-y\\|^2) ) \\end{aligned} $$ Then the denominator can be calculated as $\\text{denom} = p_{X}(y) + O(\\sigma^2)$ since $$ \\begin{aligned} \\int \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|y-x\\|^2 \\right) p_{X}(y) dx \u0026= p_{X}(y) \\\\ \\int \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|y-x\\|^2 \\right) (x-y) dx \u0026= 0 \\\\ \\int \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|y-x\\|^2 \\right) O(\\|x-y\\|^2) \u0026= O(\\sigma^2). \\end{aligned} $$It follows that $$ \\begin{aligned} p_{X|Y}(x) \u0026= \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|y-x\\|^2 \\right) \\frac{p_{X}(y) + \\langle \\nabla p_{X}(y), x-y \\rangle + O(\\|x-y\\|^2)}{p_{X}(y)+ O(\\sigma^2)} \\\\ \u0026= \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|y-x\\|^2 \\right) (1 + \\langle \\nabla\\log p_{X}(y), x-y \\rangle + \\text{h.o.t.} ) \\\\ \u0026= \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|x-y\\|^2 \\right) \\exp(\\langle \\nabla \\log p_{X}(y), x-y \\rangle ) + \\text{h.o.t.}\\\\ \u0026= \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|x-y - \\sigma^2\\nabla\\log p_{Y}(y)\\|^2 + \\text{h.o.t.} \\right) + \\text{h.o.t.} \\\\ \u0026\\approx \\mathcal{N}(y+\\sigma^2\\nabla\\log p_{Y}(y), \\sigma^2 I). \\end{aligned} $$","permalink":"http://localhost:1313/posts/diffusion_3_from_a_unified_prospective/","summary":"\u003ch2 id=\"understand-ddmp-from-vlb\"\u003eUnderstand DDMP from VLB\u003c/h2\u003e\n\u003cp\u003eIn the \u003ca href=\"/posts/diffusion_2_preliminary_sde/\"\u003eDiffusion-models-1\u003c/a\u003e we have introduced the DDMP model\n\u003c/p\u003e\n$$\nX_t|X_{t-1} = \\mathcal{N}(\\sqrt{1-\\beta_t}X_{t-1}, \\beta_t I), \\quad X_t|X_0 = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t} X_0, (1-\\bar{\\alpha}_{t})I),\n$$\u003cp\u003e\nwith $X_{0}\\sim q_{0} = p_{data}$, $\\alpha_{t} = 1-\\beta_{t}$ and $\\bar{\\alpha}_{t} = \\prod_{s=1}^t (1-\\beta_{s})$.\u003c/p\u003e\n\u003cp\u003eThe loss function is derived by minimizing the negative log-likelihood $-\\log p_{\\theta}(X_{0})$ with a variational lower bound $L_{VLB}$\n\u003c/p\u003e\n$$\nL_{VLB} = \\mathbb{E}_{q}\\left[ \\log  \\frac{q(x_{1:T}|x_{0})}{p_{\\theta}(x_{0:T})}  \\right]  = \\sum_{t=0}^T L_{t}\n$$\u003cp\u003e\nwith\n\u003c/p\u003e\n$$\nL_{t} = D_{KL}(q(x_{t}|x_{t+1},x_{0})\\| p_{\\theta}(x_{t}|x_{t+1})),\\; 1\\leq t\\leq T-1. \n$$\u003cp\u003e\nSince the conditional distribution $X_{t}|(X_{t-1}, X_{0})$ is Gaussian\n\u003c/p\u003e","title":"DL | Diffusion Models 3 - Understanding DDMP from a Unified Prospective"},{"content":"Basics ODE definition\nConsider the ordinary differential equation (ODE) $$ \\frac{dX}{dt}(t) = f(X(t), t), $$ which we also express as $dX_{t} = f(X_{t}, t)dt$, where $X(t), f(X(t),t)\\in \\mathbb{R}^d$. Then, $\\{X(t)\\}_{t}$ is a deterministic curve.\nWe can define the ODE by the limit $$ X_{k+1} = X_{k} + \\Delta _{t} f(X_{k}, k\\Delta_{t}), \\quad k = 0,1,2,\\dots, $$ under $\\Delta_{t}\\to 0$ with $t = k\\Delta_{t}$. Precisely, $\\left\\{ X_{\\left\\lfloor \\frac{t}{\\Delta_{t}} \\right\\rfloor} \\right\\}_{t} \\to \\{X_{t}\\}_{t}$ uniformly on compact intervals.\nODE solution\nWe say that $\\{X(t)\\}_{t=0}^T$ solves ODE if it satisfies the differential form of ODE $$ \\frac{dX}{dt}(t) = f(X(t),t), $$ or the integral form of ODE $$ X(t) = X_{0} + \\int_{0}^t f(X(s),s) ds. $$SDE definition\nConsider the stochastic differential equation (SDE) $$ dX_{t} = f(X_{t}, t) dt + g(X_{t}, t) dW_{t}, $$ where $X_{t}, f(X_{t}, t)\\in\\mathbb{R}^d, g(X_{t}, t)\\in\\mathbb{R}^{d\\times d}$, and $W_{t}$ is a $d$-dimensional Brownian motion or Wiener process. Then, $\\{X_{t})\\}_{t}$ is a stochastic process.\nWe can define the SDE by the limit $$ X_{k+1} = X_{k} + \\Delta _{t} f(X_{k}, k\\Delta_{t}) + g(k \\Delta_{t})\\sqrt{ \\Delta_{t} } Z_{k}, \\quad k = 0,1,2,\\dots, $$ under $\\Delta_{t}\\to 0$ with $t = k\\Delta_{t}$ and $Z_{k}\\sim \\mathcal{N}(0,I)$ i.i.d. Precisely, $\\left\\{ X_{\\left\\lfloor \\frac{t}{\\Delta_{t}} \\right\\rfloor} \\right\\}_{t} \\to \\{X_{t}\\}_{t}$ uniformly on compact intervals.\nSDE solution\nWe say that $\\{X_{t}\\}_{t=0}^T$ is a solution path for SDE if it is nice (right-continuous with left limits) with probability distribution defined by $$ X_{t} = X_{0} + \\int_{0}^t f(X_{s}, s) ds + \\int_{0}^t g(X_{s},s) d W_{s}, $$ where the Ito stochastic integral is defined as $$ \\int_{0}^t g(X_{s},s) dW_s = \\lim_{ \\Delta_{t} \\to 0 } \\sum_{k=0}^{\\lfloor t / \\Delta_{t} \\rfloor } g(X_{k\\Delta_{t}}, k\\Delta_{t}) \\sqrt{ \\Delta_{t} } Z_{k}. $$Fokker-Planck (FP) equation Given a fixed path $\\{X_{t}\\}_{t=0}^T$, we cannot determine whether it was generated from the SDE. But we can determine it with a distribution $p(x_{t}, t\\in[0.T])$. Now we consider a weaker notion: the marginal distribution $\\{p_{t}\\}_{t=0}^T$ s.t. $X_{t}\\sim p_{t}$ for all $t\\in[0, T]$.\nBut how does $p_{t}$ evolve as a function of time $t$? Fokker-Planck equation (also called forward Kolmogorov equaiton) states that for $d = 1$ $$ \\frac{\\partial p_{t}}{\\partial_{t}} = - \\frac{\\partial}{\\partial_{x}} (f p_{t}) + \\frac{g^2}{2} \\frac{\\partial^2}{\\partial x^2} p_{t} $$ and for multi-dimensional case $d\u003e1$, $$ \\partial_{t}p_{t} = - \\nabla_{x}\\cdot(f p_{t}) + \\frac{1}{2} \\mathrm{Tr}(g^T \\nabla^2_{x}p_{t} g). $$ See proof of Fokker-Planck equation.\nOrnstein-Uhlenbeck (O-U) process The O-U process $\\{X_{t}\\}$ is defined by the following SDE $$ dX_{t} = -\\beta X_{t} dt + \\sigma dW_{t} $$ Using Ito formula with $f(x,t) = e^{\\beta t}x$, we get $$ df(X_{t},t) = f_{t}dt + f_{x}dX_{t} + \\frac{1}{2}f_{x x} dt = \\beta e^{\\beta t}X_{t} dt + e^{\\beta t} dX_{t} = \\sigma e^{\\beta t} dW_{t}. $$ Integrating gives $$ X_{t} = e^{-\\beta t}X_{0} + \\sigma \\int_{0}^t e^{-\\beta (t-s)} dW_{s}. $$ This process is Gaussian being a linear combination of the Gaussian process $W_{t}$. Its mean and variance are (using the Ito isometry) $$ \\begin{align} \\mathbb{E} X_{t} \u0026= e^{-\\beta t} X_{0} \\\\ \\mathbb{E}(X_{t} - \\mathbb{E}X_{t})^2 \u0026= \\sigma^2 \\int_{0}^t e^{-2\\beta(t-s)} ds = \\frac{\\sigma^2}{2\\beta} (1- e^{-2\\beta t}). \\end{align} $$ Thus when $\\beta \u003e 0$, $X_{t} \\to \\mathcal{N}\\left( 0, \\frac{\\sigma^2}{2\\beta} \\right)$ in distribution as $t\\to \\infty$ if $X_{0} = 0$. With the density function $p_{t}$ of $X_{t}$, we can verify that $p_{t}$ satisfies the FP equation by direct calculations $$ \\partial_{t} p_{t} = -\\partial_{x} (fp_{t}) + \\frac{g^2}{2}\\partial_{x}^2 p_{t} = 0. $$ODE Recall the definition of ODE $$ X_{k+1} = X_{k} + \\Delta _{t} f(X_{k}, k\\Delta_{t}), \\quad k = 0,1,2,\\dots, $$ under $\\Delta_{t}\\to 0$ with $t = k\\Delta_{t}$. This is the forward-time ODE. It tells us how to simulate $X_{T}$ given $X_{0}$.\nTo simulate the reverse-time ODE is easy $$ X_{k-1} = X_{k} - \\Delta _{t} f(X_{k}, k\\Delta_{t}), \\quad k = K,K-1,K-2,\\dots 1, $$SDE Recall the definition of SDE $$ X_{k+1} = X_{k} + \\Delta _{t} f(X_{k}, k\\Delta_{t}) + g(k \\Delta_{t})\\sqrt{ \\Delta_{t} } Z_{k}, \\quad k = 0,1,2,\\dots, $$ under $\\Delta_{t}\\to 0$ with $t = k\\Delta_{t}$ and $Z_{k}\\sim \\mathcal{N}(0,I)$ i.i.d. With initial value $X_{0}$, we can simulate $X_{T}$ by this process.\nBut how to simulate the reverse-time SDE? Is it $$ X_{k-1} = X_{k} - \\Delta _{t} f(X_{k}, k\\Delta_{t}) - g(k \\Delta_{t})\\sqrt{ \\Delta_{t} } Z_{k}, \\quad k = K,K-1,K-2,\\dots 1 ? $$ Unfortunately, this does not work! Anderson\u0026rsquo;s theorem The main theorem of Anderson, 1982 states that given a forwrad-time SDE $$ d X_{t} = f(X_{t}, t) dt + g(X_{t}, t) d W_{t}, \\quad X_{0}\\sim p_{0}, $$ the corresponding reverse-time SDE is $$ d \\bar{X}_{t} = \\left(f(\\bar{X}_{t}, t) - g^2(\\bar{X}_{t}, t)\\nabla_{x}\\log p_{t}(\\bar{X}_{t})\\right) dt + g(\\bar{X}_{t},t) d\\bar{W}_{t}, \\quad \\bar{X}_{T}\\sim p_{T}, $$ where $\\bar{W}_{t}$ is the reverse-time Brownian motion, $dt$ is an infinitesimal negative timestep, and $p_{T}$ is the pdf of $\\bar{X}_{T}$ defined by the forward-time SDE.\nAlternatively, define $\\{Y_{t} = \\bar{X}_{T-t}\\}_{t=0}^T$ via $$ dY_{t} = -\\left(f(Y_{t}, T-t) - g^2(Y_{t}, T- t)\\nabla_{y}\\log p_{T-t}(Y_{t})\\right) dt + g(Y_{t}, T-t) dW_{t}, \\quad Y_{0}\\sim p_{T}. $$ Then we have $X_{t} \\overset{\\mathcal{D}}{=} \\bar{X}_{t} = Y_{T-t}$ for all $t\\in[0,T]$. See proof in Anderson\u0026rsquo;s proof.\nThen we simulate the reverse-time SDE by $$ \\bar{X}_{k-1} = \\bar{X}_{k} - \\left(f(\\bar{X}_{k}, k\\Delta_{t}) - g^2(\\bar{X}_{k}, k\\Delta_{t})\\nabla_{x}\\log p_{k\\Delta_{t}}(\\bar{X}_{k})\\right) \\Delta_{t} + g({\\bar{X}_{k}}, k\\Delta_{t}) \\sqrt{ \\Delta_{t} }Z_{k}, \\quad k=K,K-1,\\dots,1, $$ where $Z_{i}\\sim \\mathcal{N}(0,I)$ i.i.d.\nHow to sample from revise-time process? Reverse-time SDE Given a SDE with forward-time SDE $$ d X_{t} = f dt + g dW_{t},\\quad X_{0}\\sim p_{0}, $$ Anderson\u0026rsquo;s theorem allows us to sample $X_{0}$ from $X_{T}$ by the reverse-time SDE $$ d \\bar{X}_{t} = (f - g^2\\nabla_{x}\\log p_{t}) dt + g d \\bar{W}_{t}, \\quad \\bar{X}_{T}\\sim p_{T}. $$ More precisely, using the standard discretization (Euler-Maruyama method), we get $$ \\bar{X}_{k-1} = \\bar{X}_{k} - \\left(f - g^2{\\color{red}\\nabla_{x}\\log p_{k\\Delta_{t}}(\\bar{X}_{k})}\\right) \\Delta_{t} + g \\sqrt{ \\Delta_{t} }Z_{k}, \\quad k=K,K-1,\\dots,1. $$ However, we do not have access to the score function $\\color{red}\\nabla_{x} \\log p_{t}$.\nReverse-time ODE It can also be shown that the process $\\{ X_{t} \\}_{t=0}^T$ defined by the following reverse-time ODE\n$$ d\\bar{X}_{t} = \\left( f - \\frac{g^2}{2} \\nabla_{x}\\log p_{t} \\right) dt,\\quad \\bar{X}_{T}\\sim p_{T}, $$ possesses the same marginal density functions $\\{ p_{t} \\}_{t=0}^T$ as the original forward-time SDE.\nLet $q_{t}$ be the density of $\\bar{X}_{t}$. By the FP equation, we have $$ \\begin{aligned} \\partial_{t} q_{t} \u0026= -\\partial_{x} \\left( \\left( f - \\frac{g^2}{2} \\nabla_{x} \\log p_{t} \\right)q_{t} \\right) \\\\ \u0026= -\\partial_{x}\\left( fq_{t} - \\frac{g^2}{2} \\frac{\\partial_{x}p_{t}}{p_{t}}q_{t} \\right) \\\\ \u0026= -\\partial_{x}(f q_{t}) + \\frac{g^2}{2} \\frac{q_{t}}{p_{t}} \\partial_{x}^2 p_{t}. \\end{aligned} $$ When $q_{t} = p_{t}$ we get exactly the same FP equation as the forward-time SDE. In other words, we have verified that $q_{t} = p_{t}$ solves the FP equation for $q_{t}$. Note: This is not a strict proof, as the uniqueness of the solution to the PDE is omitted.\nHowever, this method is still not yet implementable since we do not have access to the score function $\\color{red}\\nabla_{x} \\log p_{t}$.\nHow to train the model? As mentioned above, to sample from the reverse-time SDE, we need to evaluate the score function $\\color{red}\\nabla_{x} \\log p_{t}$. To solve this issue, we can use the model $s_{\\theta}(X_{t}, t)$ to approximate the score function $\\nabla_{x} p_{t}(X_{t})$.\nA natural choice for the loss function is $$ \\mathcal{L}(\\theta) = \\int_{0}^T \\lambda_{t} \\mathbb{E}_{x_{t}\\sim p_{t}} [\\|s_{\\theta}(x_{t}, t)- \\nabla_{x}\\log p_{t}(x_{t})\\|^2], $$ where $\\lambda_{t}\u003e0$ is a weighting factor. However, we cannot use it since $p_{t}$ is inaccessible. We introduce two methods to estimate $\\nabla_{x} \\log p_{t}(x_{t})$.\nDenosing score matching (DSM) If the drift $f$ and diffusion $g$ are nice, $p_{t|0}(x_{t}|x_{0})$ is known, so DSM use the loss function $$ \\mathcal{L}_{DSM}(\\theta) = \\int_{0}^T \\lambda_{t} \\mathbb{E}_{x_{0}\\sim p_{0}}\\left[\\mathbb{E}_{{x_{t}\\sim p_{t|0}}}[\\|s_{\\theta}(x_{t}, t) - \\nabla_{x}\\log p_{t|0}(x_{t}|x_{0})\\|^2|x_{0} ]\\right]dt. $$ We shall show that $\\mathcal{L}_{DSM}$ is equivalent to $\\mathcal{L}$.\nFirst, we show that $\\nabla_{x}\\log p_{t}(x_{t}) = \\mathbb{E}_{x_{0}\\sim p_{0|t}}[\\nabla_{x}\\log p_{t|0}(x_{t}|x_{0})|x_{0}]$. $$ \\begin{aligned} \\nabla_{x}\\log p_{t}(x_{t}) \u0026= \\frac{\\nabla_{x}p_{t}(x_{t})}{p_{t}(x_{t})} \\\\ \u0026=\\frac{1}{p_{t(x_{t})}} \\nabla_{x}\\int p_{t|0}(x_{t}|x_{0}) p_{0}(x_{0}) dx_{0} \\\\ \u0026=\\int [\\nabla_{x}p_{t|0}(x_{t}|x_{0})] \\frac{p_{0}(x_{0})}{p_{t(x_{t})}} dx_{0} \\\\ \u0026= \\int [\\nabla_{x} \\log p_{t|0}(x_{t}|x_{0})] \\frac{p_{0}(x_{0}) p_{t|0}(x_{t}|x_{0})}{p_{t(x_{t})}} dx_{0} \\\\ \u0026= \\int [\\nabla_{x} \\log p_{t|0}(x_{t}|x_{0})] p_{0|t}(x_{0}|x_{t}) dx_{0} \\\\ \u0026= \\mathbb{E}_{{x_{0}\\sim p_{0|t}}} [\\nabla_{x} \\log p_{t|0}(x_{t}|x_{0})|x_{t}]. \\end{aligned} $$ Then we can show the result $$ \\begin{aligned} \\mathcal{L}(\\theta) \u0026=\\int_{0}^T \\lambda_{t} \\mathbb{E}_{x_{t}\\sim p_{t}} [\\|s_{\\theta}(x_{t}, t)- \\nabla_{x}\\log p_{t}(x_{t})\\|^2] dt\\\\ \u0026= \\int_{0}^T \\lambda_{t} \\mathbb{E}_{x_{t}\\sim p_{t}} [\\|s_{\\theta}(x_{t}, t)\\|^2-2 \\langle s_{\\theta}(x_{t},t), \\nabla_{x}\\log p_{t}(x_{t}) \\rangle ] dt + C \\\\ \u0026= \\int_{0}^T \\lambda_{t} \\mathbb{E}_{x_{t}\\sim p_{t}} [\\|s_{\\theta}(x_{t}, t)\\|^2-2 \\langle s_{\\theta}(x_{t},t), \\mathbb{E}_{{x_{0}\\sim p_{0|t}}} [\\nabla_{x} \\log p_{t|0}(x_{t}|x_{0})|x_{t}] \\rangle ] dt + C \\\\ \u0026= \\int_{0}^T \\lambda_{t} \\mathbb{E}_{x_{t}\\sim p_{t}}\\mathbb{E}_{{x_{0}\\sim p_{0|t}}} [\\|s_{\\theta}(x_{t}, t)\\|^2-2 \\langle s_{\\theta}(x_{t},t), \\nabla_{x} \\log p_{t|0}(x_{t}|x_{0}) \\rangle | x_{t}] dt + C \\\\ \u0026= \\int_{0}^T \\lambda_{t} \\mathbb{E}_{(x_{0},x_{t})\\sim p_{0,t}} [\\|s_{\\theta}(x_{t}, t)\\|^2-2 \\langle s_{\\theta}(x_{t},t), \\nabla_{x} \\log p_{t|0}(x_{t}|x_{0}) \\rangle] dt + C \\\\ \u0026= \\mathcal{L}_{DSM}(\\theta). \\end{aligned} $$Sliced score matching (SSM) What if the conditional distribution $p_{t|0}(x_{t|x_{0}})$ is unknown? The loss of SSM is $$ \\mathcal{L}_{SSM}(\\theta) = \\int_{0}^T \\lambda_{t}\\mathbb{E}_{x_{t}\\sim p_{t}} \\left[ \\|s_{\\theta}(x_{t}, t)\\|^2 + 2\\mathbb{E}_{v}\\left[ \\frac{d}{d h} v^T s_{\\theta}(x_{t}+h v) |_{h=0} \\right] \\right] dt + C, $$ where $v\\in \\mathbb{R}^n$ is a random vector s.t. $$ \\mathbb{E}_{v}[v_{i} v_{j}] = \\delta_{i,j} = 1_{i=j}. $$Before proving that $\\mathcal{L}_{SSM}$ is equivalent to $\\mathcal{L}$, we introduce the Hutchinson\u0026rsquo;s trace estimator that will be useful in later proof. Let $A\\in \\mathbb{R}^{n\\times n}$. Then $$ \\begin{aligned} \\mathbb{E}_{v}[v^T A v] \u0026= \\mathbb{E}_{v}[\\mathrm{Tr}(v^T A v)] \\\\ \u0026= \\mathbb{E}_{v}[\\mathrm{Tr}(A v v^T)] \\\\ \u0026= \\mathrm{Tr}(\\mathbb{E}_{v}[ A vv^T]) \\\\ \u0026= \\mathrm{Tr}(A\\mathbb{E}_{v}[ vv^T]) \\\\ \u0026= \\mathrm{A}. \\end{aligned} $$In the proof of DSM, we see that the expansion of $\\mathcal{L}(\\theta)$ contains term $\\langle s_{\\theta}(x_{t},t), \\nabla_{x}\\log p_{t}(x_{t}) \\rangle$. where the divergence operator takes a vector field and produces a scalar value which is defined by $$ \\nabla \\cdot F = \\frac{\\partial F_{x}}{\\partial x} + \\frac{\\partial F_{y}}{\\partial y} + \\frac{\\partial F_{z}}{\\partial z}. \\quad F = (F_{x}, F_{y}, F_{z}). $$ And $D_{x} F(x)$ should be the partial differentiation matrix $\\left[ \\frac{\\partial}{\\partial x_{1}} F, \\frac{\\partial}{\\partial x_{2}} F, \\dots, \\frac{\\partial}{\\partial x_{n}} F \\right]$ (I guess).\nOverall,\nSSM is more broadly applicable than DSM as it does not require the conditional score $\\nabla_{x} p_{t|0}(x_{t}|x_{0})$. SSM requires mixed (2nd-order) derivatives, one for $h$ and another for $\\theta$. Examples with O-U process Recall the definition of Ornstein-Uhlenbeck process $$ dX_{t} = -\\beta X_{t} dt + \\sigma dW_{t} $$ With conditional distribution $X_{t}|X_{0} \\sim \\mathcal{N}(e^{-\\beta t} X_{0}, \\frac{\\sigma^2}{2\\beta} (1- e^{-2\\beta t})I)$. In this section, we will discuss two types of O-U processes: variance-exploding (VE) SDE and variance-preserving (VP) SDE as well as their training losses and sampling methods. VE SDE Let $\\sigma_{t}$ be a non-decreasing function of $t$, the VE SDE is defined by $$ dX_{t} = \\sqrt{ \\frac{d(\\sigma_{t}^2)}{dt} } dW_{t}. $$ It can be easily checked that $X_{t}|X_{0} \\sim \\mathcal{N}(X_{0}, \\sigma_{t}^2)$, which means that variance explodes.\nVP SDE $$ d X_{t} = - \\frac{\\beta_{t}}{2} X_{t} dt + \\sqrt{ \\beta_{t} } dW_{t}. $$ By (5.50) and (5.51) of Applied Stochastic Differential Equation we can get the variance of $\\{X_{t}\\}_{t}$ is governed by the ODE $$ \\frac{d\\Sigma_{t}}{dt} = \\beta_{t}(I + \\Sigma_{t}). $$ Solving this ODE, we obtain $$ \\Sigma_{t} = I + \\exp\\left( -\\int_{0}^t \\beta_{s} ds \\right)(\\Sigma_{0}- I), $$ from which it is clear that the variance of VP SDE is bounded by $\\Sigma_{0}$ from above. Moreover, if $\\Sigma_{0} = I$, we have $\\Sigma_{t} = I$, so its name is called variance-preserving. And the conditional mean of $X_{t}|X_{0}$ is $\\exp\\left( -\\frac{1}{2}\\int_{0}^t\\beta_{s} ds\\right) X_{0}$ and conditional variance is $I- \\exp\\left( -\\int_{0}^t \\beta_{s} ds\\right)I$\nTraining with O-U Generally, for both VE and VP SDE, we write its conditional distribution $X_{t}|X_{0}$ in the form of $$ X_{t}|X_{0} \\sim \\mathcal{N}(\\gamma_{t}X_{0}, \\sigma_{t}^2 I). $$with $$ \\text{VE SDE:} \\begin{cases} \\gamma_{t} = 1 \\\\ \\sigma_{t}^2 = \\sigma_{t}^2 \\end{cases} ,\\quad \\text{VP SDE:} \\begin{cases} \\gamma_{t} = \\exp\\left( -\\frac{1}{2}\\int_{0}^t\\beta_{s} ds\\right) \\\\ \\sigma_{t}^2 = 1- \\exp\\left( -\\int_{0}^t \\beta_{s} ds\\right) \\end{cases} $$ So we can write $X_{t}$ as $X_{t} = \\gamma_{t} X_{0} + \\sigma_{t} \\epsilon$, where $\\epsilon$ is a standard Gaussian r.v.,thus the score function simplifies to $$ \\nabla_{x} \\log p_{t|0}(x_{t}) = - \\frac{X_{t}- \\gamma_{t}X_{0}}{\\sigma^2} = - \\frac{\\epsilon}{\\sigma_{t}}. $$ Define a scaled score network $$ \\epsilon_{\\theta}(x_{t}, t) = -\\sigma_{t}s_{\\theta}(x_{t},t), $$ then the DSM loss becomes $$ \\begin{aligned} \\mathcal{L}_{DSM}(\\theta) \u0026= \\int_{0}^T \\lambda_{t} \\mathbb{E}_{x_{0}\\sim p_{0}}\\left[\\mathbb{E}_{{x_{t}\\sim p_{t|0}}}[\\|s_{\\theta}(x_{t}, t) - \\nabla_{x}\\log p_{t|0}(x_{t}|x_{0})\\|^2|x_{0} ]\\right]dt \\\\ \u0026= \\int_{0}^T \\frac{\\lambda_{t}}{\\sigma_{t}^2} \\mathbb{E}_{x_{0}\\sim p_{0}}\\left[\\mathbb{E}_{{\\epsilon \\sim \\mathcal{N}(0,1)}}[\\|\\epsilon_{\\theta}(\\gamma_{t}x_{0} + \\sigma_{t}\\epsilon, t) - \\epsilon\\|^2|x_{0} ]\\right]dt \\\\ \u0026= T \\mathbb{E}_{x_{0}\\sim p_{0}, t\\sim U[0, T], \\epsilon\\sim \\mathcal{N}(0,1)} \\left[ \\frac{\\lambda_{t}}{\\sigma_{t}^2} \\|\\epsilon_{\\theta}(\\gamma_{t}x_{0} + \\sigma_{t}\\epsilon, t) - \\epsilon\\|^2 \\right]. \\end{aligned} $$ Note that when $t=0$, $\\sigma_{t} = 0$ and the loss blows up. There several ways to deal with it\nDo the integral on the interval $[\\delta, T]$ with a small $\\delta\u003e0$. Choose appropriate $\\lambda_{t}$ s.t. $\\frac{\\lambda_{t}}{\\sigma_{t}^2} \u003c C$ for all $t$. Use importance sampling to reduce the vriance. Training O-U with DSM loss Training O-U with SSM loss Sampling with O-U SDE sampling Once $s_{\\theta}$ has been trained, we can generate new samples with the approximated reverse-time SDE $$ d \\bar{X}_{t} = \\left( f - g^2 s_{\\theta} \\right) dt + g dW_{t}, \\quad \\bar{X}_{T}\\sim\\mathcal{N}(0, \\sigma_{T}^2 I). $$ Sampling VE SDE with reverse SDE Start from $X_{N}\\sim \\mathcal{N}(0,\\sigma_{T}^2I)$ For $i = N-1$ to $0$ do $X_{i} = X_{i+1} + (\\sigma_{i+1}^2 - \\sigma_{i}^2) s_{\\theta}(x_{i+1}, t_{i+1}) + \\sqrt{ \\sigma_{i+1}^2 - \\sigma_{i}^2 }Z$. Return $X_{0}$.\nSampling VP SDE with reverse SDE Start from $X_{N}\\sim \\mathcal{N}(0,\\sigma_{T}^2I)$ For $i = N-1$ to $0$ do $X_{i} = X_{i+1} + \\frac{1}{2}(\\beta_{i+1} - \\beta_{i}) + (\\beta_{i+1} - \\beta_{i}) s_{\\theta}(x_{i+1}, t_{i+1}) + (\\sqrt{ \\beta_{i+1}} - \\sqrt{ \\beta_{i} })Z$. Return $X_{0}$.\nODE sampling We can also use the approximated reverse-time ODE for sampling $$ d\\bar{X}_{t} = \\left( f - \\frac{g^2}{2} s_{\\theta} \\right)dt, \\quad \\bar{X}_{T}\\sim\\mathcal{N}(0, \\sigma_{T}^2 I). $$ Sampling VE SDE with reverse ODE Start from $X_{N}\\sim \\mathcal{N}(0,\\sigma_{T}^2I)$ For $i = N-1$ to $0$ do $X_{i} = X_{i+1} + \\frac{\\sigma_{i+1}^2 - \\sigma_{i}^2}{2} s_{\\theta}(x_{i+1}, t_{i+1})$. Return $X_{0}$.\nSampling VP SDE with reverse ODE Start from $X_{N}\\sim \\mathcal{N}(0,\\sigma_{T}^2I)$ For $i = N-1$ to $0$ do $X_{i} = X_{i+1} + \\frac{1}{2}(\\beta_{i+1} - \\beta_{i}) + \\frac{\\beta_{i+1} - \\beta_{i}}{2} s_{\\theta}(x_{i+1}, t_{i+1})$. Return $X_{0}$.\nSupplementary Integration by parts Assume $\\psi$ and $f$ are sufficiently smooth and decay sufficiently quickly as $|x|\\to \\infty$. Then $$ \\int_{\\mathbb{R}} \\psi(x)f^\\prime(x) dx = -\\int_{\\mathbb{R}} \\psi(x) f(x) dx. $$ For higher-dimensional case $\\psi: \\mathbb{R}^d \\to \\mathbb{R}^d, f:\\mathbb{R}^d \\to \\mathbb{R}$, we have $$ \\int_{\\mathbb{R}^d} \\psi(x) \\cdot \\nabla f(x) dx = - \\int_{\\mathbb{R}^d} (\\nabla \\cdot \\psi(x)) f(x) dx. $$Proof of Fokker-Planck equation We prove FP equation for $d=1$. Let $\\psi$ be a continuous function, we have $$ \\begin{aligned} \\partial_{t} \\mathbb{E}_{x\\sim p_{t}}[\\psi(x)] \u0026\\approx \\frac{1}{\\epsilon}(\\mathbb{E}_{x\\sim p_{t+\\epsilon}}[\\psi(x)] - \\mathbb{E}_{x\\sim p_{t}}[\\psi(x)])\\\\ \u0026\\approx \\frac{1}{\\epsilon} \\mathbb{E}_{x\\sim p_{t}, z\\sim\\mathcal{N}(0,I)}[\\psi(x+\\epsilon f + \\sqrt{ \\epsilon }) - \\psi(x)] \\\\ \u0026\\approx \\frac{1}{\\epsilon} \\mathbb{E}_{x\\sim p_{t}, z\\sim\\mathcal{N}(0,I)}\\left[ \\psi(x) + \\epsilon \\psi^\\prime(x)f + \\sqrt{ \\epsilon }\\psi^\\prime(x) g z + \\frac{1}{2} \\epsilon\\psi^{\\prime\\prime}(x)g^2z^2 + \\mathcal{O}(\\epsilon^{3 / 2}) - \\psi(x) \\right] ;\\quad\\text{taylor expansion}\\\\ \u0026\\approx \\mathbb{E}_{x\\sim p_{t}}\\left[ \\psi^\\prime(x)f + \\frac{1}{2}\\psi^{\\prime\\prime}(x)g^2 \\right] \\\\ \\end{aligned} $$ It follows that $$ \\begin{align} \\partial_{t} \\int \\psi(x) p_{t}(x) dx \u0026= \\int \\psi^\\prime(x)f p_{t}(x)dx + \\frac{1}{2} \\int \\psi^{\\prime\\prime}(x)g^2(t) p_{t}(x) dx \\\\ \\int \\psi(x) \\partial_{t}p_{t}(x) dx \u0026= \\int \\psi(x)(-\\partial_{x}(fp_{t})) dx + \\frac{1}{2}\\int \\psi(x) g^2 \\partial_{x}^2(p_{t}) dx ;\\quad \\text{integration by parts} \\end{align} $$ Therefore, $$ \\partial_{t}p_{t} = -\\partial_{x} fp_{t} + \\frac{g^2}{2}\\partial_{x}^2 (p_{t}). $$Proof Anderson\u0026rsquo;s theorem For the forward-time SDE, we have the FP equation $$ \\partial_{t}p_{t} = -\\partial_{x}(fp_{t}) + \\frac{g^2}{2}\\partial_{x}^2 p_{t}. $$Let $\\{q_{t}\\}_{t=0}^T$ be the marginal densities of the SDE $$ dY_{t} = -(f(Y_{t}, T-t) -g^2(T-t)\\partial_{y}\\log p_{T-t}(Y_{t}) )dt + g(T-t)dW_{t},\\quad Y_{0} \\sim p_{T}. $$ Then $\\{ q_{t} \\}$ satisfies the FP equation $$ \\partial_{t}q_{t} = \\partial_{y}\\left((f(y,T-t)-g^2(T-t)\\partial_{y}\\log p_{T-t}(y)) q_{t}(y)\\right) + \\frac{g^2(T-t)}{2}\\partial_{y}^2(q_{t}(y)). $$ Let $\\{ \\bar{p}_{t} \\}$ be the marginal densities of the reverse-time SDE given by Anderson\u0026rsquo;s theorem $$ d \\bar{X}_{t} = \\left(f(\\bar{X}_{t}, t) - g^2(\\bar{X}_{t}, t)\\nabla_{x}\\log p_{t}(\\bar{X}_{t})\\right) dt + g(\\bar{X}_{t},t) d\\bar{W}_{t}, \\quad \\bar{X}_{T}\\sim p_{T}, $$ Since $\\bar{p}_{t} = q_{T-t}$, the densities $\\{ \\bar{p}_{t} \\}$ satisfies $$ \\partial_{t}\\bar{p}_{t} = -\\partial_{x}\\left((f(x,t)-g^2(t)\\partial_{x}\\log p_{t}(x)) \\bar{p}_{t}(y)\\right) - \\frac{g^2(t)}{2}\\partial_{x}^2(\\bar{p}_{t}(x)). $$ If we substitute $\\bar{p}_{t}$ with $p_{t}$ in the last FP equation, we get $$ \\partial_{t}p_{t} = -\\partial_{x}\\left((f(x,t)-g^2(t)\\partial_{x}\\log p_{t}(x)) p_{t}(y)\\right) - \\frac{g^2(t)}{2}\\partial_{x}^2(p_{t}(x)) = -\\partial_{x}(fp_{t}) + \\frac{g^2}{2}\\partial_{x}^2(p_{t}). $$ In other words, we have verified that $\\bar{p}_{t}= p_{t}$ solves the FP equation for $\\bar{p}_{t}$, which proves $\\bar{p}_{t}= p_{t}$ provided that the solution to the PDE is unique.\n","permalink":"http://localhost:1313/posts/diffusion_2_preliminary_sde/","summary":"\u003ch2 id=\"basics\"\u003eBasics\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eODE definition\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eConsider the ordinary differential equation (ODE)\n\u003c/p\u003e\n$$\n\\frac{dX}{dt}(t) = f(X(t), t),\n$$\u003cp\u003e\nwhich we also express as $dX_{t} = f(X_{t}, t)dt$, where $X(t), f(X(t),t)\\in  \\mathbb{R}^d$. Then, $\\{X(t)\\}_{t}$ is a deterministic curve.\u003c/p\u003e\n\u003cp\u003eWe can define the ODE by the limit\n\u003c/p\u003e\n$$\nX_{k+1} = X_{k} + \\Delta _{t} f(X_{k}, k\\Delta_{t}), \\quad k = 0,1,2,\\dots,\n$$\u003cp\u003e\nunder $\\Delta_{t}\\to 0$ with $t = k\\Delta_{t}$. Precisely, $\\left\\{ X_{\\left\\lfloor  \\frac{t}{\\Delta_{t}}  \\right\\rfloor} \\right\\}_{t} \\to \\{X_{t}\\}_{t}$ uniformly on compact intervals.\u003c/p\u003e","title":"DL | Diffusion Models 2 - Preliminary ODE and SDE"},{"content":"What? A diffusion probabilistic model is a parameterized Markov chain that gradually adds noise to the data and then learn to reverse the diffusion process to generate data samples from noise.\nWhy? Compared with other AI tasks, image generation is harder, since it does not have a standard answer. To solve this issue, GAN and VAE are propsed.\nGAN uses another model (discriminator) to decide the quality of generated images.\nVAE learns how to compress an image into a latent vector $z$ and then learns how to reconstruct the image from $z$. Then for each generated image, there exists a standard answer.GAN is able to generate images with good quality, but its training is unstable. Is there a model that is as powerful as GAN but simpler for training?\nHow? Forward process Given a data point sampled from a real data distribution $x_0\\sim q(x)$, the forward process is defined by\n$$ q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I),\\quad q(x_{1:T}|x_0) = \\prod_{t=1}^T q(x_t|x_{t-1}). $$A nice property of the forward process is that it admits sampling $x_t$ at an arbitrary timestep $t$ in closed form:\n$$ \\begin{equation} q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha}_{t})I), \\end{equation} $$where $\\alpha_t := 1-\\beta_t$ and $\\bar{\\alpha}_t:= \\prod_{s=0}^t \\alpha_s$. To make sure that $q(x_t|x_0)$ is convergent, we need $\\bar{\\alpha}_t \\to C$ as $t\\to\\infty$. It can be guaranteed by setting $\\beta_t\\in (0,1)$ as an increasing sequence and therefore $\\bar{\\alpha}_1\u003e\\cdots\u003e\\bar{\\alpha}_T$.\nReverse process If we can reverse the above process and sample from $q(x_{t-1}|x_t)$, we will be able to recreate the true sample from a Gaussian noise input $x_T\\sim N(0, I)$. Note that if $\\beta_t$ is small enough, $q(x_{t-1}|x_t)$ will also be Gaussian (==We will prove it in Diffusion-model-3==). However, we cannot easily estimate $q(x_{x-1}|x_t)$ because it needs to use the entire dataset. So we try to estimate it through a model $p_\\theta(x_{t-1}|x_t)$. We set\n$$ p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t));\\quad p_\\theta(x_{0:T}) = p(x_T)\\prod_{t=1}^T p_\\theta(x_{t-1}|x_t). $$But how to define the loss function? Fitting the mean and variance of added noise from the step $q(x_{t-1}|x_t)$ is unrealistic.\nConditional posterior\nIt is noteworthy that the reverse conditional probability is tractable when conditioned on $x_0$:\n$$ q(x_{t-1}|x_t, x_0) = \\mathcal{N}(x_{t-1}; \\tilde{\\mu}_t(x_t, x_0), \\tilde{\\beta}_t I), $$where\n$$ \\begin{equation} \\tilde{\\mu}_t(x_t, x_0) := \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}x_t +\\frac{\\bar{\\alpha}_{t-1}\\beta_t}{1-\\bar{\\alpha}_t}x_0;\\quad \\tilde{\\beta}_t:= \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}\\beta_t. \\end{equation} $$See Proof of conditional posterior.\nLoss function\nThen we can use the variational lower bound to optimize the negative log-likehood.\n$$ \\begin{aligned} -\\log p_{\\theta}(x_{0}) \u0026\\leq -\\log p_{\\theta}(x_{0}) + D_{KL}(q(x_{1:T}|x_{0})\\|p_{\\theta}(x_{1:T}|x_{0})) \\\\ \u0026= -\\log p_{\\theta}(x_{0}) + \\mathbb{E}_{q}\\left[ \\log \\frac{q(x_{1:T}|x_{0})}{p_{\\theta}(x_{0:T})/p_{\\theta}(x_{0})} \\right]\\\\ \u0026=-\\log p_{\\theta}(x_{0}) + \\mathbb{E}_{q}\\left[ \\log \\frac{q(x_{1:T}|x_{0})}{p_{\\theta}(x_{0:T})} +\\log p_{\\theta}(x_{0}) \\right]\\\\ \u0026=\\mathbb{E}_{q}\\left[ \\log \\frac{q(x_{1:T}|x_{0})}{p_{\\theta}(x_{0:T})} \\right] := L_{VLB} \\end{aligned} $$Then we rewrite $L_{VLB}$ in terms of $q(x_{t-1}|x_{t},x_{0})$:\n$$ \\begin{align} L_{VLB} \u0026= L_{T} + L_{T-1} + \\cdots + L_{0}, \\\\ \\text{where} \\; L_{T} \u0026 = D_{KL}(q(x_{T}|x_{0})\\|p_{\\theta}(x_{T})), \\\\ L_{t} \u0026 = D_{KL}(q(x_{t}|x_{t+1},x_{0})\\| p_{\\theta}(x_{t}|x_{t+1})),\\; 1\\leq t\\leq T-1, \\\\ L_{0} \u0026 = -\\log p_{\\theta}(x_{0}|x_{1}). \\end{align} $$See Proof of loss. $L_{T}$ is constant because $x_{T}$ is a Gaussian noise. $L_{0}$ is modeled using a separate decoder. $L_{t}$ compares two Gaussian distributions and therefore can be computed in closed form.\nSimplification of $L_{t}$\nIt is known that the KL divergence between two Gaussian distributions $\\mathcal{N}(\\mu_{p}, \\Sigma_{p})$ and $\\mathcal{N}(\\mu_{q},\\Sigma_{q})$ is\n$$ D_{KL}(p\\|q) = \\frac{1}{2} \\left[ \\log \\frac{|\\Sigma_{q}|}{|\\Sigma_{p}|} + (\\mu_{p} -\\mu_{q})^T \\Sigma_{q}^{-1}(\\mu_{q} - \\mu_{p}) + Tr(\\Sigma_{q}^{-1}\\Sigma_{p}) \\right] + C. $$Since the authors set $\\Sigma_{\\theta}(x_{t}, t) = \\sigma_{t}^2 I$, where $\\sigma_{t}$ is not learnable. We can simplify $L_{t}$ as\n$$ \\begin{equation} L_{t} = \\frac{1}{2\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\|\\tilde{\\mu}(x_{t},x_{0}) - \\mu_{\\theta}(x_{t},t)\\|^2 + C. \\end{equation} $$We can expand the above loss by reparameterizing $q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha})I)$ as $x_{t}(x_{0}, \\epsilon) = \\sqrt{ \\bar{\\alpha}_{t} }x_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t} }\\epsilon$ for $\\epsilon\\sim\\mathcal{N}(0, I)$:\n$$ \\begin{aligned} \\tilde{\\mu}_{t} \u0026= \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}x_t +\\frac{\\bar{\\alpha}_{t-1}\\beta_t}{1-\\bar{\\alpha}_t}{\\color{green}x_0} \\\\ \u0026=\\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}x_t +\\frac{\\bar{\\alpha}_{t-1}\\beta_t}{1-\\bar{\\alpha}_t} {\\color{green} \\frac{1}{\\sqrt{ \\bar{\\alpha}_{t} }}(x_{t} - \\sqrt{ 1-\\bar{\\alpha}_{t} }\\epsilon)} \\\\ \u0026=\\frac{1}{\\sqrt{ \\alpha_{t} }}\\left( x_{t} - \\frac{1-\\alpha_{t}}{\\sqrt{ 1-\\bar{\\alpha}_{t} }} \\epsilon \\right) \\end{aligned} $$Instead of approximating $\\tilde{\\mu}_{t}$ by $\\mu_{\\theta}$ directly, we can also estimate the noise $\\epsilon$ using $\\epsilon_{\\theta}(x_{t}, t)$ (like ResNet) by setting\n$$ \\begin{align} \\mu_{\\theta}(x_{t}, t) = \\tilde{\\mu}_{t}(x_{t}, t) \u0026= \\frac{1}{\\sqrt{ \\alpha_{t} }}\\left( x_{t} - \\frac{1-\\alpha_{t}}{\\sqrt{ 1-\\bar{\\alpha}_{t} }} \\epsilon_{\\theta}(x_{t}, t) \\right).\\\\ \\end{align} $$Therefore, we can rewrite the loss function $L_{t}$ as\n$$ \\begin{align} L_{t} \u0026= \\mathbb{E}_{x_{0}, \\epsilon} \\left[\\frac{(1-\\alpha_{t})^2}{2 \\alpha_{t}(1-\\bar{\\alpha}_{t})\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\| \\epsilon - \\epsilon_{\\theta}(x_{t},t)\\|^2 \\right] + C \\\\ \u0026 = \\mathbb{E}_{x_{0}, \\epsilon} \\left[\\frac{(1-\\alpha_{t})^2}{2 \\alpha_{t}(1-\\bar{\\alpha}_{t})\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\| \\epsilon - \\epsilon_{\\theta}({\\color{green}\\sqrt{ \\bar{\\alpha}_{t} }x_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t} }\\epsilon},t)\\|^2 \\right] + C \\end{align} $$Algorithm Empirically, the authors found that training the diffusion model works better with a simplified objective that ignores the weighting term:\n$$ L_{t}^{simple} = \\mathbb{E}_{x_{0}, \\epsilon} \\left[\\| \\epsilon - \\epsilon_{\\theta}(\\sqrt{ \\bar{\\alpha}_{t} }x_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t} }\\epsilon,t)\\|^2 \\right]. $$\nSupplementary Langevin dynamics Langevin dynamics is a concept from physics, developed for statistically modeling molecular systems. Combined with stochastic gradient descent, stochastic gradient Langevin dynamics can produce samples from a probability distribution $p(x)$ using only gradients $\\nabla_{x} \\log p(x)$ in a Markov chain of updates: $$ x_{t} = x_{t-1} + \\frac{\\delta}{2}\\nabla_{x} \\log p(x_{t-1}) + \\sqrt{ \\delta }\\epsilon_{t},\\quad \\epsilon_{t}\\sim \\mathcal{N}(0,I), $$ where $\\delta$ is the step size. When $T\\to \\infty$, $\\epsilon\\to 0$, $x_{T}\\sim p(x)$.\nWe can see that we only need to know the gradients $\\nabla_{x} \\log p(x)$ (score function) to sample data from $p(x)$.\nLet $(x, \\tilde{x})$ be a pair of clean and corrupted data. The idea of denoising score matching is to estimate the score function of the noise-corrupted data distribution $q_{\\sigma}(\\tilde{x})$, and the objective is shown to be equivalent to $$ \\mathbb{E}_{q_{\\sigma}(\\tilde{x}|x) p(x)} [\\|s(\\tilde{x}, \\theta) - \\nabla_{x}\\log q_{\\sigma}(\\tilde{x}|x)\\|_{2}^2]. $$ The schedule of increasing noise levels resembles the forward diffusion process $q(x_{t}|x_{0})$. If we use the diffusion process annotation, the score approximates $s_{\\theta}(x_{t}, t)\\approx \\nabla_{x_{t}}q(x_{t}|x_{0})$. Note that given a Gaussian distribution $x\\sim\\mathcal{N}(\\mu,\\sigma^2I)$, its score function is $$ \\nabla_{x} \\log p(x) = - \\frac{x-\\mu}{\\sigma^2} = - \\frac{\\epsilon}{\\sigma}, \\quad \\epsilon\\sim \\mathcal{N}(0,I). $$ Recall that $q(x_{t}|x_{0})\\sim \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha})I)$ and therefore, $$ s_{\\theta}(x_{t},t) \\approx \\nabla_{x_{t}}\\log q(x_{t}) = \\mathbb{E}_{q(x_{0})}[\\nabla_{x_{t}}q(x_{t}|x_{0})] = - \\frac{\\epsilon}{\\sqrt{ 1-\\bar{\\alpha}_{t} }}, $$ which is equivalent to the loss function $L_{t}$ in DDPM.\nProof of conditional posterior $p(x_{t-1}|x_t,x_0)$ Using Bayes\u0026rsquo; rule, we have $$ \\begin{aligned} q(x_{t-1}|x_{t}, x_{0}) \u0026= q(x_{t}|x_{t}, x_{0}) \\frac{q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})} \\\\ \u0026 \\propto \\exp\\left( -\\frac{1}{2}\\left( \\frac{(x_{t}- \\sqrt{ \\alpha_{t} }x_{t-1})^2}{\\beta_{t}} + \\frac{(x_{t-1}- \\sqrt{ \\bar{\\alpha}_{t-1} }x_{0})^2}{1-\\bar{\\alpha}_{t-1}} - \\frac{(x_{t}- \\sqrt{ \\bar{\\alpha}_{t} }x_{0})^2}{1-\\bar{\\alpha}_{t}} \\right) \\right) \\\\ \u0026 \\propto \\exp\\left( -\\frac{1}{2}\\left( \\frac{- 2\\sqrt{ \\alpha_{t}} x_{t}{\\color{blue}x_{t-1}} + \\alpha_{t} \\color{red}x_{t-1}^2}{\\beta_{t}} + \\frac{ {\\color{red}x_{t-1}^2}- 2\\sqrt{ \\bar{\\alpha}_{t-1} }x_{0}{\\color{blue}{x_{t-1}}} }{1-\\bar{\\alpha}_{t-1}} \\right) \\right) \\\\ \u0026 \\exp\\left( -\\frac{1}{2} \\left( {\\color{red} \\left( \\frac{\\alpha_{t}}{\\beta_{t}} + \\frac{1}{1-\\bar{\\alpha}_{t-1}} \\right)x_{t-1}^2} - {\\color{blue}\\left( \\frac{2\\sqrt{ \\alpha_{t} }}{\\beta_{t}} x_{t} + \\frac{2\\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_{t-1}} \\right) x_{t-1}} \\right) \\right) \\end{aligned} $$ By completing the square, we have $$ \\begin{aligned} \\tilde{\\beta}_{t} \u0026= \\frac{1}{\\left( \\frac{\\alpha_{t}}{\\beta_{t}} + \\frac{1}{1-\\bar{\\alpha}_{t-1}} \\right)} = \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_{t}}\\beta_{t}, \\\\ \\tilde{\\mu}_{t} \u0026= \\left( \\frac{2\\sqrt{ \\alpha_{t} }}{\\beta_{t}} x_{t} + \\frac{2\\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_{t-1}} \\right) / \\tilde{\\beta}_{t} = \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}x_t +\\frac{\\bar{\\alpha}_{t-1}\\beta_t}{1-\\bar{\\alpha}_t}x_0. \\end{aligned} $$Proof of Loss function $L_{VLB}$ $$ \\begin{aligned} L_{VLB} \u0026= \\mathbb{E}_{q}\\left[ \\log \\frac{q(x_{1:T}|x_{0})}{p_{\\theta}(x_{0:T})} \\right] \\\\ \u0026= \\mathbb{E}_{q}\\left[ \\log \\frac{\\prod_{t=1}^Tq(x_{t}|x_{t-1})}{p_{\\theta}(x_{T})\\prod_{t=1}^T p_{\\theta}(x_{t-1}|x_{t})} \\right] \\\\ \u0026= \\mathbb{E}_{q}\\left[ -\\log p_{\\theta}(x_{T}) + \\sum_{t=2}^T \\log \\frac{ q(x_{t}|x_{t-1})}{p_{\\theta}(x_{t-1}|x_{t})} + \\log \\frac{q(x_{1}|x_{0})}{p_{\\theta}(x_{0}|x_{1})} \\right] \\\\ \u0026= \\mathbb{E}_{q}\\left[ -\\log p_{\\theta}(x_{T}) + \\sum_{t=2}^T \\log \\frac{ q(x_{t}|x_{t-1}, {\\color{red}{x_{0}}})}{p_{\\theta}(x_{t-1}|x_{t})} + \\log \\frac{q(x_{1}|x_{0})}{p_{\\theta}(x_{0}|x_{1})} \\right] \\\\ \u0026= \\mathbb{E}_{q}\\left[ -\\log p_{\\theta}(x_{T}) + \\sum_{t=2}^T \\log \\frac{ q(x_{t-1}|x_{t}, x_{0})}{p_{\\theta}(x_{t-1}|x_{t})} \\cdot \\frac{q(x_{t}|x_{0})}{q(x_{t-1}|x_{0})} + \\log \\frac{q(x_{1}|x_{0})}{p_{\\theta}(x_{0}|x_{1})} \\right] \\\\ \u0026= \\mathbb{E}_{q}\\left[ -\\log p_{\\theta}(x_{T}) + \\sum_{t=2}^T \\log \\frac{ q(x_{t-1}|x_{t}, x_{0})}{p_{\\theta}(x_{t-1}|x_{t})} + \\sum_{t=2}^T \\log \\frac{q(x_{t}|x_{0})}{q(x_{t-1}|x_{0})} + \\log \\frac{q(x_{1}|x_{0})}{p_{\\theta}(x_{0}|x_{1})} \\right] \\\\ \u0026= \\mathbb{E}_{q}\\left[ -\\log p_{\\theta}(x_{T}) + \\sum_{t=2}^T \\log \\frac{ q(x_{t-1}|x_{t}, x_{0})}{p_{\\theta}(x_{t-1}|x_{t})} + \\log \\frac{q(x_{T}|x_{0})}{q(x_{1}|x_{0})} + \\log \\frac{q(x_{1}|x_{0})}{p_{\\theta}(x_{0}|x_{1})} \\right] \\\\ \u0026= \\mathbb{E}_{q}\\left[ \\log \\frac{q(x_{T}|x_{0})}{p_{\\theta}(x_{T}) } + \\sum_{t=2}^T \\log \\frac{ q(x_{t-1}|x_{t}, x_{0})}{p_{\\theta}(x_{t-1}|x_{t})} - \\log p_{\\theta}(x_{0}|x_{1}) \\right] \\\\ \u0026= \\underbrace{D_{KL}(q(x_{T}|x_{0})\\| p_{\\theta}(x_{T}))}_{L_{T}} + \\sum_{t=2}^T \\underbrace{D_{KL}(q(x_{t-1}|x_{t}, x_{0})\\| p_{\\theta}(X_{t-1}\\|x_{t}))}_{L_{t-1}} \\underbrace{-\\log p_{\\theta}(x_{0}|x_{1})}_{L_{0}} \\end{aligned} $$ ","permalink":"http://localhost:1313/posts/diffusion_1_ddmp/","summary":"\u003ch2 id=\"what\"\u003eWhat?\u003c/h2\u003e\n\u003cp\u003eA diffusion probabilistic model is a parameterized Markov chain that gradually adds noise to the data and then learn to reverse the diffusion process to generate data samples from noise.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/diffusion_1_ddmp/ddmp_graph.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"why\"\u003eWhy?\u003c/h2\u003e\n\u003cp\u003eCompared with other AI tasks, image generation is harder, since it does not have a standard answer. To solve this issue, GAN and VAE are propsed.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGAN uses another model (discriminator) to decide the quality of generated images.\u003c/p\u003e","title":"DL | Diffusion Models 1 - DDMP"},{"content":"This is an inline (a^=x-b^) equation.\nThese are block equations:\n\\[a^*=x-b^*\\]\\[ a^*=x-b^* \\]\\[ a^*=x-b^* \\]These are block equations using alternate delimiters:\n$$a^*=x-b^*$$$$ a^*=x-b^* $$$$ a^*=x-b^* $$","permalink":"http://localhost:1313/math-examples/","summary":"\u003cp\u003eThis is an inline (a^\u003cem\u003e=x-b^\u003c/em\u003e) equation.\u003c/p\u003e\n\u003cp\u003eThese are block equations:\u003c/p\u003e\n\\[a^*=x-b^*\\]\\[ a^*=x-b^* \\]\\[\na^*=x-b^*\n\\]\u003cp\u003eThese are block equations using alternate delimiters:\u003c/p\u003e\n$$a^*=x-b^*$$$$ a^*=x-b^* $$$$\na^*=x-b^*\n$$","title":"Math examples"},{"content":"1. Multi-Agent Systems A multi-agent system (MAS) consists of multiple decision-making agents which interact in a shared environment to achieve common or conflicting goals.\nMAS research spans a range of problems, such as\nhow to design MAS to incentivize certain behaviors in agents, how to design algorithms enabling agents to achieve specific goals in a MAS, how information is communicated and propagated among agents. 2. Monte Carlo Tree Search (MCTS) The MCTS focus on the analysis of the most promising moves, expanding the search tree based on random sampling of the search space. MCTS is based on many palyouts, and the result of each playout is then used to weight the nodes.\nA playout, also called rollout, is a sequence of state-action-reward tuples until the terminal. Definition (static evaluation function) A static evaluation function (also known as evaluation function or heuristic evaluation function) is a function used to estimate the goodness of a state. The function is called static because it does not take into account the history of the state or explore possible forward moves. Each round of MCTS consists of four steps:\nSelection: start from root node $R$ and select successive child nodes until a leaf node $L$ is reached. The leaf is any node that has a potential child from which no playout has yet been initiated.\nExpansion: unless $L$ is the terminal state, create child nodes and choose node $C$ from one of them. Child nodes are any valid moves from the state $L$.\nSimulation: complete one random playout from node $C$.\nBackpropagation: use the result of the playout to update the information in the nodes on the path from $C$ to $R$.\nWhen selecting child nodes, the exploration-exploitation techniques, such as UCT and UCB, can be applied.\nIn the $n$-agent sequential move games, the MCTS can be quite inefficient since the search space grows exponentially. There are some improvements for MCTS\nParanoid assumption: all other agents are always conspiring against the root agent. This simplifies the game to a 2-agent game.\nProgressive widening or progressive unpruning: restrict the number optional moves to $[cn^\\alpha]$ ,where $n$ is the number of visits to the current node so far.\nOpponent move abstraction (OMA): The MCTS in its basic form estimates the value of each move $a_d$ at depth $d$ in the precise context of its entire history from the root. These estimators are unbiased, but require large number of simulations. Move abstraction is a technique that generalize context by focusing on only a part of a move\u0026rsquo;s history. OMA ignores past moves of component agents and only considers past moves of the currently moving player.\n3. Upper Confidence bounds applied to Trees (UCT) In selection, the choice of actions depends on the value the actions at the root node. The value of a state-action node is computed based on the average of the sum of rewards along the edges originating at the node and the values at the corresponding successor nodes. To find an action whose value is within the $\\epsilon$-vicinity of that of the best, the depth of the tree needed is proportional to $1/(1-\\gamma) \\log \\frac{1}{\\epsilon(1-\\gamma)}$, hence the width is proportional to $\\frac{K}{\\epsilon (1-\\gamma)}$.\nIn the selection step of MCTS, UCT treats it as a multi-armed bandits problem, selecting the action that maximizes $$ Q_{d,t}(s,a) + \\sqrt{\\frac{2\\ln N_{d,t}(s)}{N_{d,t}(s,a)}}, $$ where $N_{d,t}(\\cdot)$ is the number times state has been visited up to time t at depth $d$.\n4. Context Generalization The count $N_{d,t}(s,a)$ is hard to update, because it depends on the trajectories of states and actions. To speed up training, we need to reduce the trajectory space. This technique is called abstraction. This can be done by reducing dimensionality.\nAnother idea is to define the pseudo-counts $\\hat{N}$.\nFitting generative models. fit a density model $p_\\theta(s)$ s.t. $p_\\theta(s)$ is high even $s$ has never been seen if $s$ is similar to previously seen states. $$ p_\\theta(s) = \\frac{\\hat{N}(s)}{\\hat{n}}, \\quad p_{\\theta'}(s) = \\frac{\\hat{N}(s) + 1}{\\hat{n}+1}. $$ It leads to $\\hat{N}(s) = \\hat{n}p_\\theta(s), \\hat{n} = \\frac{1-p_{\\theta'}(s)}{p_{\\theta'}(s) - p_\\theta(s)} p_\\theta(s)$. The model $p_\\theta$ can be a deep neural network (learning hashing) 5. Rapid Action Value Estimation (RAVE) 5. Best-Reply-Search (BRS) One difficulty of MAS is that the width of tree grows exponentially with the number of agents. To solve this problem we need opponent abstraction.\nLike Paranoid, BRS assumes that all opponents are playing against the root agent, however, it restricts the move choices of this coalition-only the opponent with the strongest move against the root can move, while all other opponents pass. This also simplifies the game to a 2-agent game\nBut BRS can lead to illegal game states if passing is not allowed in the game. This motivates BRS+, which assumes access to a static move ordering function. Its basic idea is that whenever an opponent does not have the strongest move against the root, this opponent play the move ranked highest by the move ordering (a fixed move).\nReferences [1] L. Kocsis and C. Szepesvári, “Bandit Based Monte-Carlo Planning,” in Machine Learning: ECML 2006, vol. 4212, J. Fürnkranz, T. Scheffer, and M. Spiliopoulou, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2006, pp. 282–293. doi: 10.1007/11871842_29.\n[2] H. Baier and M. Kaisers, “ME-MCTS: Online Generalization by Combining Multiple Value Estimators,” in Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, Montreal, Canada, Aug. 2021, pp. 4032–4038. doi: 10.24963/ijcai.2021/555.\n[3] J. He, M. Suau, and F. A. Oliehoek, “Influence-Augmented Online Planning for Complex Environments,” arXiv:2010.11038 [cs], Jun. 2021, Accessed: Sep. 20, 2021. [Online]. Available: http://arxiv.org/abs/2010.11038\n[4] S. Srinivasan, E. Talvitie, and M. Bowling, “Improving Exploration in UCT Using Local Manifolds,” p. 7.\n[5] H. Baier and M. Kaisers, “Guiding Multiplayer MCTS by Focusing on Yourself,” in 2020 IEEE Conference on Games (CoG), Osaka, Japan, Aug. 2020, pp. 550–557. doi: 10.1109/CoG47356.2020.9231603.\n[6] S. Gelly and D. Silver, “Monte-Carlo tree search and rapid action value estimation in computer Go,” Artificial Intelligence, vol. 175, no. 11, pp. 1856–1875, Jul. 2011, doi: 10.1016/j.artint.2011.03.007.\n","permalink":"http://localhost:1313/posts/rl/rl_09_mcts/","summary":"\u003ch2 id=\"1-multi-agent-systems\"\u003e1. Multi-Agent Systems\u003c/h2\u003e\n\u003cp\u003eA multi-agent system (MAS) consists of multiple decision-making agents which interact in a shared environment to achieve common or conflicting goals.\u003c/p\u003e\n\u003cp\u003eMAS research spans a range of problems, such as\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ehow to design MAS to incentivize certain behaviors in agents,\u003c/li\u003e\n\u003cli\u003ehow to design algorithms enabling agents to achieve specific goals in a MAS,\u003c/li\u003e\n\u003cli\u003ehow information is communicated and propagated among agents.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"2-monte-carlo-tree-search-mcts\"\u003e2. Monte Carlo Tree Search (MCTS)\u003c/h2\u003e\n\u003cp\u003eThe MCTS focus on the analysis of the most promising moves, expanding the search tree based on random sampling of the search space. MCTS is based on many palyouts, and the result of each playout is then used to weight the nodes.\u003c/p\u003e","title":"RL | Multiplayer Monte Carlo Tree Search"},{"content":"One shortcoming to DP methods is that they assume the knowledge of the dynamics. To overcome it, we introduced two model-free methods: MC and TP. They are capable of learning a wide range of tasks. However, such methods suffer from very high sample complexity. In this chapter, we consider a model-based method with dynamics approximation.\n1. Dynamics Approximation Let $p(s'|s,a)$ be the unknown dynamics function, and $f_\\theta(s_t, a_t)$ be the learned dynamics function parameterized by $\\theta$. A straightforward parameterization for $f_\\theta(s_t, a_t)$ would take as input the current state-action pair and ouput the predicted next state $\\hat{s}_{t+1}$. However, this function can be difficult to learn as $s_t$ and $s_{t+1}$ can be too similar and the action has seemingly little effect on the ouput. Instead predict the next state directly, we predict the change in state $s_t$ over the time step $$ f_\\theta(s_t, a_t) = \\hat{\\Delta}_{t+1}:= \\hat{s}_{t+1} - s_t. $$Thus, the predicted next state is $\\hat{s}_{t+1} = s_t + f_\\theta(s_t, a_t)$.\nGiven a dataset $D$ of trajectories $\\tau = (s_0,a_0, s1,...,s_{T-1})$, we can train $f_\\theta$ by performing SGD on the following object: $$ J(\\theta) = \\sum_{(s_t,a_t,s_{t+1})\\in D} || (s_{t+1} - s_t) - f_\\theta(s_t, a_t) ||_2^2. $$ In practice, it\u0026rsquo;s helpful to normalize the target $s_{t+1} - s_t$ as it increases model robustness.\nNote that we can collect training data $D$ by initializing a starting state $s_0$ and executing actions from a random policy $\\pi_0$ at each timestep. It implies that the model-based methods can learn from off-policy data.\n2. Model-Based Control In this section, we consider how to make decisions given the learned dynamics model. The goal at each time step is to take the action that maximizes the future return, given by $$ Q_\\theta(a_t,...) = \\sum_{t' = t}^\\infty \\gamma^{t'- t} R(s_{t'}, a_{t'}), $$ where $R(s,a)$ is the rewrad obtained when executing action $a$ at state $s$ and $s_{t+1} = s_t + f_\\theta(s_t,a_t)$.\nIt is impossible to plan over an infinite sequence of actions, so we optimize the sequence of actions $\\mathbf{A}_t = (a_t,...,a_{t+H-1})$ over a finite horizon $H$.\n$$ \\begin{equation} \\mathbf{A}_t = \\arg\\max_{A_t'}\\sum_{t'=t}^{t+H-1}\\gamma^{t'- t} R(s_{t'}, a_{t'}), \\quad s_{t+1} = s_t + f_\\theta (s_t,a_t). \\end{equation} $$Note that the learned dynamics model is imperfect, so it is desirable to plan $H$ steps since the accumulating errors make planing far into the future inaccurate. But this approach is also limited becaue it may not be sufficient for solving long-horizon tasks.\nSolving the Equation (1) is diffifult due to the dynamics and reward functions being nonlinear. The simplest technique to solve this problem is random shooting method\nRandom Shooting.\nPick $K$ candidate action sequences $\\mathbf{A}$ from a distribution $p(\\mathbf{A})$. Evaluate the performace $Q_\\theta(\\mathbf{A})$ for each candidate. Choose the elite with highest $Q_\\theta(\\mathbf{A})$. Because of the inaccuracies in the learned dynamics model, rather than have the policy execute this action sequence, we use model predictive control (MPC): the policy executes only the first action $a_t$, receives updated state information $s_{t+1}$, and recalculates the optimal action sequence at the next timestep.\nSo far we have obtained the model-based reinforcement learning algorithm.\nModel-Based RL (version 0.5) Gather dataset $D$ from a random policy $\\pi_0$ Train $f_\\theta$ using $D$ by performing gradient descent. Plan through $f_\\theta(s,a)$ to choose actions. 3. Improving Model-Based RL Although model-based RL is in theory off-policy, in practice it will perform poorly because of the distribution dismatch problem: $$ p_{\\pi_f}(s_t) \\neq p_{\\pi_0}(s_t), $$ where $p_\\pi(s)$ is the distribution of states generated by policy $\\pi$. One solution is on-policy data aggregation.\nModel-Based RL with On-Policy Data (version 1.0) gather dataset $D$ from a random policy $\\pi_0$ initialize model $f_\\theta$ randomly. for each iter do train $f_\\theta$ using $D$ by performing gradient descent. for each timestep $t = 1:T$ do plan through $f_\\theta(s,a)$ to obtain the optimal action sequence $\\mathbf{A_t}$. MPC: execute first action $a_t$ from selected action sequene, and observe next state $s_{t+1}$. add $(s_t, a_t, s_{t+1})$ to dataset $D$. Our policy depends on the model $f_\\theta$, but we are not certain about the model. One solution is to model the uncertainty, e.g. apply a prior to $\\theta$. An alternative idea is model ensembles: rather than training one model $f_\\theta$, we train $N$ independent models $\\{f_{\\theta_i}\\}$ and average their predictions to get the final predictions $$ f(s_t,a_t) = \\frac{1}{N} \\sum_{n=1}^N f_{\\theta_n}(s_t, a_t). $$ Model-Based RL with Ensemble (version 2.0) gather dataset $D$ from a random policy $\\pi_0$ initialize models $\\{f_{\\theta_n}\\}$ randomly. for each iter do train each model $f_{\\theta_n}$ using $D$ by performing SGD, $n = 1,...,N$. for each timestep $t = 1:T$ do plan through the averaged model $f(s,a)$ to obtain the optimal action sequence $\\mathbf{A_t}$. MPC: execute first action $a_t$ from selected action sequene, and observe next state $s_{t+1}$. add $(s_t, a_t, s_{t+1})$ to dataset $D$. ","permalink":"http://localhost:1313/posts/rl/rl_08_dynamics_approximation/","summary":"\u003cp\u003eOne shortcoming to DP methods is that they assume the knowledge of the dynamics. To overcome it, we introduced two model-free methods: MC and TP. They are capable of learning a wide range of tasks. However, such methods suffer from very high sample complexity. In this chapter, we consider a model-based method with dynamics approximation.\u003c/p\u003e\n\u003ch2 id=\"1-dynamics-approximation\"\u003e1. Dynamics Approximation\u003c/h2\u003e\n\u003cp\u003eLet $p(s'|s,a)$ be the unknown dynamics function, and $f_\\theta(s_t, a_t)$ be the learned dynamics function parameterized by $\\theta$. A straightforward parameterization for $f_\\theta(s_t, a_t)$ would take as input the current state-action pair and ouput the predicted next state $\\hat{s}_{t+1}$. However, this function can be difficult to learn as $s_t$ and $s_{t+1}$ can be too similar and the action has seemingly little effect on the ouput. Instead predict the next state directly, we predict the change in state $s_t$ over the time step\n\u003c/p\u003e","title":"RL | Dynamics Approximation for Model-Based RL"},{"content":"So far all methods have been action-value methods. In this chapter we consider methods that learn a parameterized policy that can select actions with consulting a value function. We write a parametric policy $\\pi_\\theta(a|s) = \\Pr(A_t = a| S_t = s, \\theta)$.\nOne advantage of parameterizing policies is that it can learn stochastic policies and change action probabilities smoothly as a function of the learned parameter. Largely because of this, stronger convergence guarantees are available for policy-gradient methods.\n1. The Policy Gradient Theorem First we consider how to measure the performance of the parameter $\\theta$. In episodic case, we can assume a fixed start state $s_0$ w.l.o.g. and define the performance as $$ \\begin{align} J(\\theta) = v_{\\pi_\\theta}(s_0), \\end{align} $$ where $v_{\\pi_\\theta}(s_0)$ is the value function for policy $\\pi_\\theta$, the policy determined by $\\theta$. In continuous case, where not start states or termination exit and no discounting factor for future rewards, we need to define performance in terms of the average rate of reward per time step: $$ \\begin{align} J(\\theta) \u0026= \\sum_s \\mu_{\\pi_\\theta}(s) R_{\\pi_\\theta}(s) = \\mathbb{E}_{\\pi_\\theta}[r], \\end{align} $$ Where $R_{\\pi}(s)$ is the expected reward at state $s$ following the policy $\\pi$, i.e. $R_\\pi(s) = \\sum_a \\pi(a|s) r(s,a)$ and $\\mu_\\pi$ is the stationary measure of Markov chain for policy $\\pi$.\nTheorem 7.1 (policy gradient theorem) For any differentiable policy $\\pi_\\theta$, for both episodic and continuous cases, the gradient of $J(\\theta) $ w.r.t. $\\theta$ is $$ \\begin{equation} \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) q_{\\pi_\\theta}(s,a) \\right]. \\end{equation} $$ Naturally, we can define the loss function as $$ L(\\theta) = -J(\\theta) = -\\mathbb{E}_{\\pi_\\theta}[\\log \\pi_\\theta(a|s) q_{\\pi_\\theta}(s,a)], $$ and improve the performance by SGD.\nProof For the epidotic case, $$ \\begin{align*} \\nabla_\\theta v_\\pi(s) \u0026= \\nabla_\\theta \\left[\\sum_{a} \\pi_\\theta(a|s) q_{\\pi_\\theta}(s,a) \\right] \\cr \u0026= \\sum_{a} \\left[ \\nabla_\\theta\\pi_\\theta(a|s) q_{\\pi_\\theta}(s,a) + \\pi_\\theta(a|s) \\nabla_\\theta q_{\\pi_\\theta}(s,a) \\right] \\cr \u0026= \\sum_{x\\in\\mathcal{S}} \\sum_n P^{\\pi_\\theta}_n(x|s) \\sum_a \\nabla_\\theta \\pi_\\theta(a|x) q_{\\pi_\\theta}(x,a)\\cr \u0026= \\sum_{x\\in\\mathcal{S}} \\eta(x) \\sum_a \\nabla_\\theta \\pi_\\theta(a|x) q_{\\pi_\\theta}(x,a), \\end{align*} $$ where $P^\\pi_n$ is the n-step transition probability following $\\pi$, and $\\eta(s)$ is the time spent on state $s$. It follows that $$ \\begin{align*} \\nabla_\\theta J(\\theta) \u0026= \\nabla_\\theta v_\\pi(s_0) \\cr \u0026=\\sum_{s} \\eta(s) \\sum_a \\nabla_\\theta \\pi_\\theta(a|s) q_{\\pi_\\theta}(s,a) \\cr \u0026= \\sum_{s'} \\eta(s') \\sum_{s} \\frac{\\eta(s)}{\\sum \\eta(s')} \\sum_a \\nabla_\\theta \\pi_\\theta(a|s) q_{\\pi_\\theta}(s,a) \\cr \u0026= \\sum_{s'} \\eta(s') \\sum_s \\mu_{\\pi_\\theta}(s) \\sum_a \\nabla_\\theta \\pi_\\theta(a|s) q_{\\pi_\\theta}(s,a) \\cr \u0026\\propto \\sum_s \\mu_{\\pi_\\theta}(s) \\sum_a \\nabla_\\theta \\pi_\\theta(a|s) q_{\\pi_\\theta}(s,a). \\end{align*} $$ For the continuous case, we also have $\\nabla_\\theta J(\\theta) = \\sum_s \\mu_{\\pi_\\theta}(s) \\sum_a \\nabla_\\theta \\pi_\\theta(a|s) q_{\\pi_\\theta}(s,a)$. Since $\\nabla_\\theta \\pi_\\theta = \\pi_\\theta \\nabla_\\theta \\log \\pi_\\theta$, we see that $$ \\begin{align} \\sum_s \\mu_{\\pi_\\theta}(s) \\sum_a \\nabla_\\theta \\pi_\\theta(a|s) q_{\\pi_\\theta}(s,a) \u0026= \\sum_s \\mu_{\\pi_\\theta}(s) \\sum_a \\pi_\\theta(a|s) \\nabla_\\theta \\log \\pi_\\theta(a|s) q_{\\pi_\\theta}(s,a) \\cr \u0026= \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) q_{\\pi_\\theta}(s,a) \\right]. \\end{align} $$ $\\Box$\n2. Monte Carlo Policy Gradient Update $\\theta$ by $$ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta( A_t|S_t) G_t. $$ Monte Carlo Policy Gradient (REINFORCE)(episodic) Initialize $\\theta$. Repeat: for each episode from $\\pi_\\theta$, Update $\\theta$ by $$ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta( A_t|S_t) G_t. $$ However, Monte-Carlo policy gradient may of high variance and thus produce slow learning. One solution to this is to conclude a baseline $b(s)$ that does not depend on $a$. Then we have $$ \\nabla_\\theta J(\\theta) \\propto \\sum_s \\mu_{\\pi_\\theta}(s) \\sum_a \\nabla_\\theta \\pi_\\theta(a|s) (q_{\\pi_\\theta}(s,a) - b(s)). $$ The equation remains valid since the gradient of a probability measure sums up to 0. The update rule now becomes $$ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta( A_t|S_t)( G_t - b(S_t)). $$Moreover, the baseline function $b$ can be defined as a learnable function $b(s,w)$.\nREINFORCE with Baseline (episodic) Initialize $\\theta,w$. Initialize $S_0$ Repeat: for each episode from $\\pi_\\theta$, Compute $\\delta = G_t - b(S_t,w)$ Update $\\theta$ by $$ \\theta \\leftarrow \\theta + \\alpha_\\theta \\nabla_\\theta \\log \\pi_\\theta(A_t|S_t) \\delta. $$ Update $w$ by $$ w \\leftarrow w + \\alpha_w \\nabla_w b(S_t, w) \\delta. $$ 3. Actor-Critic Policy Gradient Methods that learn approximations to both policy and value functions are called actor-critic methods, where \u0026lsquo;actor\u0026rsquo; refers to the learned policy, and \u0026lsquo;critic\u0026rsquo; refers to the learned value function.\nAlthough the REINFORCE with baseline method learns both a policy and a state-value function, we don\u0026rsquo;t consider it to be an actor-critic method because its state-value function is used only as a baseline.\nTo apply the actor-critic method, we need approximate $q_\\pi(s,a)$ using a learnable function $Q(s,a,w)$. First consider one-step actor-critic methods:\nactor: update $\\theta$ by policy gradient. critic: update $w$ by TD(0). One-step AC methods replace the return $G_t$ with the one-step return $R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$\nOne-step Aactor-Critic (episodic and continuous) Initialize $\\theta,w$. Initialize $S_0$. Repeat: for each time step, Choose action $A_t \\sim \\pi_\\theta(\\cdot|S_t)$, and observe $S_{t+1}, R_{t+1}$. Compute $\\delta = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$. Update $\\theta$ by $$ \\theta \\leftarrow \\theta + \\alpha_\\theta \\nabla_\\theta \\log \\pi_\\theta(A_t|S_t) \\delta. $$ Update $w$ by $$ w \\leftarrow w + \\alpha_w \\nabla_w Q(S_t, A_t, w) \\delta. $$ For the continuous case We should compute $\\delta = R_{t+1} - \\bar{R} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$ and update $\\bar{R} \\leftarrow \\bar{R} + \\alpha_R \\delta$ since $q_{\\pi_\\theta}(s,a) = R_{t+1} - \\mathbb{E}_{\\pi_\\theta}[ r] + R_{t+2} - \\mathbb{E}_{\\pi_\\theta}[ r] + \\cdots.$\nNote that one-step methods are fully online and incremental. But the approximation to policy gradient is biased now since $Q$ dependents on action $A_t$. A biased policy gradient may not find the right solution. The following theorem shows that the bias can be avoid if we choose value function approximation carefully.\nTheorem 7.2 (compatible function approximation theorem) If the value function approximation satisfies Value function approximation is compatible to the policy, i.e. $$ \\nabla_w Q(s,a,w) = \\nabla_\\theta \\log \\pi_\\theta(a|s). $$ Parameters $w$ of $Q$ minimize the MSE $$ \\epsilon = \\mathbb{E}_{\\pi_\\theta} [ q_{\\pi_\\theta}(s,a) - Q(s,a,w)]^2. $$ Then the policy gradient is exact, $$ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta \\log \\pi_\\theta(a|a) Q(s,a,w) ] $$ Proof $\\nabla_w \\epsilon = 0$, and it follows that $$ \\mathbb{E}_{\\pi_\\theta} [( q_{\\pi_\\theta}(s,a) - Q(s,a,w)) \\nabla_w Q(s,a,w)] = 0. $$ Then the condition (i) finishes the proof. Aactor-Critic (episodic and continuous) with Eligibility Trances Initialize $\\theta,w$. Initialize $S_0$. Initialize $E_\\theta, E_\\theta = 0$ Repeat: for each time step, Choose action $A_t \\sim \\pi_\\theta(\\cdot|S_t)$, and observe $S_{t+1}, R_{t+1}$. Compute $\\delta = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$. Update $E_w = \\lambda_w E_{w} + \\nabla_w Q(S_t, A_t, w)$. Update $E_\\theta = \\lambda_\\theta E_{\\theta} + \\nabla_\\theta \\log \\pi_\\theta(A_t|S_t)$. Update $\\theta$ by $$ \\theta \\leftarrow \\theta + \\alpha_\\theta \\nabla_\\theta \\log \\pi_\\theta(A_t| S_t) \\delta E_\\theta. $$ Update $w$ by $$ w \\leftarrow w + \\alpha_w \\nabla_w Q(S_t, A_t, w) \\delta E_w. $$ For the continuous case We should compute $\\delta = R_{t+1} - \\bar{R} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$ and update $\\bar{R} \\leftarrow \\bar{R} + \\alpha_R \\delta$ since $q_{\\pi_\\theta}(s,a) = R_{t+1} - \\mathbb{E}_{\\pi_\\theta}[ r] + R_{t+2} - \\mathbb{E}_{\\pi_\\theta}[ r] + \\cdots.$\n4. Finite-Difference Policy Gradient In section 1, we derived the analytical policy gradient and both MC and AC methods introduced in section 2,3 were based on the analytical gradients. A simple way of estimating derivatives is the finite difference.\n$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_k} \\approx \\frac{J(\\theta + 1/2 e_k) - J(\\theta - 1/2 e_k)}{\\epsilon}, $$ where $e_k$ is unit vector with 1 in $k$-th element. Finite difference works even if $\\pi_\\theta$ is not differentiable.\n","permalink":"http://localhost:1313/posts/rl/rl_07_policy_gradient/","summary":"\u003cp\u003eSo far all methods have been action-value methods. In this chapter we consider methods that learn a parameterized policy that can select actions with consulting a value function. We write a parametric policy $\\pi_\\theta(a|s) = \\Pr(A_t = a| S_t = s, \\theta)$.\u003c/p\u003e\n\u003cp\u003eOne advantage of parameterizing policies is that it can learn stochastic policies and change action probabilities smoothly as a function of the learned parameter. Largely because of this, stronger convergence guarantees are available for policy-gradient methods.\u003c/p\u003e","title":"RL | Policy Gradient Methods"},{"content":"So far we have represented value function by a lookup table. But sometimes we may have large MDPs where there are too many states or actions to store in memory. One solution to this problem is estimating value function with function approximation\n$$ \\hat{v}(s,w) \\approx v_\\pi(s), \\quad \\hat{q}(s,a,w) \\approx q_\\pi(s,a). $$For the simplicity of notation, we denote the function approximator by $f$. With a differentiable loss function $J_w$, e.g. $$ J_w = \\mathbb{E}_\\pi (f(s,w) - v_\\pi(s))^2, $$ we can approximate $v$ or $q$ by GD, i.e. $$ w \\leftarrow w - \\frac{1}{2}\\alpha \\nabla_w J_w, $$ where $\\nabla_w J_w = 2 \\mathbb{E}_{\\pi} [(f(s,w) - v_\\pi(s)) \\nabla_w f(s,w)]$. We assume that samples are generated by policy $\\pi$, then we can estimate the gradient by sampling, $$ \\begin{equation} \\nabla_w J_w = 2(f(s,w) - v_\\pi(s)) \\nabla_w f(s,w). \\end{equation} $$ This is known as SGD.\nThe function approximator can be of various forms, e.g. linear function, neural network, decision tree, etc.\n1. MC and TD with Value Function Approximation So far we have assumed that the target function $v_\\pi$ or $q_\\pi$ is known, but in RL where is no supervisor, on rewards. So, in practice, we do not approximate $v_\\pi$ or $q_\\pi$ directly, but substitute a target for it.\nFor MC, the target is the return $G_t$ $$ \\begin{align*} \\nabla_w J_w \u0026= 2(f(S_t,(A_t),w) - G_t) \\nabla_w f(S_t,(A_t),w), \\cr \\end{align*} $$ For TD($0$), the target is the TD target $R_{t+1} + \\gamma f(S_{t+1},(A_{t+1}), w)$ $$ \\nabla_w J_w = 2(f(S_t,(A_t),w) - (R_{t+1} + \\gamma f(S_{t+1},(A_{t+1}), w))) \\nabla_w f(S_t,(A_t),w), $$ For TD($\\lambda$), the target is the $\\lambda$-return $G_t^\\lambda$ or $q_t^\\lambda$ $$ \\nabla_w J_w = 2(f(S_t,(A_t),w) - G_t^\\lambda) \\nabla_w f(S_t,(A_t),w). $$ When the argument $A_t$ if function $f$ exists, it means that $f$ is an approximator for $q_\\pi(s,a)$.\nThe backward view of TD($\\lambda$) becomes $$ \\begin{align*} E_0(s,(a)) \u0026= 0 \\cr E_t(s,(a)) \u0026= \\gamma \\lambda E_{t-1}(s,(a)) + \\nabla_w f(S_t,(A_t),w)\\cr \\delta_t \u0026= R_{t+1} - \\gamma f(S_{t+1},(A_{t+1}), w) - f(S_t,(A_t),w) \\cr w \u0026\\leftarrow w + \\alpha \\delta_t E_t. \\end{align*} $$2.Batch Methods All the methods we have discussed in the previous section required computation per state transition. In this section we present a method for batch computation.\nTake approximating state-value function $v_\\pi$ as an example, and the case for $q_\\pi$ is essentially the same. Assume that we have a training date set $D$ consisting of state-value pairs $$ D = \\{ (s_1, v_{\\pi,1}),...,(s_n, v_{\\pi,n}) \\}. $$ Define the loss function by $$ J_w = \\frac{1}{n}\\sum_{i=1}^n (v_{\\pi,i} - f(s_i,w))^2 = \\mathbb{E}_D [v_\\pi - f(s,w)]^2. $$The SGD yields $$ w \\leftarrow w - \\frac{1}{2} \\alpha \\nabla_w J_w. $$In particular, if the approximator $f$ is a linear function, i.e. $f(s,w) = x(s)^T w$, we can solve $w$ analytically $$ w = (X^T X)^{-1}X^T v_\\pi. $$However, the training data $D$ can be generated from many policies. To evaluate $q_\\pi$, we must learn off-policy. We use the same idea as $Q$-learning:\nLeast Square TD Q-learning (LSTDQ) Initialize parameters $w$ and target policy $\\pi_{0}$, behavior policy $\\pi$.\nGiven dataset $D$\nRepeat:\nGenerate transitions $(S_t, A_t, R_{t+1}, S_{t+1})$ from policy $\\pi$ and store in replay memory $D$.\nConsider alternative successor action $A' = \\pi_0(S_{t+1})$.\nSample mini-batch from $D$ and update $w$ by SGD $$ w \\leftarrow w - \\alpha(f(S_t, A_t,w) - (R_{t+1} + \\gamma f(S_{t+1}, A', w)))\\nabla_w f $$ update the policy $\\pi_0$ w.r.t. the function $f$ with new $w$.\nWith the policy evaluation, we can solve the control problem by policy iteration.\nLeast Square Policy Iteration Initialize parameters $w$, and derive a policy $\\pi$ w.r.t. $w$. Given the training dataset $D$ Repeat policy evaluation LSTDQ$(\\pi, D)$. policy improvement, update $\\pi$. Example 6.1 (Experience replay in deep Q-networks) Deep Q-networks (DQN) uses deep neural networks as the approximator for the action-value function $q_\\pi(s,a)$. Initialize parameters $w$ and a target action-value function $Q$. Given states $s$, take actions $a$ by $\\epsilon$-greedy w.r.t. $f(s,a,w)$. Store transitions $(S_t, A_t, R_{t+1}, S_{t+1})$ in replay memory $D$. Sample random mini-batch, of size $n$, of transitions $(s,a,r,s')$ from $D$. Compute Q-learning targets w.r.t network $f(s,a,w)$. Compute MSE $$ J_w = \\frac{1}{n} \\sum_{i=1}^n \\left( r + \\gamma \\max_{a'} Q(s',a') - f(s,a,w) \\right)^2. $$ Compute $w$ by SGD, and update the target $Q$ using $f(s,a,w^{old}).$ ","permalink":"http://localhost:1313/posts/rl/rl_06_value_function_approximation/","summary":"\u003cp\u003eSo far we have represented value function by a lookup table. But sometimes we may have large MDPs where there are too many states or actions to store in memory. One solution to this problem is estimating value function with function approximation\u003c/p\u003e\n$$ \n \\hat{v}(s,w) \\approx v_\\pi(s), \\quad \\hat{q}(s,a,w) \\approx q_\\pi(s,a).\n$$\u003cp\u003eFor the simplicity of notation, we denote the function approximator by $f$. With a differentiable loss function $J_w$, e.g.\n\u003c/p\u003e\n$$ \n J_w = \\mathbb{E}_\\pi (f(s,w) - v_\\pi(s))^2,\n$$\u003cp\u003e\nwe can approximate $v$ or $q$ by GD, i.e.\n\u003c/p\u003e","title":"RL | Value Function Approximation"},{"content":"Temporal-difference (TD) learning is a combination of Monte Carlo ideas and DP ideas. Like MC methods, TD can learn from raw experience without knowledge of the environment\u0026rsquo;s dynamics. Like DP, TD updates estimates based in part on other learned estimates, without waiting for a final output ($G_t$).\n1. TD Prediction Recall that a simple MC methods for nonstationary environment updates $V(S_t)$ by $$ V(S_t) \\leftarrow V(S_t) + \\alpha [ G_t - V(S_t)], $$ where $G_t$ can only be known when a visit to $S_t$ occurs. The simplest TD method updates $V$ by $$ \\begin{equation} V(S_t) \\leftarrow V(S_t) + \\alpha \\big[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\big]. \\end{equation} $$It implies that TD can update $V$ immediately on transition to $S_{t+1}$ and receiving $R_{t+1}$. This TD method is called TD(0), or one-step TD, because it is a special case of the $TD(\\lambda)$, $n$-step TD.\nBecause TD(0) update $V(S_t)$ using an existing estimate $V(S_{t+1})$, we say that is is a bootstrapping method, like DP. We know that $$ \\begin{align} v_\\pi(s) \u0026= \\mathbb{E}_\\pi [G_t|S_t = s] \\cr \u0026= \\mathbb{E}_\\pi [R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t = s]. \\end{align} $$ MC methods use an estimate of (2) as a target, whereas DP methods use an estimate of (3) as a target. The MC methods estimate the expectation in (2) by sampling and TD methods estimate $v_\\pi$ using existing estimate $V(S_{t+1})$. The TD methods combine the sampling of MC with the bootstrapping of DP.\n2. TD($\\lambda$) and $n$-step TD $n$-step returns: $G_t^{(n)} := R_{t+1} + \\cdots + \\gamma^{n-1}R_{t+n} + \\gamma^n V(S_{t+n})$ for $n=1,2,...,\\infty$. Then the $n$-step TD learning is $$ V(S_t) \\leftarrow V(S_t) + \\alpha [ G_t^{(n)} - V(S_t)]. $$ $\\lambda$-returns: $G_t^\\lambda:= (1-\\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1}G_t^{(n)}$. Then the $TD(\\lambda)$ learning is $$ V(S_t) \\leftarrow V(S_t) + \\alpha [ G_t^{\\lambda} - V(S_t)]. $$Like MC, the $\\lambda$-returns can only be computed from complete episodes.\nDefinition 5.1 (TD error) The TD error is defined as $\\delta_t:= R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$. Definition 5.2 (Eligibility trace) The Eligibility trace is defined as $$ \\begin{align*} E_0(s) \u0026= 0 \\cr E_t(s) \u0026= \\gamma \\lambda E_{t-1}(s) + 1_{S_t = s}. \\end{align*} $$ If we view $TD(\\lambda)$ backward, keeping $E_t(s)$ for every $s$ and $\\delta_t$ when updating $V(s)$, the $TD(\\lambda)$ can be written as an online learning algorithm $$ V(s) \\leftarrow V(s) + \\alpha \\delta_t E_t(s). $$Note that if the array $V$ does not change during the episode (episodic offline), then the MC error can be written as a sum of TD errors: $$ \\begin{align} G_t - V(S_t) \u0026= \\delta_t + \\gamma \\delta_{t+1} + \\cdots + \\gamma_{T-t-1}\\delta_{T-1} + \\gamma^{T-t}(G_T - V(S_T)) \\cr \u0026= \\delta_t + \\gamma \\delta_{t+1} + \\cdots + \\gamma_{T-t-1}\\delta_{T-1} + \\gamma^{T-t}(0 - 0) \\cr \u0026= \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k. \\end{align} $$ Theorem 5.3 The sum of offline updates is identical for forward-view and backward-view $TD(\\lambda)$, i.e. $$ \\sum_{t=1}^T \\alpha \\delta_t E_t(s) = \\sum_{t=1}^T \\alpha(G_t^\\lambda - V(S_t)) 1_{S_t = s}. $$ Consider an episode where state $s$ is visited once at time-step $k$. The eligibility trace of TD(1) is $$ E_t(s) = \\gamma^{t-k} 1_{t\\geq k}. $$By the Theorem 5.3, we see that $$ \\sum_{t=1}^{T-1} \\alpha \\delta_t E_t(s) = \\alpha \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k = \\alpha ( G_t - V(S_t)), $$ which is exactly the total update for MC. It implies that TD(1) is roughly equivalent to every-visit MC.\n3. On-policy TD Control :SARSA We now turn to the use of TD prediction methods for the control problem. The first step is to learn an action-value function $q_\\pi$. In the first section, we considered transitions from state to state and learned the value function $v_\\pi$. The idea of estimating $q_\\pi$ is essentially identical to estimating $v_\\pi$. Apply TD(0) to updating $Q(S_t, A_t)$ by $$ \\begin{equation} Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]. \\end{equation} $$ This rules uses the tuple $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$, that makes up a transition from one state-action pair to the next, $(S_t, A_t)\\to R_{t+1} \\to (S_{t+1}, A_{t+1})$. This tuple gives rise to the name SARSA for the algorithm.\nSARSA for On-policy Control Initialize $Q(s,a)$ arbitrarily for all $a,s$, and $Q(S_T,\\cdot) = 0$.\ninitialize a policy $\\pi$ w.r.t. $Q$.\nFor each episode:\nInitialize $S_0$ Choose $A_0$ using the policy $\\pi$. Loop for each step $t = 0,...,T-1$: take action $A_t$, observe $R_{t+1}, S_{t+1}$. choose action $A_{t+1}$ using policy derived from $Q$ (e.g., $\\epsilon$-greedy). incremental update $Q$ by $$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]. $$ Theorem 5.4 (convergence of SARSA) SARSA converges to the optimal action-value function $q_*$, under the following conditions: GLIE sequence of policies $\\pi_t(a|s)$. Robbins-Monro sequence of $\\alpha_t$, $$ \\sum_{t=1}^\\infty \\alpha_t = \\infty, \\quad \\sum_{t=1}^\\infty \\alpha_t^2\u003c\\infty. $$ We can also apply the TD($\\lambda$) idea or $n$-step TD learning to estimating $q_*$.\n$n$-step SARSA: $q_t^{(n)} := R_{t+1} + \\cdots + \\gamma^{n-1}R_{t+n} + \\gamma^n Q(S_{t+n}, A_{t+n})$ for $n=1,2,...,\\infty$. Then the $n$-step SARSA updates $$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [ q_t^{(n)} - Q(S_t, A_t)]. $$ $q^\\lambda$-return: $q_t^\\lambda:= (1-\\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1}q_t^{(n)}$. Then the forward SARSA($\\lambda$) updates $$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [ q_t^{\\lambda} - Q(S_t, A_t)]. $$Note that $q_t^\\lambda$ is unknown until the terminal state. Just like TD($\\lambda$), we use eligibility traces in an online algorithm. We need the eligibility trace for each state-action pair, $$ \\begin{align*} E_0(s,a) \u0026= 0 \\cr E_t(s,a) \u0026= \\gamma \\lambda E_{t-1}(s,a) + 1_{(S_t = s, A_t = a)}. \\end{align*} $$And the TD error $\\delta_t$ now becomes $$ \\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t), $$ and hence we update $Q(s,a)$ by $$ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\delta_t E_t(s,a). $$ SARSA($\\lambda$) for On-policy Control Initialize $Q(s,a)$ arbitrarily for all $a,s$ and $Q(S_T,\\cdot) = 0$.\ninitialize a policy $\\pi$ w.r.t. $Q$.\nFor each episode:\nInitialize $E(s,a) = 0$ for all $a,s$. Initialize $S_0$ Choose $A_0$ using the policy $\\pi$. Loop for each step $t = 0,...,T-1$: take action $A_t$, observe $R_{t+1}, S_{t+1}$. choose action $A_{t+1}$ using policy derived from $Q$ (e.g., $\\epsilon$-greedy). compute $\\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$, update $E(S_t, A_t) += 1 $. incremental update $Q, E$ for all $s,a$ by $$ \\begin{align*} Q(s,a) \u0026\\leftarrow Q(s,a) + \\alpha \\delta_t E(s,a), \\cr E(s,a) \u0026\\leftarrow \\gamma \\lambda E(s,a). \\end{align*} $$ 4. Off-policy TD Control: Q-learning We now apply TD methods to off-policy control. Given the a policy $\\pi$ and a behavior policy $\\mu$, when choosing next action $A_{t+1}$ conditioned on state $S_{t+1}$ using the policy $\\mu$, we also consider alternative successor action $A'$ using the target policy $\\pi$. Then we can update $Q$ by $$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A') - Q(S_t, A_t)], $$ where reward $R_{t+1}$ is a result of action $A_{t+1}$.\nNow we allow both behavior and target policies to improve:\nthe target policy $\\pi$ is greedy w.r.t. $Q(s,a)$. the behavior policy $\\mu$ is e.g. $\\epsilon$-greedy w.r.t. $Q(s,a)$. The Q-learning target then simplifies: $$ \\begin{align*} R_{t+1} + \\gamma Q(S_{t+1}, A') \u0026= R_{t+1} + \\gamma Q(S_{t+1}, \\arg\\max_{a'} Q(S_{t+1}, a')) \\cr \u0026= R_{t+1} + \\max_{a'}\\gamma Q(S_{t+1}, a'). \\end{align*} $$Hence Q-learning becomes $$ \\begin{equation} Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\max_{a'}\\gamma Q(S_{t+1}, a') - Q(S_t, A_t)] \\end{equation} $$ Theorem 5.5 Q-learning control converges to the optimal action-value function $q_*$. Q-learning for Off-policy Control Initialize $Q(s,a)$ arbitrarily for all $a,s$ and $Q(S_T,\\cdot) = 0$.\ninitialize the behavior policy $\\mu$ w.r.t. $Q$.\nFor each episode:\nInitialize $S_0$\nChoose $A_0$ using the policy $\\mu$ (derived from $Q$).\nLoop for each step $t = 0,...,T-1$:\ntake action $A_t$, observe $R_{t+1}, S_{t+1}$. choose action $A_{t+1}$ using policy $\\mu$ derived from $Q$ (e.g., $\\epsilon$-greedy). incremental update $Q$ by $$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\max_{a'}\\gamma Q(S_{t+1}, a') - Q(S_t, A_t)]. $$ $Q(s,a)\\to q_*(s,a)$ by theorem 5.5, then we obtain $\\pi_*$ from $q_*$.\n","permalink":"http://localhost:1313/posts/rl/rl_05_temporal_difference/","summary":"\u003cp\u003eTemporal-difference (TD) learning is a combination of Monte Carlo ideas and DP ideas. Like MC methods, TD can learn from raw experience without knowledge of the environment\u0026rsquo;s dynamics. Like DP, TD updates estimates based in part on other learned estimates, without waiting for a final output ($G_t$).\u003c/p\u003e\n\u003ch2 id=\"1-td-prediction\"\u003e1. TD Prediction\u003c/h2\u003e\n\u003cp\u003eRecall that a simple MC methods for nonstationary environment updates $V(S_t)$ by\n\u003c/p\u003e\n$$ \n V(S_t) \\leftarrow V(S_t) + \\alpha [ G_t - V(S_t)],\n$$\u003cp\u003e\nwhere $G_t$ can only be known when a visit to $S_t$ occurs. The simplest TD method updates $V$ by\n\u003c/p\u003e","title":"RL | Model-Free: Temporal Difference Learning"},{"content":"We have introduced the DP algorithm for estimating value functions and optimal policies. One drawback to DP is that assumes complete knowledge of the environment. Now we will introduce two model-free methods: Monte Carlo methods and temporal-difference learning.\nIn this chapter we will consider the Monte Carlo methods for prediction and control in an unknown MDP.\n1. Monte Carlo Prediction We begin by consider Monte Carlo methods for learning the value function $v_\\pi$ for a given policy $\\pi$. Suppose that we wish to estimate $v_\\pi(s)$ given a set of episodes under policy $\\pi$ $$ S_0, A_0 , R_1, ..., S_{T-1}, A_{T-1}, R_T \\sim \\pi. $$Recall that the $v_\\pi(s)$ is the expected return, i.e. expected cumulative future discounted rewards, starting from state $s$, $$ v_\\pi(s) = \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma R_{t+2} + \\cdots| S_t = s]. $$ The idea of Monte Carlo methods is to estimate the expected return by the mean of empirical returns.\nDefinition 4.1 (visit to states) Each occurrence of state $s$ in an episode is called a visit to $s$. 1.1 First-visit MC prediction Initialize the total return $S(s) = 0$. For each episode under $\\pi$: $S_0, A_0 , R_1, ..., S_{T-1}, A_{T-1}, R_T$, initialize $G = 0$.\nFor $t = T-1,...,0$:\nupdate $G_t = \\gamma G_{t+1} + R_{t+1}$. if $S_t = s$, update the total return $S(s)+= G_t$ and end the for loop. Do steps 1-2 for each episode and record the total number of the first visits to $s$, denoted $N(s)$. By LLN, $$ \\frac{S(s)}{N(s)} \\to v_\\pi(s), $$ as $N(s) \\to \\infty$.\n1.2 Every-visit MC prediction Initialize the total return $S(s) = 0$. For each episode under $\\pi$: $S_0, A_0 , R_1, ..., S_{T-1}, A_{T-1}, R_T$, initialize $G = 0$.\nFor $t = T-1,...,0$:\nupdate $G_t = \\gamma G_{t+1} + R_{t+1}$. if $S_t = s$, update the total return $S(s)+= G_t$ and reset the $G =0$. Do steps 1-2 for each episode and record the total number of visits to $s$, denoted $N(s)$. By LLN, $$ \\frac{S(s)}{N(s)} \\to v_\\pi(s), $$ as $N(s) \\to \\infty$.\nRecall the incremental mean that $$ \\mu_n = \\sum_{i=1}^n x_i = \\mu_{n-1} + \\frac{1}{n}(x_k - \\mu_{k-1}). $$ We can update $V(s):= \\frac{S(s)}{N(s)}$ incrementally by $$ V^{new}(s) = V^{old}(s) + \\frac{1}{N(s)}(G_t - V^{old}(s)). $$ In non-stationary problems, we can replace the factor $\\frac{1}{N(s)}$ by $\\alpha \\in(0,1)$.\n2. Monte Carlo Control Recall that the policy improvement discussed in DP is given by $$ \\begin{align*} \\pi'(s) \u0026:= \\arg\\max_{a} q_\\pi(s,a) \\cr \u0026=\\arg\\max_a \\sum_{s',r}p(s',r|s,a) [ r+ \\gamma v_\\pi(s')]. \\end{align*} $$For an unknown MDP, the transition probabilities $p$ are unknown, so that it is impossible to implement the policy improvement with value function $v_\\pi$ alone. Instead, it is more useful to use the action-value function $q_\\pi$.\nThe Monte Carlo methods for estimating $q_\\pi$ are essentially the same as presented for $v_\\pi$, except now we consider visits to the state-action pair $(s,a)$ rather than to a state $s$.\nMonte Carlo prediction for $q_\\pi$ For each episode: Generate an episode following $\\pi$: $S_0,A_0,R_1,...,R_T$. Initialize $G = 0$. Loop for each step $t = T - 1 ,...,0$: update $G_t = \\gamma G_{t+1} + R_{t+1}$. if $S_t = s, A_t = a$, update the total return $S(s,a)+= G_t$ and end the loop. $q_\\pi(s,a)$ is estimated by the empirical mean $Q(s,a) = \\frac{S(s,a)}{N(s,a)}$.\nThen we can estimate the optimal policy $\\pi_*$ by the policy iteration $$ \\pi_0 \\xrightarrow{E} q_{\\pi_0} \\xrightarrow{I} \\pi_1 \\xrightarrow{E} q_{\\pi_1} \\to \\cdots \\pi_* \\xrightarrow{E} q_*, $$ where $\\xrightarrow{E}$ denotes a policy evaluation done by Monte Carlo methods and $\\xrightarrow{I}$ denotes the policy improvement. The policy improvement $$ \\pi'(s) = \\arg\\max_{a} q_\\pi(s,a) $$ is greedy, which can prevent further exploration of nongreedy actions, so that we cannot obtain the optimal policy $\\pi_*$. One solution is to use the $\\epsilon$-greedy policy improvement, $$ \\pi(a|s) = \\begin{cases} \\frac{\\epsilon}{|\\mathcal{S}|} + 1 - \\epsilon, \u0026 a = \\arg\\max_{a'} Q(s,a') \\cr \\frac{\\epsilon}{|\\mathcal{S}|} ,\u0026 otherwise. \\end{cases} $$ Theorem 4.3($\\epsilon$-greedy policy improvement) For any $\\epsilon-$greedy policy $\\pi$, the $\\epsilon-$greedy policy $\\pi'$ w.r.t. $q_\\pi$ is an improvement, i.e. $v_{\\pi'}(s) \\geq v_\\pi(s)$ for all $s\\in\\mathcal{S}$. Proof $$ \\begin{align*} q_\\pi(s,\\pi'(s)) \u0026= \\sum_a \\pi'(a|s) q_\\pi(s,a) \\cr \u0026= \\frac{\\epsilon}{|\\mathcal{S}|} \\sum_{a} q_\\pi(s,a) + (1-\\epsilon) \\max_a q_\\pi(s,a) \\cr \u0026\\geq \\frac{\\epsilon}{|\\mathcal{S}|} \\sum_{a} q_\\pi(s,a) + (1-\\epsilon) \\sum_{a} \\frac{\\pi(a|s) - \\epsilon/|\\mathcal{S}|}{1-\\epsilon} \\cr \u0026= \\sum_a \\pi(a|s)q_\\pi(s,a) = v_\\pi(s). \\end{align*} $$ Then the desired result follows from the policy improvement theorem. $\\Box$\nThis theorem guarantees that the $\\epsilon$-greedy policy improvement does find a better policy using action-state functions $q_\\pi$.\nDefinition 4.2 (GLIE) A learning is called greedy in the limit with infinite exploration (GLIE) if it satisfies the following two properties: all state-action pairs are explored infinitely many times, $$ \\lim_{n\\to\\infty} N_n(s,a) = \\infty. $$ the learning policy converges to a greedy policy w.r.t. the learned $Q_n$. $$ \\lim_{n\\to\\infty} \\pi_n(a|s) = 1_{a= a_*}, $$ where $a_* = \\arg\\max_{a'} Q_n(s,a')$. $\\epsilon$-greedy policy is GLIE if $\\epsilon = \\epsilon_n = 1/n$. Naturally, we have a Monte Carlo Control method.\nGLIE Monte Carlo Control Initialize $Q(s,a)$ arbitrarily for all $a,s$.\ninitialize a policy $\\pi$ w.r.t. $Q$.\nFor each episode:\nGenerate an episode following $\\pi$: $S_0,A_0,R_1,...,R_T$. Initialize $G = 0$. Loop for each step $t = T - 1 ,...,0$: update $G_t = \\gamma G_{t+1} + R_{t+1}$. if $S_t = s, A_t = a$: $N(s,a)+= 1$. incremental update $Q$ by $$Q(s,a)\\leftarrow Q(s,a) + \\frac{1}{N(s,a)}(G_t - Q(s,a)).$$ $\\epsilon$-greedy policy improvement with $\\epsilon = 1/n$. By the $\\epsilon$-greedy policy improvement theorem, and the definition of GLIE, we see that GLIE Monte Carlo control converges to the optimal action-value function, $Q(s,a) \\to q_*(s,a)$ for all $s,a$.\n3. Off-policy Monte Carlo Control The Monte Carlo control methods introduced in the previous section are called on-policy control as they evaluate or improve the policy that is used to make decisions.\non-policy learning. Learn about policy $\\pi$ from experience sampled from $\\pi$. off-policy learning. Learn about policy $\\pi$ from experience sample from $\\mu$, where the policy $\\pi$ is called the target policy and the policy $\\mu$ used to generate experience is called the behavior policy. We begin with considering the prediction problem, where both target policy $\\pi$ and behavior policy $\\mu$ are given, and we wish to estimate $v_\\pi$ or $q_\\pi$.\nThe importance sampling method allows us to estimate the expectation under a measure $P$ by sampling from another measure $Q$, $$ \\begin{align*} \\mathbb{E}_P [f(X)] \u0026= \\mathbb{E}_Q \\left[ \\frac{P(X)}{Q(X)} f(X) \\right], \\end{align*} $$ where $L(P,Q):=\\frac{P(X)}{Q(X)}$ is called the importance sampling ratio.\nGiven a starting state $S_t$, the probability of the subsequent state-action trajectory, $A_t, S_{t+1}, A_{t+1},...,S_T$, following the policy $\\pi$ is $$ \\Pr(A_t, S_{t+1},...,S_T| S_t, A_{t:T-1}\\sim \\pi) = \\prod_{k=t}^{T-1} \\pi(A_k|S_k) p(S_{s+1}| S_k, A_k), $$ where $p$ the state-transition probability function. Thus we can easily obtain that the importance sampling ratio of policy $\\pi$ and policy $\\mu$ is given by $$ L_t(\\pi, \\mu) = \\prod_{k=t}^{T-1} \\frac{\\pi(A_k|S_k)}{\\mu(A_k|S_k)}. $$ Then by the importance sampling, we have $$ v_\\pi(s) = \\mathbb{E}_\\pi[G_t|S_t = s] = \\mathbb{E}_\\mu[L_t(\\pi, \\mu)G_t|S_t = s]. $$If we estimate $v_\\pi(s)$ by the ordinary average $$ V(s) = \\frac{\\sum L_t(\\pi,\\mu) G_t}{N(s)}, $$ the estimator typically have high variance. An alternative is weighted importance sampling, which uses a weighted average, given by $$ V(s) = \\frac{\\sum L_t(\\pi,\\mu) G_t}{\\sum L_t(\\pi,\\mu)}. $$ This estimator has lower variance, but higher bias. Written in the form of incremental implementation, we can update $V(s)$ by $$ \\begin{align*} V(s) \\leftarrow V(s) + \\frac{1}{N(s)}(L_{t}(\\pi,\\mu)G_t - V(s)), \u0026\\quad \\text{ordinary} \\cr V(s) \\leftarrow V(s) + \\frac{W}{C(s)}(G_t - V(s)), \u0026\\quad \\text{weighted} \\end{align*} $$ where $C(s) \\leftarrow C(s) + W$ is the cumulative sum of the weights $W$, e.g. $L_t(\\pi,\\mu)$. Similarly, we can use this method to estimate the action-value function $q_\\pi$.\nNow we consider estimating the optimal policy $\\pi_*$ by off-policy Monte Carlo.\nOff-policy Monte Carlo Control Initialize $Q(s,a)$ arbitrarily for all $a,s$.\nInitialize $C(s,a) = 0$ for all $a,s$.\ninitialize a policy $\\pi$ w.r.t. $Q$.\nFor each episode:\nGenerate an episode following the behavior policy $\\mu$: $S_0,A_0,R_1,...,R_T$. Initialize $G = 0, W = 1$. Loop for each step $t = T - 1 ,...,0$: update $G_t = \\gamma G_{t+1} + R_{t+1}$.\n$C(s,a)+= W$.\nif $S_t = s, A_t = a$:\nincremental update $Q$ by $$Q(s,a)\\leftarrow Q(s,a) + \\frac{W}{C(s,a)}(G_t - Q(s,a)).$$ improve the target policy $\\pi$ by greedy or $\\epsilon$-greedy policy improvement. exit the inner loop. update $W\\leftarrow W\\frac{\\pi(A_t|S_t)}{\\mu(A_t|S_t)}$\n","permalink":"http://localhost:1313/posts/rl/rl_04_monte_carlo/","summary":"\u003cp\u003eWe have introduced the DP algorithm for estimating value functions and optimal policies. One drawback to DP is that assumes complete knowledge of the environment. Now we will introduce two model-free methods: \u003cem\u003eMonte Carlo\u003c/em\u003e methods and \u003cem\u003etemporal-difference\u003c/em\u003e learning.\u003c/p\u003e\n\u003cp\u003eIn this chapter we will consider the Monte Carlo methods for prediction and control in an unknown MDP.\u003c/p\u003e\n\u003ch2 id=\"1-monte-carlo-prediction\"\u003e1. Monte Carlo Prediction\u003c/h2\u003e\n\u003cp\u003eWe begin by consider Monte Carlo methods for learning the value function $v_\\pi$ for a given policy $\\pi$. Suppose that we wish to estimate $v_\\pi(s)$ given a set of episodes under policy $\\pi$\n\u003c/p\u003e","title":"RL | Model-Free: Monte Carlo Methods"},{"content":"There are two common tasks in an MDP\nprediction: estimating the valued function $\\pi_\\pi$ given a MDP and a policy $\\pi$. control: finding optimal policy $\\pi_*$ and corresponding optimal value function $v_*$ given a MDP. In this chapter we will show how to use DP to solve the prediction and control problems in an MDP. DP is of limited utility in RL both because of their assumption of a perfect model (environment\u0026rsquo;s dynamics are completely known) and because of their great computational expense, but they are important theoretically.\n1. Policy Evaluation Recall the Bellman equation for value functions that, for any $s\\in\\mathcal{S}$, $$ \\begin{equation} v_\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a) (r + \\gamma v_\\pi(s')). \\end{equation} $$ Define\n$R_\\pi(s) = \\sum_a \\pi(a|s) r(s,a)$. $P_\\pi(s'|s) = \\sum_{a} \\pi(a|s) p(s'|s,a)$. Then we can represent (1) in the matrix form: $$ v_\\pi = R_\\pi + \\gamma P_\\pi v_\\pi. $$If the environment\u0026rsquo;s dynamics are completely known, then (1) is a system of $|\\mathcal{S}|$ linear equations with direct solution $$ v_\\pi = (I - \\gamma P_\\pi)^{-1}R_\\pi. $$Alternatively, it can be solved by iterative methods.\nGiven an initial approximation $v_0$, by the Bellman equation (1), the successive approximation can be obtained by $$ v_{k+1}(s):= \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a) (r + \\gamma v_k(s')). $$ If the $v_\\pi$ exits ($\\gamma\u003c1$), the sequence $v_k$ would converge to $v_\\pi$ as $k\\to\\infty$.\nMoreover, in view of the special form of Bellman optimality equation for $v_*$,\n$$ v_*(s) = \\max_a \\sum_{s'}\\sum_r p(s',r|s,a)[r+ \\gamma v_*(s')] = , $$ that it does not involve the factor $\\pi_*(a|s)$. It implies that we can estimate $v_*$ even though we do not know what the optimal policy is. In particular, we can estimate $v_*$ by solving $$ v_*(s) = \\max_a \\left(r(s,a) + \\gamma \\sum_{s'}p(s'|s,a) v_*(s') \\right) $$ for using the iterative relationship $$ v_{k+1}(s) = \\max_{a} \\sum_{s',r} p(s',r|s,a)(r +\\gamma v_k(s')). $$2. Policy Iteration For simplicity, we take the deterministic policy $\\pi$ as an example. Suppose we have solved the value function $v_\\pi$ for an arbitrary deterministic policy $\\pi$, we want to improve it.\nTheorem 3.1 (policy improvement theorem) Let $\\pi,\\pi'$ be any pair of deterministic policies s.t., for all $s\\in\\mathcal{S}$, $$ q_\\pi(s, \\pi'(s)) \\geq v_\\pi(s), $$ then $v_{\\pi'}(s) \\geq v_\\pi(s)$ for all $s\\in\\mathcal{S}$. Proof By the Bellman equation for $q_\\pi$, we have $$ q_\\pi(s,a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma v_\\pi(s')]. $$ Then by the given condition, we have $$ \\begin{align*} v_\\pi(s) \u0026\\leq q_\\pi(s,\\pi'(s)) \\cr \u0026= \\mathbb{E}[R_{t+1}+ \\gamma v_\\pi(S_{t+1})| S_t = s, A_t = \\pi'(s)] \\cr \u0026= \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma v_\\pi(S_{t+1})| S_t = s]\\cr \u0026\\leq \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma q_{\\pi}(S_{t+1}, \\pi'(S_{t+1}))| S_t = s]\\cr \u0026= \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma \\mathbb{E}[R_{t+2} + \\gamma v_\\pi(S_{t+2})|S_{t+1}, A_{t+1} = \\pi'(S_{t+1})]| S_t = s]\\cr \u0026= \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma R_{t+2} + \\gamma^2 v_\\pi(S_{t+2})| S_t = s] \\cr \u0026\\vdots \\cr \u0026\\leq \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots| S_t = s] = v_{\\pi'}(s). \\end{align*} $$ $\\Box$\nThis theorem implies that we can obtain an improved policy $\\pi'$ by $$ \\begin{align*} \\pi'(s) \u0026:= \\arg\\max_{a} q_\\pi(s,a) \\cr \u0026=\\arg\\max_a \\sum_{s',r}p(s',r|s,a) [ r+ \\gamma v_\\pi(s')]. \\end{align*} $$Suppose that we have got the new policy $\\pi'$ s.t. $v_\\pi = v_{\\pi'}$, then it follows that $$ v_{\\pi'}(s) = \\max_a \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_{\\pi'}(s')]. $$ This is the same as the Bellman equation for $v_{*}$, and hence $\\pi'$ must be the optimal policy. So far we have shown how to get an optimal deterministic policy. In the general case, a stochastic policy $\\pi$ can be improved by the same way.\nSo far we have studied how to improve a policy $\\pi$ using its value function $v_\\pi$. We can thus improve it iteratively: given an initial policy $\\pi_0$, $$ \\pi_0 \\xrightarrow{E} v_{\\pi_0} \\xrightarrow{I} \\pi_1 \\xrightarrow{E} v_{\\pi_1} \\to \\cdots \\pi_* \\xrightarrow{E} v_*, $$ where $\\xrightarrow{E}$ denotes a policy evaluation and $\\xrightarrow{I}$ denotes a policy improvement. This way of finding an optimal policy is called policy iteration.\n3. Value Iteration One drawback of policy iteration is that each step involves policy evaluation, which may itself be an iterative computation. One may consider truncating the policy evaluation in some ways without losing convergence.\nThe idea of the value iteration algorithm is to estimate the optimal value function $v_*$ first, and then do the policy improvement. In other words, $$ v_0 \\to v_1 \\to \\cdots \\to v_* \\xrightarrow{I} \\pi_{*}, $$ where the policy evaluation steps are given by $$ v_{k+1}(s) = \\max_{a} \\sum_{s',r} p(s',r|s,a)(r +\\gamma v_k(s')). $$Note that the value iteration requires the maximum to be taken over all actions.\n4. Convergence of Policy Evaluation In the policy evaluation step, we use the iterative relationship $$ v_{k+1}(s):= \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a) (r + \\gamma v_k(s')). $$But how do we know the policy evaluation converges to $v_\\pi$? First we define the Bellman expectation backup operator $T_\\pi$ by $$ T_\\pi(v) = R_\\pi + \\gamma P_\\pi v. $$ We say that the operator $T_\\pi$ is a $\\gamma$-contraction, i.e. it makes value function closer by at least $\\gamma$, $$ \\begin{align*} ||T_\\pi(u) - T_\\pi(v)||_\\infty \u0026= || \\gamma P_\\pi(u -v)||_\\infty \\cr \u0026\\leq \\gamma ||u - v||_\\infty. \\end{align*} $$ Theorem 3.2 (contraction mapping theorem) For any metric space $\\mathcal{V}$ that is complete under an operator $T(v)$, if $T$ is a $\\gamma$-contraction, then $T$ converges to a unique fixed point in $\\mathcal{V}$ at a linear rate of $\\gamma$. By the contraction mapping theorem, the Bellman expectation operator $T_\\pi$ has a unique fixed point. Furthermore, by the bellman equation, $v_\\pi$ is a fixed of $T_\\pi$. Hence the iterative policy evaluation converges to $v_\\pi$ and then policy iteration converges to $v_*$.\nFor the value iteration, we use the iterative relationship $$ v_{k+1}(s) = \\max_{a} \\sum_{s',r} p(s',r|s,a)(r +\\gamma v_k(s')). $$ We can define the Bellman optimality backup operator $T_*$ by $$ T_*(v) = \\max_a( r(\\cdot, a) + \\gamma P_{*,a} v), $$ which is also a $\\gamma$-contraction. It follows that the value iteration converges to the unique fix point $v_*$.\n","permalink":"http://localhost:1313/posts/rl/rl_03_dynamic_programming/","summary":"\u003cp\u003eThere are two common tasks in an MDP\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eprediction: estimating the valued function $\\pi_\\pi$ given a MDP and a policy $\\pi$.\u003c/li\u003e\n\u003cli\u003econtrol: finding optimal policy $\\pi_*$ and corresponding optimal value function $v_*$ given a MDP.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn this chapter we will show how to use DP to solve the prediction and control problems in an MDP. DP is of limited utility in RL both because of their assumption of a perfect model (environment\u0026rsquo;s dynamics are completely known) and because of their great computational expense, but they are important theoretically.\u003c/p\u003e","title":"RL | Dynamic Programming"},{"content":"Notations $\\mathcal{S}$: state space. $\\mathcal{A}$: action space. $\\mathcal{R}$: reward space. 1. Agent-Environment Interaction On each step, the agent selects an action $A_t\\in\\mathcal{A}$ based on the state $S_t$, and then the environment reveals a reward $R_{t+1}$ and moves to a new state $S_{t+1}$ corresponding the selected action $A_t$ and state $S_t$.\nWrite $$ p(s',r|s,a) = \\Pr(S_t = s', R_t = r| S_{t-1} = s, A_{t-1} = a). $$ The function $p$ defines the dynamics of the MDP. Given the four-argument function $p$, one can compute anything else one might want tot know about the environment, such as\nstate-transition probabilities. $$ p(s'|s,a) = \\sum_{r\\in\\mathcal{R}} p(s',r|s,a). $$ expected rewards given $(s,a)$. $$ r(s,a) = \\mathbb{E}[R_t|S_{t-1}=s ,A_{t-1} =a] = \\sum_{r} r \\sum_{s'} p(s',r|s,a). $$ expected rewards given $(s,a,s')$. $$ r(s,a,s') = \\mathbb{E}[R_t|S_{t-1}=s ,A_{t-1} =a, S_t = s'] = \\sum_{r} r \\frac{p(s',r|s,a)}{p(s'|s,a)}. $$ The objective is to maximize the expected return, denoted $G_t$, that measures the cumulative reward in the long run. The simplest definition is given by $$ G_t = R_{t+1} + R_{t+2} + \\cdots $$Given a discount factor $\\gamma\\in[0,1]$, we can define the discounted return by $$ G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots = \\sum_{k=1}^\\infty \\gamma^k R_{t+k+1} = R_{t+1} + \\gamma G_{t+1}. $$2. Policies and Value Functions Definition 2.1 (policy) A policy is a mapping $\\pi:\\mathcal{S}\\to \\mu(\\mathcal{A})$ where $\\mu$ is a probability measure. We use the notion $\\pi(a|s)$ to denote the probability that $A_t =a $ given the state $S_t = s$. Definition 2.2 (value function) The value function of a state $s$ under a policy $\\pi$, denoted $v_\\pi(s)$, is the expected return when starting in state $s$ and following the policy $\\pi$. For MDPs, we can define $v_\\pi(s)$ by $$ v_\\pi(s) = \\mathbb{E}_{\\pi}[G_t|S_t = s] = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}| S_t = s \\right]. $$ Definition 2.3 (action-value function) The action-value function of taking action $a$ in state $s$ under a policy $\\pi$, denoted $q_\\pi(s,a)$, is defined as the expected return starting from state $s$, taking the action $a$ and following the policy $\\pi$, i.e. $$ \\begin{align*} q_\\pi(s,a) \u0026= \\mathbb{E}_{\\pi}[G_t|S_t = s, A_t = a] \\cr \u0026= \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}| S_t = s, A_t = a \\right] \\cr \u0026= r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) \\sum_{a'} \\pi(a'|s') q_\\pi(s',a'). \\end{align*} $$ A fundamental property of value functions is that they satisfy recursive relationships $$ \\begin{align} v_\\pi(s) \u0026= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1}|S_t = s] \\cr \u0026= \\sum_a \\pi(a|s) \\sum_{s'}\\sum_{r} p(s',r|s,a) \\left( r + \\gamma \\mathbb{E}_\\pi[G_{t+1}|S_{t+1} = s'] \\right) \\cr \u0026= \\sum_a \\pi(a|s) \\left( r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) v_\\pi(s') \\right). \\end{align} $$Equation (3) is the Bellman equation for $v_\\pi$. It expresses a relationship between the value of a state and the values of its successor states.\n3. Optimal Policies and Optimal Value Functions We would like to find a policy that achieves highest expected return. We say a policy $\\pi$ is better than policy $\\pi'$ if $v_\\pi(s) \\geq v_{\\pi'}(s)$ for all $s\\in\\mathcal{S}$. There is always at least one optimal policy, denoted $\\pi_*$. Then we can define\noptimal value function. $$ v_*(s):= \\max_{\\pi}v_\\pi(s). $$ optimal action-value function. $$ q_*(s,a) = \\max_\\pi q_\\pi(s,a). $$ If there are more than one optimal policies, they share the same value functions and action-value functions.\nThe Bellman equation (2) for the optimal value function $v_*$ can be written in a specific form. $$ \\begin{align} v_*(s) \u0026= \\max_{a} q_{\\pi_*}(s,a) \\cr \u0026= \\max_a \\mathbb{E}_{\\pi_*}[R_{t+1}+ \\gamma G_{t+1}|S_t = s, A_t = a] \\cr \u0026= \\max_a \\mathbb{E}[R_{t+1}+ \\gamma v_*(S_{t+1})|S_t = s, A_t = a] \\cr \u0026= \\max_a \\sum_{s'}\\sum_r p(s',r|s,a)[r+ \\gamma v_*(s')] \\cr \u0026= \\max_a r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) v_*(s') \\end{align} $$The Bellman equation for optimal action-value function $q_*$ is $$ \\begin{align} q_*(s,a) \u0026= \\mathbb{E}[R_{t+1}+ \\gamma v_*(S_{t+1})|S_t = s, A_t = a] \\cr \u0026= \\mathbb{E}[R_{t+1}+ \\gamma \\max_{a'} q_*(S_{t+1},a')|S_t = s, A_t = a] \\cr \u0026=\\sum_{s'}\\sum_{r} p(s',r|s,a) [r + \\gamma \\max_{a'} q_*(s',a')] \\cr \u0026= r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) v_*(s'). \\end{align} $$","permalink":"http://localhost:1313/posts/rl/rl_02_markov_decision_process/","summary":"\u003ch2 id=\"notations\"\u003eNotations\u003c/h2\u003e\n\u003chr\u003e\n\u003cul\u003e\n\u003cli\u003e$\\mathcal{S}$: state space.\u003c/li\u003e\n\u003cli\u003e$\\mathcal{A}$: action space.\u003c/li\u003e\n\u003cli\u003e$\\mathcal{R}$: reward space.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-agent-environment-interaction\"\u003e1. Agent-Environment Interaction\u003c/h2\u003e\n\u003cp\u003eOn each step, the agent selects an action $A_t\\in\\mathcal{A}$ based on the state $S_t$, and then the environment reveals a reward $R_{t+1}$ and moves to a new state $S_{t+1}$ corresponding the selected action $A_t$ and state $S_t$.\u003c/p\u003e\n\u003cp\u003eWrite\n\u003c/p\u003e\n$$ \n p(s',r|s,a) = \\Pr(S_t = s', R_t = r| S_{t-1} = s, A_{t-1} = a).\n$$\u003cp\u003e\nThe function $p$ defines the \u003cem\u003edynamics\u003c/em\u003e of the MDP. Given the four-argument function $p$, one can compute anything else one might want tot know about the environment, such as\u003c/p\u003e","title":"RL | Markov Decision Processes"},{"content":"Notations $A_t$: the action selected at time $t$.\n$R_t$: the reward received at time $t$.\n$q_*(a)$: the expected reward given an action $a$, i.e. $q_*(a) = \\mathbb{E}[R_t|A_t = a]$.\n$Q_t(a)$: the estimated value of action $a$ at time $t$.\n1. A k-armed Bandit Problem Consider a problem that we have k different actions, and each choice leads to a reward with a certain probability distribution depending on the action selected. Our objective is to maximize the expected total reward.\nHowever, the value $q_*(a)$ is usually unknown, we need to estimate it by $Q_t(a)$, and we would like $Q_t(a)$ to be close to $q_*(a)$.\n1.1 Action-value methods One natural way to estimate this is by averaging the rewards received $$ Q_t(a) = \\frac{\\sum_{i=1}^{t-1} R_i 1_{A_i = a}}{\\sum_{i=1}^{t-1} 1_{A_i = a}} $$Then the simplest action selection rule is $A_t = \\arg\\max_{a} Q_t(a)$, which is also known as greedy method. Greedy action selection always exploits current knowledge, and never tries inferior actions. An alternative method is called $\\epsilon$-greedy that select random actions with some probability.\n1.2 Incremental methods The action-value methods save all action rewards. A more efficient way is $$ \\begin{align*} Q_{n+1} \u0026= \\frac{1}{n}\\sum_{i=1}^n R_i \\cr \u0026= \\frac{1}{n}(R_n + \\sum_{i=1}^{n-1} R_i) \\cr \u0026= \\frac{1}{n}(R_n + (n-1)Q_{n}) \\cr \u0026= Q_{n} + \\frac{1}{n}(R_n - Q_n) \\end{align*} $$ where the notation for action $a$ is omitted for simplicity.\nIn some cases, the reward probabilities are nonstationary, i.e. the reward probabilities change over time. In such cases, we give more weight to recent rewards. We introduce a discounting factor $\\alpha\\in(0,1)$. We can update the values by $$ Q_{n+1} = Q_n + \\alpha (R_n - Q_n), $$ which results in a weighted average of past rwards, $$ \\begin{align*} Q_{n+1} \u0026= Q_n + \\alpha (R_n - Q_n) \\cr \u0026= (1-\\alpha)^n Q_1 + \\alpha \\sum_{i=1}^n (1-\\alpha)^{n-i} R_i. \\end{align*} $$2. UCB Action Selection The $\\epsilon$-greedy method try non-greedy actions with equal probability. It would be better to select among the non-greedy actions according to their likelihood of being optimal. The UCB method selects action by $$ A_t = \\arg\\max_a \\bigg[Q_t(a) + c \\sqrt{\\frac{\\log t}{N_t(a)}}\\bigg], $$ where $N_t(a)$ is the number of times that action $a$ has been selected until time $t$.\n3. Gradient Algorithms So far we have considered methods that estimate action values and use them to select actions. Now we introduce a method that learns the preference for each action $a$, denoted by $H_t(a)$. Then we select actions by $$ \\Pr(A_t = a) = \\frac{\\exp(H_t(a))}{\\sum_{b=1}^k \\exp(H_t(b))} := \\pi_t(a). $$ We would like to generate a policy $\\pi$ with high performance, measured in terms of expected reward, so we can update $H_t(a)$ by the gradient ascent method, $$ \\begin{equation} H_{t+1}(a) = H_t(a) + \\alpha \\frac{\\partial \\mathbb{E}[R_t]}{\\partial H_t(a)}, \\end{equation} $$ where $$ \\mathbb{E}[R_t] = \\sum_{x} \\pi_t(x) q_*(x). $$ Of course, it is not possible to implement the algorithm exactly since $q_*(x)$ are unknown. Take a closer look at the gradient $$ \\begin{align*} \\frac{\\partial \\mathbb{E}[R_t]}{\\partial H_t(a)} \u0026= \\frac{\\partial }{\\partial H_t(a)} \\left[ \\sum_{x} \\pi_t(x) q_*(x) \\right] \\cr \u0026= \\sum_x q_*(x) \\frac{\\partial }{\\partial H_t(a)} \\pi_t(x) \\cr \u0026= \\sum_x (q_*(x) - B_t)\\frac{\\partial }{\\partial H_t(a)} \\pi_t(x), \\end{align*} $$ where $B_t$, called the baseline, does not depend on $x$. The last equality holds because the gradient sums to zero over all actions, i.e. $\\sum_x \\frac{\\partial }{\\partial H_t(a)} \\pi_t(x) = 0$. Next, we rewrite it in the expectation form $$ \\frac{\\partial \\mathbb{E}[R_t]}{\\partial H_t(a)} = \\sum_x \\pi_t(x)(q_*(x) - B_t)\\frac{\\partial \\pi_t(x)}{\\partial H_t(a)} /\\pi_t(x). $$ It can be represented as $$ \\mathbb{E}\\left[ (q_*(A_t) - B_t) \\frac{\\partial \\pi_t(A_t)}{\\partial H_t(a)} /\\pi_t(A_t) \\right] \\approx \\mathbb{E}\\left[ (R_t - \\bar{R}_t) \\frac{\\partial \\pi_t(A_t)}{\\partial H_t(a)} /\\pi_t(A_t) \\right], $$ where $\\bar{R}_t$ is the average of all rewards up to but not including time $t$. Note that the $\\pi_t$ is in the form of soft-max function, so the partial derivative $\\frac{\\partial \\pi_t(x)}{\\partial H_t(a)}$ can be easily computed $$ \\frac{\\partial \\pi_t(x)}{\\partial H_t(a)} = \\pi_t(x) (1_{a=x} - \\pi_t(a)). $$ Combining all above, we have $$ \\frac{\\partial \\mathbb{E}[R_t]}{\\partial H_t(a)} = \\mathbb{E}\\left[ (R_t - \\bar{R}_t) (1_{a=A_t} - \\pi_t(a)) \\right]. $$ Using a sample of the expectation to estimate the expectation yields $$ H_{t+1}(a) = H_t(a) + \\alpha (R_t - \\bar{R}_t)(1_{a=A_t} - \\pi_t(a)). $$","permalink":"http://localhost:1313/posts/rl/rl_01_multi_armed_bandits/","summary":"\u003ch2 id=\"notations\"\u003eNotations\u003c/h2\u003e\n\u003chr\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e$A_t$: the action selected at time $t$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e$R_t$: the reward received at time $t$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e$q_*(a)$: the expected reward given an action $a$, i.e. $q_*(a) = \\mathbb{E}[R_t|A_t = a]$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e$Q_t(a)$: the estimated value of action $a$ at time $t$.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-a-k-armed-bandit-problem\"\u003e1. A k-armed Bandit Problem\u003c/h2\u003e\n\u003cp\u003eConsider a problem that we have k different actions, and each choice leads to a reward with a certain probability distribution depending on the action selected. Our objective is to maximize the expected total reward.\u003c/p\u003e","title":"RL | Multi-armed Bandits"},{"content":" Roy, A. (n.d.). Optimal Bayesian Smoothing of Functional Observations over a Large Graph. 16. ","permalink":"http://localhost:1313/posts/reading/gp_graph_3/","summary":"\u003cul\u003e\n\u003cli\u003eRoy, A. (n.d.). Optimal Bayesian Smoothing of Functional Observations over a Large Graph. 16.\u003c/li\u003e\n\u003c/ul\u003e","title":"Reading | Estimating a smooth function on a large graph (3)"},{"content":"This paper follows from the one discussed in the previous article and shows that the introduced estimator achieves the optimal rate.\nKirichenko, A., \u0026amp; van Zanten, H. (2018). Minimax lower bounds for function estimation on graphs. ArXiv:1709.06360 [Math, Stat]. http://arxiv.org/abs/1709.06360 1. Main results Theorem 1 (Regression) Under conditions (G), (L) and (S), $$ \\inf_{\\hat{f}} \\sup_{f\\in H^\\beta(Q)} \\mathbb{E}_f || \\hat{f} - f||_n^2 \\asymp n^{-2\\beta/(2\\beta+r)}. $$ where the infimum is taken over all estimators $\\hat{f} = \\hat{f}(Y_1,...,Y_n)$. This theorem shows that the minimax rate for the regression problem on the graph is equal to $n^{-\\beta/(2\\beta+r)}$.\nTheorem 2 (Classification) Under conditions (G), (L) and (S), suppose that the Laplacian eigenfunctions are uniformly bounded by a constant $C$, independent of $n$. Let $\\Psi:\\mathbb{R}\\to (0,1)$ be a differentiable link function with bounded derivative. Then for $\\beta\\geq r/2$ and $Q\u003e0$ $$ \\inf_{\\hat{\\rho}} \\sup_{\\rho_f: f\\in H^\\beta(Q)}\\mathbb{E}_{f} || \\hat{\\rho} - \\rho_f||_n^2 \\asymp n^{-2\\beta/(2\\beta+r)}, $$ where the infimum is taken over all estimators $\\hat{\\rho} = \\hat{\\rho}(Y^{(n)})$. Compared to the regression case, there is an extra requirement $\\beta\\geq r/2$.\n2. Preliminaries In the regression case, we assume that\n$$ Y_i = f_i + \\sigma\\xi_i, $$ where $\\xi_i$ are independent standard Gaussians. Let $\\psi_i$ be the orthonormal (normalized) eigenfunctions of the Laplacian matrix s.t. $$ \\psi_i^T \\psi_i = n. $$ Write $Y = (Y_1,...,Y_n)$, and $\\xi = (\\xi_1,...,\\xi_n)$. Denote $$ \\epsilon_i = \\langle \\xi, \\psi_i \\rangle_n, $$ and observe that the $h_i$ are centered Gaussian with $$ \\mathbb{E} \\epsilon_i \\epsilon_j = \\frac{1}{n} \\delta_{ij}. $$ The inner product $Z_i := _n$ satisfy $$ Z_i = \\langle Y,\\psi_i\\rangle_n = h_i + \\delta \\epsilon_i, $$ where $h_i$ are coefficients in the series representation of the target function $f^* = \\sum_{i=1}^n h_i \\psi_i$. Additionally, consider the decomposition of an estimator $\\hat{f} = \\sum_{i=1}^n \\hat{h}_i \\psi_i$. Then $$ ||\\hat{f} - f^*||_n^2 = \\sum_{i=1}^n(h_i - \\hat{h}_i)^2. $$Hence the original problem is converted into the problem of recovering coefficients $h_i$, given the observations $$ \\begin{equation} Z_i = h_i + \\sigma \\epsilon_i = h_i + e \\xi_i, \\end{equation} $$ where $e = \\sigma/\\sqrt{n}$ and $\\xi_i$ are independent standard Gaussians. The Sobolev-type ball $H^\\beta(Q)$ can be represented in terms of coefficients $h_i$, which is given by $$ B_n(Q) = \\{ h\\in\\mathbb{R}^n: \\sum_{i=1}^n a_i^2 h_i^2 \\leq Q^2 \\}, $$ where $a_i^2 = 1 + \\lambda_i^{\\beta} n^{2\\beta/r}$.\nWe introduce Pinsker\u0026rsquo;s estimator and recall the linear minimax lemma showing that Pinsker\u0026rsquo;s estimator is optimal in the class of linear estimators. The risk of a linear estimator $\\hat{f}(l) = (l_1 Z_1,...,l_n Z_n)$ with $l = (l_1,...,l_n)\\in\\mathbb{R}^n$ is given by $$ R(l,f) = \\mathbb{E}_f \\sum_{i=1}^n(\\hat{h}_i - h_i)^2 = \\sum_{i=1}^n( (1-l_i)^2 h_i^2 + e^2 l_i^2). $$ The Pinsker estimator for $B_n(Q)$ is $\\hat{f}(l')$ for $l' = (1+ x a_j)_+$, where $x$ is the unique solution of the followsing equation $$ \\begin{equation} \\frac{e^2}{x} \\sum_{i=1}^n a_j(1 - x a_j)_+ = Q^2. \\end{equation} $$ Lemma 1 (Linear minimax lemma) Suppose that the general ellipsoid $B_n(Q)$ has positive coefficients $a_i$. Suppose there exists a unique solution $x$ of the equation (2) and suppose that the associated coefficients $l_i'$ satisfy $$ \\begin{equation} S = e^2 \\sum_{i=1}^n l_i'\u003c\\infty. \\end{equation} $$ Then the linear minimax risk satisfies $$ \\inf_{l\\in\\mathbb{R}^n}\\sup_{f\\in B_n(Q)} R(l,f) = \\sup_{f\\in B_n(Q)} R(l',f) = S. $$ It shows that the Pinsker estimator is the linear minimax estimator.\nLemma 2 With $B_n(Q)$ and $a_i^2 = 1 + \\lambda_i^\\beta n^{2\\beta/r}$, we have The unique solution of the equation (2) satisfies $$ x\\asymp n^{-\\beta/(2\\beta + r)}. $$ The associated Pinsker\u0026rsquo;s coefficients $l'_i$ satisfy $$ S\\asymp n^{-2\\beta/(2\\beta + r)}. $$ For $e = \\sigma/\\sqrt{n}$ define $v_i = \\frac{e^2 (1+x a_j)_+}{x a_j}$. Then $$ \\max_{i\\leq n} v_i^2 a_j^2 = O(n^{-r/(2\\beta +r)}). $$ 3. Proofs of main results Proof of Theorem 1 First, we show the upper bound of the risk. Consider the Pinsker estimator $\\hat{f} = (l_1' Z_1,...,l_n' Z_n)$ with $$ l_i' = (1 - xa_j)_+. $$ By Lemma 1 and (ii) of Lemma 2, we conclude that $$ \\sup_{f\\in B_n(Q)} \\mathbb{E}_f\\sum_{i}(\\hat{h}_i - h_i)^2 \\asymp n^{-2\\beta/(2\\beta +r)}. $$ Then we show the lower bound of the risk. Define $N$ by $$ N = \\max\\{m: e^2 \\sum_{i=1}^n a_i(a_m - a_j)\u003c Q^2 \\}. $$ Define $$ B_n(Q,N) = \\{h^{(N)} = (h_1,...,h_N, 0 ,...,0): \\sum_{i=1}^N a_i^2 h_i^2 \\leq Q^2 \\}. $$ Denote the minimax risk by $R_n$, i.e. $$ R_n = \\inf_{\\hat{f}} \\sup_{f\\in H^\\beta(Q)} \\mathbb{E}_f \\sum_{i=1}^n(\\hat{h}_i - h_i)^2, $$ then we have $B_n(Q,N)\\subset B_n(Q)$ and hence $$ R_n \\geq \\inf_{\\hat{f}^{(N)} \\in B_n(Q,N)} \\sup_{f^{(N)}\\in B_n(Q,N)} \\mathbb{E}_f \\sum_{i=1}^n(\\hat{h}_i - h_i)^2. $$ Then we can bound the minimax risk from below by the Bayes risk $$ \\begin{equation} R_n \\geq \\inf_{\\hat{f}^{(N)} \\in B_n(Q,N)} \\sum_{i=1}^N \\int_{B_n(Q,N)} \\mathbb{E}_f (\\hat{h}_i - h_i)^2 \\mu(f^{(N)}) d f^{(N)} \\geq I^* - r^*, \\end{equation} $$ where $$ \\begin{align*} I^* \u0026= \\inf_{\\hat{f}^{(N)} \\in B_n(Q,N)} \\sum_{i=1}^N \\int_{\\mathbb{R}^N} \\mathbb{E}_f (\\hat{h}_i - h_i)^2 \\mu(f^{(N)}) d f^{(N)} \\cr r^* \u0026= \\sup_{\\hat{f}^{(N)} \\in B_n(Q,N)} \\sum_{i=1}^N \\int_{\\mathbb{R}^N \\setminus B_n(Q,N)} \\mathbb{E}_f (\\hat{h}_i - h_i)^2 \\mu(f^{(N)}) d f^{(N)}. \\end{align*} $$ From the proof of Pinsker\u0026rsquo;s theorem, we get that $$ I^* \\gtrsim S, \\quad r^* \\lesssim \\exp(-K(\\max_{i\\leq n} v_j^2 a_j^2)^{-1}). $$ By Lemma 2, we conclude that $R_n\\gtrsim n^{-2\\beta/(2\\beta+r)}$. $\\Box$\nProof of Theorem 2 Consider the estimator $\\hat{\\rho} = \\Psi(\\sum_{i=1}^n \\hat{h}_i \\psi_i)$. The upper bound can be easily proved by the properties of the link function $\\Psi$, $$ \\sup_{\\rho:\\Psi^{-1}(\\rho)\\in H^\\beta(Q)} \\mathbb{E}_\\rho||\\hat{\\rho} - \\rho||_n^2 \\lesssim \\sup_{f\\in B_n(Q)} \\mathbb{E}_f \\sum_{i=1}^n (\\hat{h}_i - h_i)^2 \\lesssim n^{-2\\beta/(2\\beta + r)}. $$ To show the lower bound of the risk, we use the Markov\u0026rsquo;s inequality to obtain $$ \\inf \\sup \\mathbb{E}_\\rho n^{2\\beta/(2\\beta+r)} ||\\hat{\\rho} -\\rho||_n^2 \\gtrsim \\inf \\sup \\Pr(||\\hat{\\rho} -\\rho||_n^2 \\geq n^{-2\\beta/(2\\beta+r)}). $$ Conside probability measures $P_1,...,P_M$ corresponding to the soft label function $\\rho_1,...,\\rho_M$. For a test $\\phi:\\mathbb{R}^n\\to\\{1,...,M\\}$ define the average probability of error by $$ \\bar{\\rho}_M(\\phi) = \\frac{1}{M} \\sum_{i=1}^M P_i(\\phi = i). $$ Additionally, let $$ \\bar{\\rho}_M = \\inf_\\phi \\bar{\\rho}_M(\\phi). $$ If we have $$ \\begin{equation} ||\\rho_i - \\rho_j||_n^2 \\gtrsim n^{-2\\beta/(2\\beta+r)} ,\\quad i\\neq j,\\quad i,j\\leq M, \\end{equation} $$ then $$ \\inf_{\\hat{\\rho}} \\max_{\\rho \\in \\{\\rho_1,...,\\rho_M\\}} \\Pr_\\rho(||\\hat{\\rho} - \\rho||_n^2 \\geq C c^{-2\\beta/(2\\beta+r)}) \\gtrsim \\bar{\\rho}_M. $$ In view of the fact that if $P_1,...,P_M$ satisfy $$ \\begin{equation} \\frac{1}{M} \\sum_{i=2}^M KL(P_j||P_1) \\leq \\alpha \\log M, \\end{equation} $$ for some $\\alpha\\in(0,1)$, then $$ \\bar{\\rho}_M \\geq \\frac{\\log M - \\log 2}{\\log (M-1)} - \\alpha. $$ It follows that $$ \\sup_{\\rho:\\Psi^{-1}(\\rho)\\in H^\\beta(Q)} \\mathbb{E}_\\rho n^{2\\beta/(2\\beta+r)} ||\\hat{\\rho} -\\rho||_n^2 \\gtrsim \\frac{\\log M - \\log 2}{\\log (M-1)} - \\alpha \\to 0, $$ as $M\\to\\infty$. This is the desired result. Now we construct the required measures $P_1,...,P_M$ satisfying (5)-(6). Let $N = n^{r/(2\\beta +r)}$. For $\\delta\u003e0$ and $\\theta = (\\theta_1,...,\\theta_N)\\in \\{\\pm 1\\}^N$ define $$ f_\\theta = \\delta N^{-(2\\beta + r)/(2r)} \\sum_{i=1}^N \\theta_i \\psi_i. $$ We try to select M vectors of coefficients $\\theta$ s.t. the measures corresponding to $\\rho_i = \\Psi(f_{\\theta,i})$ satisfy (6). Notice that $f_\\theta \\in H^\\beta(Q)$, and by the assumption (G), we have $$ \\langle f_\\theta, (I + (n^{2/r} L)^\\beta) f_\\theta\\rangle_n = \\delta^2 N^{-(2\\beta + r)/r} \\sum_{i=1}^N (1 + \\lambda_i^{\\beta} n^{2\\beta/r} ) \\leq K_1 \\delta^2 $$ for some constant $K_1\u003e0$. Then we pick a subset $\\{ \\theta^{(2)},...,\\theta^{(M)} \\}$ of $\\{\\pm 1\\}^N$ s.t. $$ d_h(\\theta^{(i)}, \\theta^{(j)}) \\gtrsim N, $$ where $d_h$ is the Hamming distance, i.e. $$ d_h(\\theta, \\theta') = \\sum_{i=1}^N 1_{\\theta_i = \\theta'_i}. $$ It can be shown that such subset exists, and the size $M$ satisfies $$ M \\geq b^N, \\quad b\\in(1,2). $$ Set $\\theta^{(1)} = (0,...,0)\\in\\mathbb{R}^N$. We define the measures $P_0,...,P_M$ by $P_i = P_{\\rho_i}$, where $\\rho_i = \\Psi(f_{\\theta,i})$. Since $KL(P_{\\Psi(v)}||P_{\\Psi(w)}) \\leq cn||v - w||_n^2$, we obtain that $$ KL(P_i||P_1) \\leq K_2 n || f_{\\theta^{(i)}} - 0||_n^2 \\leq 4K_2 n \\delta^2 N^{-2\\beta/r}, $$ according to the fact that $||f_\\theta - f_{\\theta'}||_n^2 = 4\\delta^2 N^{-(2\\beta+r)/r} d_h(\\theta, \\theta')$. It gives the condition (6). Finally, we show the condition (5). By the assumption of the theorem we have for any $j\\leq M$ $$ \\max_{i\\leq n} |f_{\\theta^{(j)},i}| \\lesssim N^{(r-2\\beta)/2r}. $$ For $\\beta\\geq r/2$ the norm is then bounded by some constant. Hence, there esists $K_3$ s.t. for every $i\\leq n$ and every $j\\leq M$ $$ |f_{\\theta^{(j)},i}| \\leq K_3. $$ Since $|\\Psi(x) - \\Psi(y)| \\geq K_4|x- y|$ for any $x,y \\in[-K_3,K_3]$, for any $i,j\\leq M, i\\neq j$ $$ ||\\rho_i - \\rho_j||_n^2 \\gtrsim ||f_{\\theta^{(i)}} - f_{\\theta^{(j)}}||_n^2 = 4\\delta^2 N^{-(2\\beta+r)/r} d_h(\\theta, \\theta') \\gtrsim N^{_-2\\beta/r}. $$ This completes the proof since $N = n^{r/(2\\beta +r)}$. ","permalink":"http://localhost:1313/posts/reading/gp_graph_2/","summary":"\u003cp\u003eThis paper follows from the one discussed in the previous article and shows that the introduced estimator achieves the optimal rate.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eKirichenko, A., \u0026amp; van Zanten, H. (2018). Minimax lower bounds for function estimation on graphs. ArXiv:1709.06360 [Math, Stat]. \u003ca href=\"http://arxiv.org/abs/1709.06360\"\u003ehttp://arxiv.org/abs/1709.06360\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"1-main-results\"\u003e1. Main results\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eTheorem 1 (\u003cem\u003eRegression\u003c/em\u003e)\u003c/dt\u003e\n\u003cdd\u003eUnder conditions (G), (L) and (S),\n$$ \n \\inf_{\\hat{f}} \\sup_{f\\in H^\\beta(Q)} \\mathbb{E}_f || \\hat{f} - f||_n^2 \\asymp n^{-2\\beta/(2\\beta+r)}.\n$$\nwhere the infimum is taken over all estimators $\\hat{f} = \\hat{f}(Y_1,...,Y_n)$.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003cp\u003eThis theorem shows that the minimax rate for the regression problem on the graph is equal to $n^{-\\beta/(2\\beta+r)}$.\u003c/p\u003e","title":"Reading | Estimating a smooth function on a large graph (2)"},{"content":"This paper proposed an approach to estimating smooth functions on graphs using GP priors.\nKirichenko, A., \u0026amp; van Zanten, H. (2015). Estimating a smooth function on a large graph by Bayesian Laplacian regularisation. ArXiv:1511.02515 [Math, Stat]. http://arxiv.org/abs/1511.02515 1. Problem setup Let $G = (V,E)$ be a connected undirected graph and $A$ its adjacency matrix, $D$ its degree matrix. Then $L = D-A$ is the Laplacian of the graph. Suppose that there is a function $f:[0,1]\\to \\mathbb{R}$ on the graph. We are interested in the $n$-dimentional vector $f = (f_1,...,f_n)^T$, where $f_i = f(i/n)$. We measure distances and norms of functions using the norm $||\\cdot||_n$ defined by $$ ||f||_n^2 = n^{-1}\\sum_{i=1}^n f_i^2. $$ The corresponding inner product of two funtions $f,g$ is denoted by $$ \\langle f,g \\rangle_n = \\frac{1}{n}\\sum_{i=1}^n f_i g_i. $$Assumptions (G). Let $\\lambda_i$ be the (ordered) eigenvalues of the Laplacian matrix. For some $r\\geq 1$, there exist $i_0\\in\\mathbb{N}, \\kappa\\in(0,1]$ and $C_1,C_2\u003e0$ s.t. for all $n$ large enough, $$ \\begin{equation} C_1(\\frac{i}{n})^{2/r} \\leq \\lambda_i \\leq C_2(\\frac{i}{n})^{2/r} \\end{equation} $$ for all $i\\in\\{i_0,...,\\kappa n\\}$\n(L). The graph $G$ is connected, simple, undirected.\n(S). We assume $f$ belongs to a Sobolev-type ball of the form\n$$ H^\\beta(Q) = \\{f: \\langle f, (I + (n^{2/r} L)^\\beta) f\\rangle_n \\leq Q^2\\} $$ for some $\\beta,Q\u003e0$. The parameter $\\beta$ determines the smoothness of the function.\n2. Main results In the regression case we assume that we have observations $Y = (Y_1,...,Y_n)$ at the vertices of the graph satisfying $$ Y_i = f_i^* + \\sigma \\xi_i, $$ where $\\xi_i$ are independent standard Gaussian. By Theorem 1, we have $$ \\begin{equation} \\Pi(||f - f^*||_n \\geq M \\epsilon_n| Y^{(n)}) \\to 0 \\end{equation} $$ in $P_{f^*}$-probability at the rate $\\epsilon_n\\asymp n^{-\\beta/(2\\beta+r)}$ for $\\beta\\leq \\alpha+r/2$.\nIn the binary classification case, we assume that $Y_1,...,Y_n$ are 0-1 valued variables satisfying $$ p^*_i = \\Pr(Y_i = 1). $$ We introduce a link function $\\Psi:\\mathbb{R}\\to (0,1)$ and set $p = \\Psi(f)$. We assume that the link function is differentiable with bounded derivative $\\phi$. Then the Theorem also leads to posterior contraction rates $\\epsilon_n\\asymp n^{-\\beta/(2\\beta+r)}$ for $\\beta\\leq \\alpha+r/2$ by the following fact $$ || p_v - p_w||_r \\lesssim ||v - w||_{r,G}. $$ Theorem 1 (Power of the Laplacian) Under the assumptions (G), (L) and (S). Let $\\alpha\u003e0$ be fixed and assume that $f^*\\in H^\\beta(Q)$ be the \u0026ldquo;true\u0026rdquo; function and $0\u003c \\beta\\leq \\alpha+r/2$. Let the random function $f$ on the graph be defined by $$ \\begin{align} c \u0026\\sim \\exp(1) \\cr f|c \u0026\\sim N(0, \\Sigma_{c}), \\end{align} $$ where $\\Sigma_c^{-1} = (\\frac{n}{c})^{2/r}(L + n^{-2} I)^{\\alpha+r/2}$. Then there exists constants $K_1\u003e0,K_2\u003e1$ and Borel measurable subsets $B_n\\subset \\mathbb{R}^n$ s.t. $$ \\begin{align} \\Pr(||f - f^*||\u003c\\epsilon_n) \u0026\\gtrsim \\exp(-K_1 n\\epsilon_n^2), \\cr \\Pr(f\\notin B_n) \u0026\\lesssim \\exp(-K_2 n \\epsilon_n^2), \\cr \\log N(\\epsilon_n , B_n , ||\\cdot||_n) \u0026\\lesssim n\\epsilon_n^2, \\end{align} $$ where $\\epsilon_n\\asymp n^{-\\beta/(2\\beta+r)}$. This theorem obtains the rate $n^{-\\beta/(2\\beta+r)}$ for all $\\beta$ up to $\\alpha+r/2$.\nTheorem 2 (Exponential of the Laplacian) Under the assumptions (G), (L) and (S). Let $\\alpha\u003e0$ be fixed and assume that $f^*\\in H^\\beta(Q)$ be the \u0026ldquo;true\u0026rdquo; function and $\\beta\u003e0$. Let the random function $f$ on the graph be defined by $$ \\begin{align} c \u0026\\sim \\exp(1) \\cr f|c \u0026\\sim N(0, \\Sigma_{c}), \\end{align} $$ where $\\Sigma_c = n \\exp(-(n/c)^{2/r}L)$. Then there exists constants $K_1\u003e0,K_2\u003e1$ and Borel measurable subsets $B_n\\subset \\mathbb{R}^n$ s.t. $$ \\begin{align} \\Pr(||f - f^*||\u003c\\epsilon_n) \u0026\\gtrsim \\exp(-K_1 n\\epsilon_n^2), \\cr \\Pr(f\\notin B_n) \u0026\\lesssim \\exp(-K_2 n \\epsilon_n^2), \\cr \\log N(\\tilde{\\epsilon}_n , B_n , ||\\cdot||_n) \u0026\\lesssim n\\tilde{\\epsilon}_n^2, \\end{align} $$ where $\\epsilon_n\\asymp (n/\\log^{1+r/2}n)^{-\\beta/(2\\beta+r)}$ and $\\tilde{\\epsilon}_n \\asymp \\epsilon_n\\log^{1/2+r/4}n$. 3. Proofs of main results Given $c$, the random vector $f$ is an element in the GP process $GP(0,\\Sigma_c)$. The corresponding RKHS $\\mathbb{H}^c$ is the entire space $\\mathbb{R}^n$, and the corresponding RKHS-norm is given by $$ ||h||_{\\mathbb{H}^c}^2 = h^T\\Sigma_c^{-1}h. $$Let $\\psi_i$ be the engenfunctions of $L$, normalized w.r.t. the norm $||\\cdot||_n$. In particular, the eigenfunctions $\\psi_i$ are normalized s.t.\n$$ \\psi_i^T \\psi_i = n. $$They are also eigenfunctions of $\\Sigma_c^{-1}$. We denote the corresponding engenvalues by $\\mu_i$. The Gaussian $N(0,\\Sigma_c)$ admits the series representation $$ \\sum_i Z_i \\psi_i/\\sqrt{n\\mu_i}, $$ where $Z_i$ are standard Gaussian. In particular the functions $\\psi_i/\\sqrt{n\\mu_i}$ form an orthonormal basis of the RKHS $\\mathbb{H}^c$. Hence, the ordinary $||\\cdot||_n$-norm and the RKHS-norm of a function $h$ with expansion $h = \\sum_i h_i \\psi_i$ are given by $$ \\begin{equation} ||h||_n^2 = \\sum_i h_i^2,\\quad ||h||_{\\mathbb{H}^c}^2 = n\\sum_i \\mu_i h_i^2. \\end{equation} $$Proof of Theorem 1 Lemma 1 (small ball) For $c^{-p/r}\\epsilon\\sqrt{n}$ small enough, $$ -\\log \\Pr(||f||_n \\leq \\epsilon|c) \\lesssim (\\frac{c^{(\\alpha+4/2)/r}}{\\epsilon\\sqrt{n}})^{r/\\alpha}. $$ Proof By the series representation of $f$, we have $$ \\Pr(||f||_n\\leq \\epsilon|c) = \\Pr(\\sum_i Z_i^2/(n\\mu_i)\\leq \\epsilon^2). $$ The assumption (G) allows us to bound $\\mu_i$ from below, then it follows that $$ \\begin{align*} \\Pr(||f||_n\\leq 2\\epsilon^2|c) \u0026\\geq \\Pr(\\sum_{i\\leq i_0}Z_i^2/(n\\mu_i)\\leq \\epsilon^2, \\sum_{i\u003ei_0}Z_i^2/(n\\mu_i)\\leq \\epsilon^2) \\cr \u0026\\geq \\Pr(\\sum_{i\\leq i_0} Z_i^2 \\leq (C_1^p c^{-2p/4} n^{(2\\alpha+2r - 2pr)/4})\\epsilon^2)\\Pr(\\sum_{i\u003ei_0}\\frac{Z_i^2}{e^{2p/r}}\\leq C_1^p c^{-2p/r}n\\epsilon^2), \\end{align*} $$ where we write $p+r/2$. The second factor can be bounded from below by $$ \\exp(-const(c^{p/r}\\epsilon\\sqrt{n})^{-r/\\alpha}), $$ provided $c^{p/r}\\epsilon\\sqrt{n}$ is small. By the triangle inequality and independence, the first factor is bounded from below by $$ \\left(\\Pr(|Z_1|\\leq i_0^{1/2} C_1^{p/2} c^{-p/r} n^{(\\alpha+r-pr)/r}\\epsilon)\\right)^{i_0}. $$ Since $r\\geq 1$, we have $c^{-p/r} n^{(\\alpha+r-pr)/r}\\epsilon = O(c^{-p/r} \\epsilon\\sqrt{n})$. Hence, for $c^{-p/r}\\epsilon\\sqrt{n}$ small, the probability is bounded from below by $$ const\\times(c^{-p/r} n^{(\\alpha+r-pr)/r}\\epsilon)^{i_0} $$ Combining the bounds for the two factors, we have $$ -\\log \\Pr(||f||_n^2 \\leq 2\\epsilon^2|c) \\lesssim (\\frac{c^{p/r}}{\\epsilon\\sqrt{n}})^{r/\\alpha} + \\log (\\frac{c^{p/r}}{n^{(\\alpha+r-pr)/r}\\epsilon}) $$ Lemma 2 (discentering) Let $f\\in H^\\beta(Q)$ for $\\beta\\leq \\alpha +r/2$. For $\\epsilon$ s.t. $\\epsilon\\to 0$ and $1/\\epsilon = o(n^{\\beta/r})$, $$ \\inf_{||h-f||_n\\leq \\epsilon} ||h||_{\\mathbb{H}^c}^2\\lesssim nc^{-(2\\alpha+r)/r} \\epsilon^{-\\frac{2(\\alpha-\\beta) +r}{\\beta}} $$ Proof Use the series expansion of $f = \\sum_i h_i \\psi_i$. By the assumption (S) we have $\\sum_i (1 + n^{2\\beta/r}\\lambda_i^\\beta) h_i^2 \\leq Q^2$. Now for $I$ to be determined later, consider $h = \\sum_{i\\leq I} h_i\\psi_i$. By assumption (G), for $I$ large enough $$ ||h - f||_n^2 = \\sum_{i\u003eI}h_i^2\\leq \\frac{Q^2}{1 + n^{2\\beta/r}\\lambda_I^\\beta} \\leq Q^2 C_1^{-\\beta}I^{-2\\beta/r}. $$ Putting $I\\propto \\epsilon^{-r/\\beta}$ we obtain $||h-f||_n \\leq \\epsilon$. Then the RKHS-norm of $h$ satisfies $$ \\begin{align*} ||h||_{\\mathbb{H}^c}^2 \u0026= n\\sum_{i\\leq I}\\left((n/c)^{2/r} (\\lambda_i + n^{-2}) \\right)^{\\alpha+r/2} h_i^2 \\cr \u0026\\lesssim nc^{-2p/r}Q^2 + c^{-2p/r}Q^2 n^{2+2(\\alpha-\\beta)/r}\\lambda_I^{p-\\beta} \\end{align*} $$ Then by the assumption (G), we obtain the disired result. Lemma 3 (entropy) $$ \\log N(\\epsilon, \\mathbb{H}_{1,c}, ||\\cdot||_n) \\lesssim c (\\frac{1}{\\epsilon\\sqrt{n}})^{r/(\\alpha+r/2)}. $$ Proof of Theorem 1 Given $c$, the eigenvalues of $\\Sigma_c^{-1}$ are given by $$ \\mu_i = \\left( (\\frac{n}{c})^{2/r} (\\lambda_i + \\frac{1}{n^2})\\right)^{\\alpha+r/2}. $$ For the first assertion. By the fact that $$ \\phi_w(\\epsilon) \\leq -\\log \\Pr(||W-w||\u003c \\epsilon) \\leq \\phi_w(\\epsilon/2) $$ and the Lemma 1 and Lemma 2, for $\\epsilon = \\epsilon_n = n^{-\\beta/(r+2\\beta)}$ and $c = c_n$ s.t. $$ \\sqrt{n} \\epsilon_n^{(\\beta-\\alpha)/\\beta}\\leq c_n^{(\\alpha+r/2)/r}\\leq 2\\sqrt{n} \\epsilon_n^{(\\beta-\\alpha)/\\beta}, $$ we have $-\\log\\Pr(||f- f^* ||_n\u003c\\epsilon_n|c)\\lesssim \\epsilon_n^{-r\\beta}$. By direct computation, we obtain $$ \\Pr(||f-f^*||_n\u003c\\epsilon_n) \\gtrsim e^{-K_1 \\epsilon_n^{-r/\\beta}}. $$ Then we show the left two assertions. Define $B_n = M_n\\mathbb{H}_{1,c_n} + \\epsilon_n\\mathbb{B}_1$, where $\\mathbb{B}_1$ is the unit ball of $(\\mathbb{R}^2,||\\cdot||_n)$. For $\\epsilon_n = n^{-\\beta/(2\\beta+r)}$ and $c_n,M_n$ to be determined later, by Lemma 3, we have $$ \\log N(2\\epsilon_n, B_n, ||\\cdot||_n) \\leq \\log N(\\epsilon_n, M_n\\mathbb{H}_{1,c_n}, ||\\cdot||_n) \\lesssim c_n (\\frac{M_n}{\\epsilon_n\\sqrt{n}})^{r/p}. $$ Choosing $M_n = M\\sqrt{n\\epsilon_n^2}, c_n^{p/r} = N \\sqrt{n} \\epsilon_n^{(\\beta-\\alpha)/\\beta}$, we obtain the third assertion. It remains to show the second assertion. $$ \\Pr(f\\notin B_n) \\leq \\int_0^{c_n} \\Pr(f\\notin B_n|c) e^{-c} dc + \\int_{c_n}^\\infty e^{-c} dc. $$ For $c\\leq c_n$, we have $\\mathbb{H}_{1,c}\\subset \\mathbb{H}_{1,c_n}$. Hence, by the Borell\u0026rsquo;s\u0026rsquo; inequality, we have for $c\\leq c_n$ that $$ \\begin{align*} \\Pr(f\\notin B_n|c) \u0026\\leq \\Pr(f\\notin M_n \\mathbb{H}_{1,c} + \\epsilon_n \\mathbb{B}_1|c) \\cr \u0026\\leq 1 - \\Phi(\\Phi^{-1}(\\Pr(||f||_n \\leq \\epsilon_n|c) + M_n)) \\cr \u0026\\leq 1 - \\Phi(\\Phi^{-1}(\\Pr(||f||_n \\leq \\epsilon_n|c_n) + M_n)) \\end{align*} $$ By Lemma 1 with $c_n^{p/r} = N\\sqrt{n}\\epsilon_n^{(\\beta-\\alpha)-\\beta}$, the small ball probability in this expression is bounded from below by $\\exp(-K \\epsilon_n^{-r/\\beta})$. Then with appropriate choice $M_n$, we obtain the desired result.\n","permalink":"http://localhost:1313/posts/reading/gp_graph_1/","summary":"\u003cp\u003eThis paper proposed an approach to estimating smooth functions on graphs using GP priors.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eKirichenko, A., \u0026amp; van Zanten, H. (2015). Estimating a smooth function on a large graph by Bayesian Laplacian regularisation. ArXiv:1511.02515 [Math, Stat]. \u003ca href=\"http://arxiv.org/abs/1511.02515\"\u003ehttp://arxiv.org/abs/1511.02515\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"1-problem-setup\"\u003e1. Problem setup\u003c/h2\u003e\n\u003cp\u003eLet $G = (V,E)$ be a connected undirected graph and $A$ its adjacency matrix, $D$ its degree matrix. Then $L = D-A$ is the Laplacian of the graph. Suppose that there is a function $f:[0,1]\\to \\mathbb{R}$ on the graph. We are interested in the $n$-dimentional vector $f = (f_1,...,f_n)^T$, where $f_i = f(i/n)$. We measure distances and norms of functions using the norm $||\\cdot||_n$ defined by\n\u003c/p\u003e","title":"Reading | Estimating a smooth function on a large graph (1)"},{"content":"Traditionally, we reduce the multiclass classification problem to a binary problem by 1-vs-1 or 1-vs-rest. This article proposed an alternative method to solve the multiclass classification problem.\nLi, X., \u0026amp; Ghosal, S. (2018). Bayesian Classification of Multiclass Functional Data. ArXiv:1808.00662 [Stat]. http://arxiv.org/abs/1808.00662\n1. Problem setup Consider a response $Y$ taking values $k = 1,...,K$, with functional covariate $(X(t), t\\in[0,1])$. The main problem is to estimate the probability $P(Y = k|X)$, which can be modeld by $$ P(Y= k|X) = H_k(\\int \\beta(t)X(t) dt), $$ where $H_K$ is a cdf, and $\\beta(\\cdot)$ is unknown.\n2. Model Let $X_i(t)$, $i=1,...,n$, be the observed functional data associated with a categotical variable $Y_i$. The probability of choosing category $k = 1,...,K-1$ is given by $$ \\begin{equation} P(Y_i = k|X_i) = \\frac{\\exp(\\int \\beta_k(t) X_i(t) dt)}{1 + \\sum_{l=1}^{K-1} \\exp(\\int \\beta_l(t)X_i(t) dt)}, \\end{equation} $$ and the probability of choosing category $K$ is given by $$ \\begin{equation} P(Y_i = K|X_i) = \\frac{1}{1 + \\sum_{l=1}^{K-1} \\exp(\\int \\beta_l(t)X_i(t) dt)}. \\end{equation} $$The functional coefficient $\\beta(t)$ is given a prior which is a finite linear combination of a certain basis functions: $$ \\beta(t) = \\sum_{j=1}^J \\theta_j \\psi_j(t), $$ where $(\\psi_j)$ is a basis. A prior is put on the unknown coefficinets $(\\theta_j)$. The number of basis function $J$ is also unknown and is given a hyperprior.\nLet $\\beta_k(t)= \\sum_{j=1}^J \\theta_{k,j}\\psi_j(t)$ and $Z_{ij} = \\int \\psi_j(t)X_i(t) dt$. Then the classification probabilty can be written as, for $k = 1,...,K_1$ $$ \\begin{align*} P(Y_i = k|X_i) \u0026= \\frac{\\exp(\\sum_{j=1}^J \\theta_{k,j}Z_{ij})}{1 + \\sum_{l=1}^{K-1}\\exp(\\sum_{j=1}^J \\theta_{l,j}Z_{ij})} \\cr \u0026= \\frac{\\exp(Z_i^T \\theta_k)}{1 + \\sum_{l=1}^{K-1}\\exp(Z_i^T \\theta_l)}, \\end{align*} $$ and $$ P(Y_i = K|X_i) = \\frac{1}{1 + \\sum_{l=1}^{K-1}\\exp(Z_i^T \\theta_l)}, $$ where $\\theta_k = (\\theta_{k,q},...,\\theta_{k,J})^T, Z_i = (Z_{i,1},...,Z_{i,J})^T$.\nFor each $\\theta_k, k=1,...,K$, we assign a Gaussian prior $N_J(\\mu_k,\\Sigma_k)$.\n3. MCMC Metropolis-Hastings algorithm to sample $\\theta_k$.\nSample $\\theta_k'$ from the proposal distribution $q(\\theta_k',\\theta_k|Y,\\theta_{-1})$. Accept $\\theta_k'$ w.p. $\\alpha(\\theta_k,\\theta_k'|Y,\\theta_{-k})$ given by $$ \\begin{align*} \u0026\\frac{\\pi(\\theta_k'|Y,\\theta_{-k})q(\\theta_k',\\theta_k|Y,\\theta_{-k})}{\\pi(\\theta_k|Y,\\theta_{-k})q(\\theta_k,\\theta_k'|Y,\\theta_{-k})}\\wedge 1 \\cr =\u0026 \\frac{\\pi(\\theta_k',\\theta_{-k}) f(Y|\\theta_k',\\theta_{-k})q(\\theta_k',\\theta_k|Y,\\theta_{-k})}{\\pi(\\theta_k,\\theta_{-k}) f(Y|\\theta_k,\\theta_{-k})q(\\theta_k,\\theta_k'|Y,\\theta_{-k})}\\wedge 1 \\end{align*} $$ Moreover, we can put a prior on $J$, for example, geometric or Possion distribution.\n4. Posterior contraction rate Write $\\pi = (\\pi_1,...,\\pi_K)$, where $\\pi_k = P(Y=k|X)$, and let $\\pi_{0k}$ be the true probability of the k-th category conditioned on $X$. Assume that the joint distribution of $(X,Y)$ follows $\\nu\\times G$, where $\\nu$ is the counting measure on $\\{1,...,K\\}$.\nDefine\nnorm $||f||_{p,G} = (\\int |f|^p d G)^{1/p}$. $KL(p||q)$ the KL-divergence, $H^2(p,q)$ the squared Helinger distance, and $$ V(p||q) = \\int p \\log^2 p/q d\\mu. $$ Then we have $$ \\begin{align*} KL(p_0||p) \u0026= \\int\\int p_0(x,y) \\log \\frac{p_0(x,y)}{p(x,y)} d\\nu(y) dx \\cr \u0026= \\int\\int \\pi_0(y|x) \\log \\frac{\\pi_0(y|x)}{\\pi(y|x)} d\\nu(y) d G(x) \\cr \u0026= \\mathbb{E}_X\\left[\\sum_{k=1}^K \\pi_{0,k}(X) \\log \\frac{\\pi_{0k}(X)}{\\pi_k(X)}\\right] \\cr \u0026= KL(\\pi_0||\\pi) \\end{align*} $$ Similarly, $V(p_0||p) = V(\\pi_0||\\pi)$ and $H^2(p_0,p) = H^2(\\pi_0,\\pi)$. We define a metric $d$ by $$ d(\\pi,\\pi_0) = \\sqrt{\\sum_{k=1}^K \\mathbb{E}_X |\\pi_k(X) - \\pi_{0k}(X)|^2}. $$Then we have the following theorem\nTheorem 1 Assume that $\\pi_0$ is bounded, $X$ is bounded. Let $\\epsilon_n \\geq \\epsilon_n'$ be two sequences of positive numbers satisfying $\\epsilon_n\\to 0, n\\epsilon_n'\\to\\infty$. Let $\\Theta_n$ be a subset of the parameter space s.t. $$ \\begin{align} \\log N(\\epsilon_n,\\Theta_n , H) \u0026\\lesssim n\\epsilon_n^2 \\cr \\Pi(\\Theta\\setminus \\Theta_n) \u0026\\leq \\exp(-a_1 n \\epsilon_n'^2) \\cr \\Pi(\\pi: \\sum_{k=1}^K ||\\pi_k - \\pi_{0k}||_{\\infty}^2 \u003c\\epsilon_n'^2) \u0026\\geq \\exp(-a_2n\\epsilon_n'2). \\end{align} $$ Then for every $M_n\\to\\infty$, we have $\\Pi(d(\\pi,\\pi_0)\\geq M_n \\epsilon_n| X^{(n)}, Y^{(n)}) \\to 0 $ in probability. Proof $H^2 \\geq \\mathbb{E}_X \\sum_{k=1}^K |\\pi_k(X) - \\pi_{0k}(X)|^2$ and by Taylor\u0026rsquo;s expansion, $$ K_2 \\leq \\sum_{k=1}^K ||\\pi_k - \\pi_{0k}||_\\infty^2. $$ Then the result follows from the general posterior contraction theorem. Assumption (A1) $\\Pi(J= j) \\leq \\exp(-c_2 j \\log^{t_2} j)$. (A2) Given $J$, $\\Pi(||\\theta- \\theta_0||\u003c \\epsilon) \\geq \\exp(-c_3 J\\log (1/\\epsilon))$ for every $||\\theta_0||\\leq M_1$. Also assume that $\\Pi(\\theta\\notin [-M_2, M_2]^J) \\leq J\\exp(-CM^{t_3})$. Theorem 2 Assume that $||X||_1$ is a bounded random variable, the priors satisfy assumptions (A1) and (A2), and that the basis satisfies $$ ||\\beta_0 - \\theta_0^T \\psi||_\\infty \\lesssim J^{-\\alpha}, $$ and $$ ||\\theta_1^T\\phi - \\theta_2^T \\psi||_\\infty \\lesssim J^{K_0} ||\\theta_1 - \\theta_2||_2. $$ The the posterior contracts at $\\pi_0$ at rate $\\epsilon_n \\asymp n^{-\\alpha/(2\\alpha+1)} (\\log n)^{\\alpha/(2\\alpha+1) + (1-t_2)/2}$ relative to $d(\\pi,\\pi_0)$. Proof Becaue $||X||_1$ is bounded, there is some $M$ s.t. $||X||_1 \\leq M$. By the logistic model $$ |\\pi_k - \\pi_{0 k}| \\lesssim ||\\beta_k - \\beta_{0k}||_\\infty. $$ The $L^\\infty$ distance is bounded by $$ ||\\beta_k - \\beta_{0k}||_\\infty \\leq ||\\theta_k^T \\psi - \\theta_{0k}\\psi||_\\infty + ||\\theta_{0k}^T\\psi - \\beta_{0k}||_\\infty. $$ Then we have $$ \\begin{equation} \\Pi(\\sum_k^K||\\pi_k - \\pi_{0k}||_\\infty^2 \\leq \\epsilon_n'^2) \\gtrsim \\exp(-J_n \\log (2\\sqrt{K} J_n^{K_0} /\\epsilon_n')) \\end{equation} $$ Choose appropriate $J_n$ to satisfy the conditions of Theorem 1, and therefore obtain the result. ","permalink":"http://localhost:1313/posts/reading/gp_multiclass_classification/","summary":"\u003cp\u003eTraditionally, we reduce the multiclass classification problem to a binary problem by 1-vs-1 or 1-vs-rest. This article proposed an alternative method to solve the multiclass classification problem.\u003c/p\u003e\n\u003cp\u003eLi, X., \u0026amp; Ghosal, S. (2018). Bayesian Classification of Multiclass Functional Data. ArXiv:1808.00662 [Stat]. \u003ca href=\"http://arxiv.org/abs/1808.00662\"\u003ehttp://arxiv.org/abs/1808.00662\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"1-problem-setup\"\u003e1. Problem setup\u003c/h2\u003e\n\u003cp\u003eConsider a response $Y$ taking values $k = 1,...,K$, with functional covariate $(X(t), t\\in[0,1])$. The main problem is to estimate the probability $P(Y = k|X)$, which can be modeld by\n\u003c/p\u003e","title":"Reading | Bayesian Classification of Multiclass Functional Data"},{"content":" Ghosal, S., \u0026amp; Roy, A. (2006). Posterior consistency of Gaussian process prior for nonparametric binary regression. The Annals of Statistics, 34(5). https://doi.org/10.1214/009053606000000795 The previous article discussed the methodology of GP priors for binary classification. This article would show the consistency results.\n1. Problem setup $$ p(x) = P(Y=1|x) = H(\\eta(x)). $$ A GP prior with mean function $\\mu(x)$ and covariance kernle $\\sigma(x,x')$ is put on the function $\\eta$. The covariance kernel is assumed to be of the form $$ \\sigma(x,x') = \\tau^{-1}\\sigma_0(\\lambda x, \\lambda x'), $$ where $\\sigma_0$ is a nonsingular covariance kernel and the hyper-parameters $\\tau\u003e0,\\lambda\u003e0$ play the roles of a scaling parameter and a bandwidth parameter, respectively. Let $\\tau\\sim \\Pi_\\tau, \\lambda\\sim \\Pi_\\lambda$, respectively.\nA popular method of prior construction on function is by expanding the function in a series $\\sum_j \\theta_j \\psi_j(x)$ and then putting independent $N(0,\\tau_j^2)$ priors on the coefficients. Such a prior leads to a GP prior on the function with covariance kernel $\\sigma(x,y) = \\sum_j \\tau_j^2 \\psi_j(x) \\psi_j(y)$.\n2. Main results Let $D^w \\eta = \\frac{\\partial \\eta(t_1,...,t_d)}{\\partial t_1 \\cdots \\partial t_d}$, $|w| = \\sum w_j$. Let sequences $\\lambda_n ,\\tau_n$ be s.t. $$ \\Pi_\\tau(\\tau\u003c\\tau_n) = e^{-cn} ,\\quad \\Pi_\\lambda(\\lambda\u003e\\lambda_n) = e^{-cn}, $$ for some constant $c$.\nDefine $$ \\Theta_n = \\Theta_{n,\\alpha} = \\{ p: P(x) = H(\\eta(x)), || D^2\\eta||_\\infty \u003c M_n, |w|\\leq \\alpha\\}. $$Assumptions\n(P). For every fixed $x$, the covariance function $\\sigma_0(x,\\cdot)$ has continuous partial derivatives up to order $2\\alpha+2$, where $\\alpha\u003e0$ is to be specified later.\n(C). The covariate space $\\mathcal{X}$ is a bounded subset of $\\mathbb{R}^d$.\n(T). The transformed true response function $\\eta_0 \\in \\bar{\\mathbb{H}}$, where $\\mathbb{H}$ is the RKHS of GP.\n(G). For every $b_1\u003e0, b_2\u003e0$, there exist sequences $M_n,\\tau_n, \\lambda_n$ s.t. $$ M_n^2 \\tau_n \\lambda_n^{-2} \\geq b_1 n , \\quad M_n^{d/\\alpha} \\leq b_2 n. $$ Theorem 1 Suppse $X$ is random covariate sampled from a prob dist $Q$. Supppse that assumptions (P), (C), (T) and (G) hold. Then for any $\\epsilon\u003e0$, $$ \\Pi(p: ||p - p_0||_{1,Q} \u003e\\epsilon| X^{(n)}, Y^{(n)}) \\to 0 $$ in $P_0^n$-probability. Theorem 2 Let $(x_i)$ be fixed design points, and $Q_n$ be empirical measure of the design points. Then under assumptions (P), (C), (T) and (G), for any $\\epsilon\u003e0$, $$ \\Pi(p: ||p - p_0||_{1,Q_n} \u003e\\epsilon| Y^{(n)}) \\to 0 $$ in $P_0^n$-probability. Let $\\mathcal{X}$ be one-dimensional. Let $S_{i,n} = X_{i+1,n} - x_{i,n}$ be the spacings between consecutive covariate values.\nAssumption\n(U). Given $\\delta\u003e0$, there exist a constant $K_1$ and an integer $N$ s.t., for $n\u003eN$, we have that $\\sum_{i:S_{i,n}\u003eK_1 n^{-1}} S_{i,n}\\leq \\delta$. This assumption means that the measure of the part of the design space where data is parse is small.\nTheorem 3 Suppose that the values of the covariate arise as design points satisfying assumption (U) and $\\mathcal{X}$ is a bounded interval of $\\mathbb{R}$. Assume that the prior satisfies assumption (P). The mapping $x\\mapsto \\eta_0(x)$ and the prior mean $\\mu$ are assumed to have two continuous derivatives and the covariance kernel is assumed to have continous partial derivatives up to order 6. Assume that $\\Pi_\\lambda ,\\Pi_\\tau$ are such that $\\tau_n^{-1} \\lambda_n^4 = O(n)$. Then for any $\\epsilon\u003e0$, $$ \\Pi(p: ||p - p_0||_{1} \u003e\\epsilon| Y^{(n)}) \\to 0 $$ in $P_0^n$-probability. Theorem 4 Assume that (W(t)) is a GP with continuous sample paths having mean function $\\mu$ and continuous covariance kernel $\\sigma(s,t)$. Assume that $\\mu$ and a function $w$ belong to the RKHS of the kernel $\\sigma(s,t)$. Then $$ P(\\sup_{t\\in T} |W(t) - w(t)| \u003c \\epsilon) \u003e0 \\quad \\text{for all } \\epsilon\u003e0. $$ proof W.l.o.g. we assume that $\\mu$ is the zero function. Let $\\sum_i \\sqrt{\\lambda_i} \\xi_i \\psi_i(t)$ be the Karhunen-Loeve expansion of $W(t)$, where $\\lambda_i's$ are the eigenvalues of the kernel $\\sigma(s,t)$, $\\psi_i$ are the corresponding eigenfunctions and $\\xi_i$ are independent $N(0,1)$. Represent $w(t)$ also as $\\sum_i \\sqrt{\\lambda_i} a_i \\psi_i(t)$, where $\\sum_ki \\lambda_i a_i^2 \u003c\\infty$. It follows from Mercer\u0026rsquo;s theorem that the series $\\sum_i \\sqrt{\\lambda_i} a_i \\psi_i(t)$ converges uniformly, and hence, the tail sum is uniformly small. Theorem 5 : Let $\\eta$ be a GP on $\\mathcal{X}$, a bouned subset of $\\mathbb{R}^d$. Assume that the mean function $\\mu \\in C^\\alpha(\\mathcal{X})$ and the covariance kernel $\\sigma$ has $2\\alpha+2$ mixed partial derivatives for some $\\alpha\\geq 1$. Then $\\eta$ has differentiable sample paths with mixed partial derivatives up to order $\\alpha$ and the successive derivative process $D^w \\eta$ are also Gaussian with continuous sample paths. Also the derivative processes are sub-Gaussian w.r.t. a constant multiple of the Euclidean distance. Further, there exits a constant $d_w$ s.t. $$ P(\\sup_x |D^2 \\eta (x)|\u003e M)\\leq K(\\eta)e^{-d_w M^2/\\sigma_w^2(\\eta)} $$ for $w = (w_1,...,w_d)$, $w_i\\in\\{0,1,2,...,\\alpha\\}$, $|w| \\leq \\alpha$ and $\\sigma_w^2(\\eta) = \\sup_x var(D^w\\eta(x)) \u003c\\infty$, $K(\\eta)$ is a polynomial in the supremum of the $(2\\alpha+2)$-order derivatives of $\\sigma$ and the covariance functions of the derivative processes $D^w\\eta(x)$ are functions of the derivatives of the covariance kernel $\\sigma$.\nproof . 3. Necessary lemmas Without proof we state some lemmas that would be used in the proof of main theorems.\nLemma 1 (prior mass) Under assumptions (P), (C) and (G), $\\Pi(\\Theta_n^\\complement)\\leq Ae^{-cn}$ for some constants $A$ and $c$. Lemma 2 (entropy bound) $\\log N(\\epsilon, \\Theta_n, ||\\cdot||_\\infty)\\leq KM_n^{d/\\alpha} \\epsilon^{-d/\\alpha}$ for some constant $K$. Lemma 3 Let $\\nu$ be a finite meaasure on $\\mathcal{X}$ and let $\\psi_1,\\psi_2$ be measurable functions s.t. $0\\leq \\psi_1,\\psi_2\\leq M$ and $\\int |\\psi_1 - \\psi_2| d\\nu \u003e (1 + \\nu(\\mathcal{X})\\epsilon$ for some $M,\\epsilon\u003e0$. Then $$ \\nu(x: |\\psi_1(x) - \\psi_2(x)|\u003e\\epsilon) \\geq \\epsilon/M. $$ Let $\\psi_1 = p, \\psi_2 = p_0$ and $\\nu = Q_n$, we see $$ \\begin{equation} I_{n,p} = \\#\\{x_i: |p(x_i) - p_0(x_i)|\u003e\\epsilon\\} \\geq K'n \\end{equation} $$ for some $K'$. $I_{n,p}$ contains those points s.t. either $p(x) \u003c p_0(x) - \\epsilon$ or $p(x) \u003e p_0(x) + \\epsilon$, and at least one case contains more than $K'n/2$ points. So we would not lose order of the number of indices to work with one case only.\nLemma 4 (tests) Let $Y_j$ be independent Bernoulli variable with $P(Y_j = 0) = \\mu_j$. Consider testing $H_0: u = u_0$ against $H_0: u \u003e u_0 + \\epsilon$. Then there exist tests $\\phi_n$ s.t. $$ P_0(\\phi_n) \\leq e^{-m\\epsilon^2/2}, \\quad P_1(1-\\phi_n)\\leq e^{-m\\epsilon^2/2}. $$ It can be extend to any $p_1 \\in \\Theta_n$. Consider the case $x_i$ s.t. $p(x_i) \u003e p_0(x_i) + \\epsilon$ and $||p^* - p||\u003c\\epsilon/2$, then $$ p^*(x_i) - p_0(x_i) \\geq p(x_i) - p_0(x_i) - ||p - p^*||_\\infty \u003e\\epsilon /2. $$With $\\epsilon$ replaced by $\\epsilon/2$, Lemma 4 implies the existence of tests $\\phi_{n,p}$ s.t. $$ P_0(\\phi_{n,p}) \\leq e^{-m\\epsilon^2/8}, \\quad P^*(1-\\phi_{n,p})\\leq e^{-m\\epsilon^2/8}. $$ Let $p_1,...,p_N \\in\\Theta_n$ be the maximal $\\epsilon/2$-separated points. Consider $\\phi_n = \\max_{j\\leq N} \\phi_{n,p_j}$, then $$ P_0\\phi_n \\leq \\sum_j^N P_0(\\phi_{n,p_{j}}) \\leq N e^{-m\\epsilon^2/8} = exp(\\log N - m\\epsilon^2/8). $$ For any $p\\in\\Theta_n$, we can find $p_j\\in\\Theta_n$ s.t. $||p - p_j||_\\infty \u003c \\epsilon/2$. Then $$ P(1-\\phi_n) \\leq P(1 - \\phi_{n,p_j}) \\leq e^{-m\\epsilon^2/8} $$With $M_n$ satisfying assumption (G), we have $\\log N \\leq b_2' n$, then $$ P_0 \\phi_n\\leq e^{-c'n}, \\quad \\sup_{||p- p_0|| \u003e \\epsilon/2,p\\in\\Theta_n} P(1-\\phi_n) \\leq e^{-c'n} $$ for some constant $c'$.\nLemma 5 (taylor\u0026rsquo;s expansion) Let $0\u003c \\epsilon_0 \u003c 1/2$ and $\\epsilon_0 \u003c \\alpha,b \u003c 1-\\epsilon_0$. Then there exists a constant $L_{\\epsilon_0}$ s.t. $$ \\alpha(\\log \\alpha/\\beta)^m + (1-\\alpha)(\\log \\frac{1-\\alpha}{1-\\beta}) \\leq L(\\alpha-\\beta)^2. $$ 4. Proof of the main theorems Proof of Thm 1 $Y_i|X_i \\sim Bin(1, p(X_i)), X_i \\sim Q$ i.i.d. then the joint density of $(X,Y)$ is given by $$ f(x,y) = p(x)^y(1-p(x))^{1-y}. $$ The corresponding true joint density is $f_0(x,y) = p_0(x)^y (1-p_0(x))^{1-y}$. Assumption (T) implies that $p_0$ is bounded s.t. $\\epsilon_0 \u003c p_0(x) \u003c 1-\\epsilon_0$ for some $\\epsilon_0\u003c1/2$. Thus $f_0(x,y) \u003e\\epsilon_0$. Note that $||f_1 - f_2||_{1} = 2||p_1 - p_2||_{1,Q} $ and $$ KL(f_0||f) = KL(p_0||p) + \\int(1-p_0)\\log \\frac{1-p_0}{1-p} dQ. $$ First, we show that $f_0\\in KL(\\Pi)$, or equivalently, $$ \\Pi(p: KL(p_0||p) + \\int(1-p_0)\\log \\frac{1-p_0}{1-p} dQ \u003c\\epsilon) \u003e0, $$ for any $\\epsilon\u003e0$. Lemma 5 implies that it suffices to show that $$ \\Pi(p:||p-p_0||_\\infty \u003c\\epsilon)\u003e0. $$ Because $p_0 = H(\\eta)$ and the link funciton $H$ is bounded and Lipschitz continuous, thus it is enough to show that $$ \\Pi(\\eta: ||\\eta- \\eta_0||_\\infty\u003c\\epsilon)\u003e0. $$ This is an immediate result of Theorem 4. Then we check the entropy condition. Consider the set $F_n = \\{f: p\\in\\Theta_n\\}$. Lemma 2, with $M_n = bn^{\\alpha/d}$, gives $$ \\log N(\\epsilon, F_n, ||\\cdot||_\\infty)\\leq K \\epsilon^{-d/\\alpha}b^{d/\\alpha}n. $$ Chose $b\u003c(\\beta/K)^{\\alpha/d}\\epsilon$, we have that $$ \\log N(\\epsilon, F_n, ||\\cdot||_\\infty)\\leq n\\beta. $$ Finally, we check the prior mass, which is given by Lemma 1.\nProof of Thm 2 Same. 5. Discussion This paper didn\u0026rsquo;t study the contraction rate. A more general result with contraction rates was studied in\nvan der Vaart, A. W., \u0026amp; van Zanten, J. H. (2008). Rates of contraction of posterior distributions based on Gaussian process priors. The Annals of Statistics, 36(3), 1435–1463. https://doi.org/10/ctnd8h ","permalink":"http://localhost:1313/posts/reading/gpconsistency_for_binary_classification/","summary":"\u003cul\u003e\n\u003cli\u003eGhosal, S., \u0026amp; Roy, A. (2006). Posterior consistency of Gaussian process prior for nonparametric binary regression. The Annals of Statistics, 34(5). \u003ca href=\"https://doi.org/10.1214/009053606000000795\"\u003ehttps://doi.org/10.1214/009053606000000795\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe previous article discussed the methodology of GP priors for binary classification. This article would show the consistency results.\u003c/p\u003e\n\u003ch2 id=\"1-problem-setup\"\u003e1. Problem setup\u003c/h2\u003e\n$$ \n p(x) = P(Y=1|x) = H(\\eta(x)).\n$$\u003cp\u003e\nA GP prior with mean function $\\mu(x)$ and covariance kernle $\\sigma(x,x')$ is put on the function $\\eta$. The covariance kernel is assumed to be of the form\n\u003c/p\u003e","title":"Reading | Posterior consistency of Gaussian process prior for nonparametric binary regression"},{"content":"Let\u0026rsquo;s discuss this paper\nChoudhuri, N., Ghosal, S., \u0026amp; Roy, A. (2007). Nonparametric binary regression using a Gaussian process prior. Statistical Methodology, 17. 1. Problem Setup Consider a binary classification probem given by $$ p(x) = P(Y = 1|X=x) = 1- P(Y = -1|X = x). $$ This problem commonly occurs in many fields of application, such as medical and spatial statics. Traditionally, we would model this problem as $$ p(x) = H(\\eta(x)), $$ where $H$ is commonly choosen as a cdf, called the link function, and $\\eta$ can be choosen parametrically or nonparametrically. In this paper, a nonparametric approach was studied, where $H$ was known, and the function $\\eta$ was estimated with a Gaussian prior.\nThere are two functional hyper-parameters in the GP prior. The mean function $\\mu(x)$ and the covariance kernel $r(x,y)$. The most important parameter is the covariance kernel that controls the local smoothness of the sample paths of $\\eta(x)$ and thus determines the extent of strength borrowed from neighbors in obtaining the posterior process.\n2. MCMC algorithm for posterior The posterior of $\\eta$ is not analytically tractable, and thus an MCMC procedure will be used to compute the posterior.\n2.1 Probit link Consider $H = \\Phi$. Suppoe that $\\eta\\sim GP(\\mu,\\Sigma)$. Let $Z = \\eta(x)|\\eta$ be latent variables, then $Z \\sim N(\\eta(x), I)$. Assume that the observations $Y_i's$ are functions of these latent variables defined as $Y_i = I(Z_i\\geq 0)$. Then $$ Y_i|\\eta \\sim Bernoulli(H(\\eta(x_i))). $$ The procedure can be summarized as:\n$\\eta\\sim GP(\\mu,\\Sigma)$. $Z|\\eta \\sim N(\\eta(x), I)$. $Y_i|Z_i \\sim Bernoulli(H(Z_i))$. The posterior of $\\eta|Z$ could be computed analytically by the conjugacy in the Gaussian observation and the GP prior.\n$$ \\eta|Z,Y = \\eta|Z \\sim N(\\mu^*, \\Sigma^*) $$where $\\Sigma^* = (I + \\Sigma^{-1})^{-1}, \\mu^* = \\Sigma^* (Z + \\Sigma^{-1}\\mu) = \\Sigma^* (Z - \\mu) + \\mu$. Since $Z$ is unobservable, we shall sampe from the joint posterior distribution of $(Z,\\eta)$ via a Gibbs sampler. The value of $Y_i$ indicates the sign of $Z_i$, and thus $$ Z_i|\\eta, Y \\sim \\begin{cases} N(\\eta(x_i), 1) | Z_i\\geq 0, \u0026 \\; Y_i = 1, \\cr N(\\eta(x_i), 1) | Z_i\u003c 0, \u0026\\; Y_i = -1. \\end{cases} $$ Since $\\Sigma$ can be near-singular if points $x_i$ are close to each other, the computation of $\\Sigma^{-1}$ should be avioded. A spectral decomposition of the form $\\Sigma = P \\Lambda P^T$ will be helpful. Then $$ \\Sigma^* = P(\\Lambda + I)^{-1}\\Lambda P^T. $$\n2.2 Arbitrary unimodal symmetric link Let the link funciton $H$ be the cdf having a smooth unimodal symmetric density on the real line. Then $H$ may be represented as the scale mixture of $\\Phi$, and hence $$ H(t) = \\int_0^\\infty \\Phi(t\\sqrt{v}) d G(v), $$ for some known $G$. Introduce two sets of unobservable latent variables $Z,T$ and model it hierarchically by $$ \\begin{align*} \\eta \u0026\\sim GP(\\mu,\\Sigma) \\cr V_i|\\eta \u0026\\sim \\text{ i.i.d. } G, \\cr Z_i|\\eta,V \u0026\\sim \\text{ independent } N(\\eta_i, V_i^{-1}) \\cr Y_i|Z_i \u0026= I(Z_i\\geq 0). \\end{align*} $$ Then, given $\\eta$, the $Y_i's$ are independent Bernoulli random variables with success probability $H(\\eta_i)$. Let $D_V$ be the diagonal matrix with $j$-th element equals to $v_j$, and then by the Gaussian conjugacy, $$ \\Sigma_V^* = (D_V + \\Sigma^{-1})^{-1} ,\\quad \\mu_V^* = \\Sigma_V^* (D_V Z + \\Sigma^{-1}\\mu) = \\Sigma_V^* D_V (Z - \\mu) + \\mu. $$Suppose $G$ has a Labesgue density $g$. Then $$ \\begin{align} \\eta|Z,V,Y = \\eta|Z,V \u0026\\sim N(\\mu_V^*,\\Sigma_V^*), \\cr V_i|Z,\\eta, Y = V_i|\\eta,Z \u0026\\propto \\phi((Z_i - \\eta_i)\\sqrt{v_i}) g(v_i), \\cr Z_i|V,\\eta, V,Y = Z_i| Y,V \u0026\\sim \\begin{cases} N(\\eta(x_i), V_i^{-1}) | Z_i\\geq 0, \u0026 \\; Y_i = 1, \\cr N(\\eta(x_i), V_i^{-1}) | Z_i\u003c0, \u0026\\; Y_i = -1. \\end{cases} \\end{align} $$3. Hierarchical models The choice of the hyper-parameters $\\mu,\\Sigma$ are critical in the prior elicitation. One may put priors on these hyper-parameters. For instance, we can consider a parametric form for $\\Sigma$ by $$ \\Sigma_\\tau = \\Sigma/\\tau, $$ for some known $\\Sigma$ and unknown hyper-parameter $\\tau\u003e0$. Note that $\\tau$ determines the smoothness of data. A gamma distribution is an appropriate prior for $\\tau$, thus we consider the following hierarchical model\n$\\tau \\sim Gamma(a,b)$. $\\eta|\\tau \\sim N(\\mu,\\Sigma_\\tau)$. $Z|\\eta,\\tau \\sim N(\\eta(x), I)$. $Y_i|Z_i \\sim Bernoulli(H(Z_i))$. The distribution of $\\tau$ given $Y,Z,\\eta$ can be easily derived, which is still a Gamma distribution, wirte $Gamma(a^*, b^*)$. Then we can use a Gibbs sampler to generate samples from the distribution of $(\\tau,Z,\\eta|Y)$.\n4. Discussion This article described a nonparametric Bayesian approach to estimating the regression function for binary classification. The case where the link function $H$ who has smooth unimodal symmetric density was discussed. However, consistency behaviour was not mentioned.\n","permalink":"http://localhost:1313/posts/reading/gppriors_for_binary_classification/","summary":"\u003cp\u003eLet\u0026rsquo;s discuss this paper\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eChoudhuri, N., Ghosal, S., \u0026amp; Roy, A. (2007). Nonparametric binary regression using a Gaussian process prior. Statistical Methodology, 17.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"1-problem-setup\"\u003e1. Problem Setup\u003c/h2\u003e\n\u003cp\u003eConsider a binary classification probem given by\n\u003c/p\u003e\n$$ \n p(x) = P(Y = 1|X=x) = 1- P(Y = -1|X = x).\n$$\u003cp\u003e\nThis problem commonly occurs in many fields of application, such as medical and spatial statics. Traditionally, we would model this problem as\n\u003c/p\u003e\n$$ \n p(x) = H(\\eta(x)),\n$$\u003cp\u003e\nwhere $H$ is commonly choosen as a cdf, called the \u003cem\u003elink function\u003c/em\u003e, and $\\eta$ can be choosen parametrically or nonparametrically. In this paper, a nonparametric approach was studied, where $H$ was known, and the function $\\eta$ was estimated with a Gaussian prior.\u003c/p\u003e","title":"Reading | Nonparametric binary regression using a Gaussian process prior"},{"content":"In GP(1) and GP(2), we studied the RKSH and consistency rate of GP priors. In this sectin we are going to consider the smoothness of the target function and see how the smoothness level impacts the consistency rates.\n1. Introduction Suppose that we want to estimate a density function $p_0 \\in C^\\beta[0,1]$, where $C^\\beta[0,1]$ denotes the H$\\mathrm{\\\"o}$lder space of order $\\beta$. By Assouad’s method it can shown that no estimator can achieve better rates than $n^{-\\beta/(2\\beta+1)}$ uniformly, in terms of the distance $d(p_0, p) = ||p_0 - p||_1$. It has been long known that some estimators can achieve this rate given known smoothness level $\\beta$, for instance, kernel estimators.\nIn general the smoothness parameter is unknown. If we choose a parameter $\\beta'\u003e\\beta$, then we lose in terms of the consistency rate. On the other hand, if we choose a family of functions with smoothness level $\\beta'\u003c\\beta$, then the model is misspecified. In this case, consistency cannot be guaranteed. Therefore, we intend to find an estimator that achieves the optimal minimax rate without knowing $\\beta$, which is called adaption.\n2. RKHS of BM and Smoothness Let us review the contraction rate of GP priors\nIt implies that the optimal consistency rate for GP priors cannot be better than $n^{-1/2}$ when $w_0\\in \\bar{\\mathbb{H}}$. To estimate the density function, we need a RKHS containing continuous fucntions, so that Brownian motion is of interest.\nLemma (RKSH of BM) The RKSH of BM is the family of fuctions in $B^1[0,1]$ vanishing at zero, where $B^k[0,1]$ is the Sobolev space of all functions $f\\in L^2[0,1]$ that are $k-1$ times differentiable with $f^{(k-1)}$ absolutely continuous with $f^{(k)}\\in L^2[0,1]$, equipped with the inner product $$ \\langle f,g\\rangle_{\\mathbb{H}} = \\int_0^1 f' g' d\\mu. $$ proof The covariance kernel of BM is $r(s,t) = s\\wedge t$. The RKHS is the closed linear span of functions $t\\mapsto s\\wedge t$ as $s$ runs through $[0,1]$, unnder the inner product given by $$ \\langle s_1\\wedge \\cdot, s_2\\wedge\\cdot\\rangle_{\\mathbb{H}} = s_1\\wedge s_2 = \\int (s_1\\wedge t)' (s_2\\wedge t)' dt. $$ Thus the RKHS contains every function that vanishes at 0, continuous, and piecewise linear on any partition of $[0,1]$. The derivatives of these functions are piecewise constant, and the set of piecewise constant function is dense in $L^2[0,1]$. $\\Box$\nWithout proof, we state that, if $w_0\\in C^\\beta[0,1]$ for some $\\beta\\in(0,1]$, the contraction rate of BM prior is $$ n^{-\\frac{\\beta\\wedge 1/2}{2}}. $$ The optimal minimax rate can only be obtained for functions $p_0\\in C^{1/2}[0,1]$. For any $\\beta\\neq 1/2$, the contraction rate is at most $n^{-1/4}$, which is caused by the rough paths of BM. But the BM is a good building block for smoother function spaces.\nIntegrated BM. The $k$-fold integration of BM $I_{0+}^k B = I_{0+} I_{0+}^{k-1}B$, where $(I_{0+} f)(t) = \\int_0^t f(s) ds $. The RKHS of $I_{0+}^k B$ is $B^{k+1}[0,1]$, with inner product $$ \\langle f,g\\rangle_{\\mathbb{H}} = \\int_0^1 f^{(k+1)} (s) g^{(k+1)} (s) ds. $$ The k-fold integrated BM priors achieve contraction rates $n^{-\\frac{\\beta\\wedge k + 1/2}{2k+2}}$, where the optimal rate is obtained at $\\beta = k + 1/2$ only. For $\\beta\\geq k+1/2$, the performance does not improve for increasing $\\beta$.\n3. Adaptive GP Priors The GP priors discussed so far possess itself a certain regularity, and are optimal iff this matches the regularity of the target function. To obtain a prior that is adaptive for unknown $\\beta\u003e0$, there are two popular methods:\nHierachical prior, impose a prior on the smoothness prameter $\\beta$, which is also known as mixtures of GP. Rescaling, $W^a = (W_{ct}:t \\in [0,1])$. Here, we shall discuss rescaling only. The rescaling has the purpose of changing the appearance of the sample paths, making our prior reflect the true function better. Scaling factor $c\u003e1$ shrinks the sample paths $t\\mapsto W_t$ on a long interval $W_{[0,ct]}$ to the interval $[0,1]$, which roughens the sample paths by incorporating the randomness of a longer period. Conversely, factor $c\u003c1$ stretchs the sample paths to a shorter interval $[0,ct]$, which smooths the sample paths.\nDef (Self-similar) A SP $W$ is called self-similar of order $\\alpha$ if $(W_{ct}: t\\in[0,1])$ and $(c^\\alpha W_t: t\\in [0,1])$ are equal in distribution. Lemma (RKHS of rescaled GP) The RKHS $\\mathbb{H}^c$ of the rescaled process $W^c$ corresponding to a self-similar process $W$ of order $\\alpha$ is a RKHS of $W$, equipped with the norm $||h||_{\\mathbb{H}^c} = c^{-\\alpha}||h||_{\\mathbb{H}}$ instead. Without proof we state that the optimal contraction rate of rescaled GP corresponding to a self-similar process $W$ of order $\\alpha$ is $$ n^{-\\frac{2+r}{(4+ 4r + rs)}}, \\quad \\text{ by setting } c_n = n^{\\frac{s-r}{4\\alpha + 4r\\alpha + rs\\alpha}}, $$ where $r,s\u003e0$ are some constants s.t. $$ \\psi_0(\\epsilon) \\asymp \\epsilon^{-r} ,\\quad \\inf \\{ ||h||_{\\mathbb{H}}: ||h-w_0||\\leq \\epsilon \\} \\asymp \\epsilon^{-s}. $$ Note that the optimal contraction rate does not depends on the parameter $\\alpha$. BM is self-similar of order $1/2$, and thus the k-fold integrated BM is self-similar of order $1/2 + k$. If $w_0 \\in C^\\beta [0,1]$ with $\\beta\\leq k+1$. The rescaled k-fold integraed BM priors yield the minimax rate $n^{-\\beta/(2\\beta + 1)}$ by setting appropriate scaling factor $c_n$.\nReferences [1] van der Vaart, A. W., \u0026amp; van Zanten, J. H. (2008). Reproducing kernel Hilbert spaces of Gaussian priors.\n[2] Ghosal, S., \u0026amp; Van der Vaart, A. (2017). Fundamentals of nonparametric Bayesian inference.\n[3] Vaart, A. V. D., \u0026amp; Zanten, H. V. (2007). Bayesian inference with rescaled Gaussian process priors.\n[4] Belitser, E., \u0026amp; Ghosal, S. (2003). Adaptive Bayesian inference on the mean of an infinite-dimensional normal distribution. The Annals of Statistics, 31(2), 536–559.\n[5] Tsybakov, A. B. (2009). Introduction to nonparametric estimation. Springer.\n","permalink":"http://localhost:1313/posts/bayes/bayes_7_gp3/","summary":"\u003cp\u003eIn GP(1) and GP(2), we studied the RKSH and consistency rate of GP priors. In this sectin we are going to consider the smoothness of the target function and see how the smoothness level impacts the consistency rates.\u003c/p\u003e\n\u003ch2 id=\"1-introduction\"\u003e1. Introduction\u003c/h2\u003e\n\u003cp\u003eSuppose that we want to estimate a density function $p_0 \\in C^\\beta[0,1]$, where $C^\\beta[0,1]$ denotes the H$\\mathrm{\\\"o}$lder space of order $\\beta$. By Assouad’s method it can shown that no estimator can achieve better rates than $n^{-\\beta/(2\\beta+1)}$ uniformly, in terms of the distance $d(p_0, p) = ||p_0 - p||_1$. It has been long known that some estimators can achieve this rate given known smoothness level $\\beta$, for instance, kernel estimators.\u003c/p\u003e","title":"Bayesian Statistics| Gaussian Process Priors (3)"},{"content":" In GP(1) we introduced the RKHS of the GP and some of its properties. The main content of this section is to derive the posterior contraction rate of the GP.\nAt the same time, in Posterior Consistency and Contraction, we derived the most important conclusion:\nTo show the posterior contraction of GP, we only need to check conditions 5.7-5.9.\n1. Posterior Contraction Theorem (GP contraction) :\nThese three conclusions correspond to the three conditions in Theorem 5.19 mentioned above. Therefore, this result implies that the GP contracts at $w_0$ at rate $\\epsilon_n$.\nProof The assertion (7.7) is an immediate result of Lemma Small ball Then $Pr(||W-w_0||\u003c2\\epsilon_0) \\geq e^{-\\psi_{w_0}(\\epsilon_n)}\\geq e^{-n\\epsilon_n^2}$. Set $B_n = \\epsilon_n\\mathbb{B}_1 + M_n\\mathbb{H}_1$, then by Borell\u0026rsquo;s inequality we have $$ \\Pr(W\\notin B_n) \\leq 1 - \\Phi(\\alpha_n + M_n), $$ where $\\alpha_n = \\Phi^{-1}(e^{-\\psi_0(\\epsilon_n)})$. Since $$ \\psi_0(\\epsilon_n) \\leq \\psi_w(\\epsilon_n) \\leq n\\epsilon_n^2, $$we have that $\\alpha_n \\geq -M_n/2$ if $M_n = -2\\Phi^{-1}(e^{-Cn \\epsilon_n^2})$, for some $C\u003e1$. It follows that $$ 1 - \\Phi(\\alpha_n + M_n) \\leq 1 - \\Phi(M_n/2) = e^{-Cn\\epsilon_n^2}. $$For the assertion (7.8). Let $h_1,h_2,...,h_N \\in M_n \\mathbb{B}_n$ be $2\\epsilon_n$-separated in terms of the Banach space norm, then the $\\epsilon_n$-balls $h_i + \\epsilon_n \\mathbb{B}_1$ are disjoint, and hence by Lemma decentered,\n$$ \\begin{align} 1 \\geq \\sum_{j=1}^N \\Pr(W\\in h_j + \\epsilon_n\\mathbb{B}_1) \u0026\\geq \\sum_{j=1}^N e^{-1/2 || h_j||^2} \\Pr(||W||\u003c\\epsilon_n)\\cr \u0026\\geq N e^{-1/2M_n^2} e^{-\\psi_0(\\epsilon_n)}. \\end{align} $$Let $N$ be the maximal size of $2\\epsilon_n$-separated elements in $M_n\\mathbb{B}_1$, then we have $$ N(2\\epsilon_n, M_n\\mathbb{B}_1, ||\\cdot||) \\leq N\\leq e^{1/2 M_n^2}e^{\\psi_0(\\epsilon_n)}. $$ Notice that $d(w, M_n\\mathbb{B}_1)\\leq \\epsilon_n$ for any $w\\in B_n$, we have have $$ N(3\\epsilon_n, B_n, ||\\cdot||) \\leq e^{1/2 M_n^2}e^{\\psi_0(\\epsilon_n)}. $$ We can finish the proof by choosing $M_n$ s.t. $M_n^2/2 \\leq 8C n \\epsilon_n^2$.\n2. Examples Gaussian regression $$ Y_i = f(x_i) + \\epsilon_i. $$ We can construct a prior for $f$ by setting $f(x) = W_x$, where $(W_x)$ is a GP.\nSuppose that observations are generated by the true function $f_0 \\in W$, then by the preceding theorem, we can show that the posterior $$ \\Pi_n(|| f -f_0 || \u003e M \\epsilon_n| Y^{(n)}) \\to 0 $$ in $P_0^n$-probability for some $M\u003e0$.\nDensity estimation GP cannot serve as a prior for a density function directly, because it can take negative values. Instead we use a link function, and construct a prior $\\Pi$ for density function $p$ by $$ p(x) = \\frac{e^{W_x}}{\\int e^{W_y} dy}. $$ Because the distance between two densities $p_w, p_h$, corresponding to $w,h\\in W$, respectively, satisfies $$ d(p_w,p_h) \\leq C || w- h ||, $$ in terms of some norm $||\\cdot||$, the posterior can also satisfy $$ \\Pi_n(d(p, p_0) \u003e M\\epsilon_n| X^{(n)}) \\to 0 $$ in $P_0^n$-probability for some $M\u003e0$.\n[1] van der Vaart, A. W., \u0026amp; van Zanten, J. H. (2008). Reproducing kernel Hilbert spaces of Gaussian priors.\n[2] Ghosal, S., \u0026amp; Van der Vaart, A. (2017). Fundamentals of nonparametric Bayesian inference.\n","permalink":"http://localhost:1313/posts/bayes/bayes_6_gp2/","summary":"\u003c!-- # Bayesian Statistics| Gaussian Process Priors (2) --\u003e\n\u003cp\u003eIn \u003ca href=\"http://localhost:1313/posts/bayes/bayes_4_gp1/\"\u003eGP(1)\u003c/a\u003e we introduced the RKHS of the GP and some of its properties. The main content of this section is to derive the \u003cstrong\u003eposterior contraction rate\u003c/strong\u003e of the GP.\u003c/p\u003e\n\u003cp\u003eAt the same time, in \u003ca href=\"http://localhost:1313/posts/bayes/bayes_5_contraction/\"\u003ePosterior Consistency and Contraction\u003c/a\u003e, we derived the most important conclusion:\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"contraction_thm\" loading=\"lazy\" src=\"/img_bayes_GP/Contraction_2.PNG\"\u003e\u003c/p\u003e\n\u003cp\u003eTo show the posterior contraction of GP, we only need to check conditions 5.7-5.9.\u003c/p\u003e\n\u003ch2 id=\"1-posterior-contraction\"\u003e1. Posterior Contraction\u003c/h2\u003e\n\u003cp\u003eTheorem (GP contraction)\n:\u003cimg alt=\"GP contraction\" loading=\"lazy\" src=\"/img_bayes_GP/GPrate_1.PNG\"\u003e\u003c/p\u003e","title":"Bayesian Statistics| Gaussian Process Priors (2)"},{"content":" In the previous chapters, we introduced the Dirichlet Process (DP) prior, which is primarily used as a prior on measure spaces. In this chapter, we will introduce the Gaussian Process (GP), which can be used as a prior on function spaces. Consider the scenario where we have sample pairs $(X_i,Y_i),i\\leq n$. We are interested in studying the relationship between the inputs $X_i$ and the outputs $Y_i$. A common model for such a relationship is $$Y = \\beta X + \\epsilon.$$In this case, we are looking to estimate the parameter $\\beta$. However, we are considering a more generalized model by replacing the linear function with an unknown function $f(X_{i})$, as: $$Y_i = f(X_i) + \\epsilon_i,$$where $f$ is an unknown function. The goal is to estimate the function $f$, which is an infinite-dimensional model because $f$ represents a function defined over the continuous input space. Gaussian process regression assign $f$ a prior distribution $\\Pi$, and this prior is specifically a Gaussian Process.\n1. Gaussian Process Gaussian Process (GP) can be defined through its finite-dimensional distributions (FDDs).\nDefinition (Gaussian process) the map $t\\mapsto W_t(\\omega)$ is called the sample path of of $W$. Sample paths are functions from $T$ to $\\mathbb{R}$, so the process can be viewed as a map $W:\\Omega \\to \\mathbb{B}$, where $\\mathbb{B}$ is a function space, e.g. the space of continuous functions on $T$.\nThe problem with defining a Gaussian Process solely by its finite-dimensional distributions is that sample paths (i.e., the realizations of the process) may not exhibit the desired properties. For example, a Gaussian Process with a particular kernel might have sample paths that are not continuous, even though the FDDs at any finite set of points may still define a valid Gaussian distribution. To resolve this issue, we seek to find a version $\\tilde{W}$ of the process $W$ s.t. the sample paths $\\tilde{W}$ possess the desired properties. For instance, if we need a space of continuous functions on $T$, we may apply the following theorem\nA more abstract and general formulation of a Gaussian process is related to the dual space of a Banach space. Let $\\mathbb{B}$ be a Banach space. The dual space $\\mathbb{B}^*$ is the space of continuous linear functionals on $\\mathbb{B}$, i.e., the set of all linear maps from $\\mathbb{B}$ to the scalar field.\nDefinition (Gaussian random element) In this framework, the Gaussian Process can be viewed as a random element in the dual space $\\mathbb{B}^*$∗, where the process is defined in terms of the behavior of random variables indexed by elements of a Banach space.\nWhat is the relation between the above two definition of GP？With the second definition we can always construct a Gaussian stochastic process by $(b^{\\*}(W):b^{\\*}\\in T)$, which are jointly normally distributed, for any subset $T\\subset \\mathbb{B}^*$. Additionally，if the sample paths $t\\mapsto W_t$ of the stochastic process $W = (W_t:t\\in T)$ belong to a Banach process $\\mathbb{B}$ of functions $z:T\\to \\mathbb{R}$, then under some conditions the process will be a Gaussian random element. More specifically, the sample paths $t\\mapsto W_t$ belonging to a Banach space means that the process must satisfy certain regularity conditions on the covariance structure and continuity of the sample paths.\n2. Reproducing Kernel Hilbert Space Every Gaussian process is naturally associated with a Hilbert space, determined by its covariance function. For a GP $W = (W_t: t\\in T)$, let $\\overline{lin}(W)$ be the closure of the set of all linear combinations $\\sum_i \\alpha_i W_{t_i}$ in the $L^2$-space of square-integrable variables. Then the space $\\overline{lin}(W)$ is a Hilbert space, with inner product $\u003c f,g\u003e = E f\\bar{g}$, called the first order chaos of $W$.\nDefinition (RKHS, 1) It can be verified that $\\mathbb{H}$ is a Hilbert space.\nLemma (Properties of RKHS) A Gaussian random element in a separable Banach space also comes with a RKHS. First we define the map $S:\\mathbb{B}^*\\to \\mathbb{B}$ by\n$$Sb^* = E[b^*(W)W],$$where the right side is the Pettis integral.\nDefinition (RKHS, 2) The relationship between the two definitions is\n$$r(s,t) = E W_s W_t = E[\\pi_s(W) \\pi_t(W)] = \\langle S\\pi_s, S\\pi_t \\rangle_{\\mathbb H},$$ where $\\pi_t: b\\mapsto b(t)$ are elements in $\\mathbb B^*$.\n3. Small Ball Probability As a Bayesian, we are always interested in the posterior consistency. Here we give some lemmas which are useful to study the posterior consistency of Gaussian process priors.\nLemma (Borell\u0026rsquo;s) Let $\\Phi$ be the CDF of the standard normal. The additional small ball $\\epsilon\\mathbb B_1$ creates an $\\epsilon$-cushion around $M\\mathbb{B}_1$. This lemma suggests that if we want to use GP prior to estimate a function, we must ensure that the true function is contained in the closure of RKHS.\nLemma (Decentered) Lemma (Small ball) 4. Posterior Contraction Before discussing the posterior contraction of GP priors, we need to introduce some theoretical concepts from infinite-dimensional Bayes theory. This will be a lengthy section, so I will dedicate a separate chapter to it. After that, we will return to discuss the case of GP priors.\nReferences [1] van der Vaart, A. W., \u0026amp; van Zanten, J. H. (2008). Reproducing kernel Hilbert spaces of Gaussian priors.\n[2] Ghosal, S., \u0026amp; Van der Vaart, A. (2017). Fundamentals of nonparametric Bayesian inference.\n","permalink":"http://localhost:1313/posts/bayes/bayes_4_gp1/","summary":"\u003c!-- # Bayesian Statistics| Gaussian Process Priors (1) --\u003e\n\u003cp\u003eIn the previous chapters, we introduced the \u003cstrong\u003eDirichlet Process (DP) prior\u003c/strong\u003e, which is primarily used as a prior on measure spaces. In this chapter, we will introduce the \u003cstrong\u003eGaussian Process (GP)\u003c/strong\u003e, which can be used as a prior on function spaces. Consider the scenario where we have sample pairs $(X_i,Y_i),i\\leq n$. We are interested in studying the relationship between the inputs $X_i$ and the outputs $Y_i$.  A common model for such a relationship is\n\u003c/p\u003e","title":"Bayesian Statistics| Gaussian Process Priors (1)"},{"content":" This chapter discusses the theoretical question of whether non-parametric Bayes methods truly work. In other words, it addresses whether the posterior distribution really converges to the so-called \u0026ldquo;true\u0026rdquo; parameter $\\theta_0$. Contraction is a richer concept than consistency, as it also involves the rate of convergence.\nDirichlet Process priors and Gaussian Process priors are common non-parametric Bayesian methods, along with their various variants. It would be too cumbersome to discuss the convergence issues of these methods on a case-by-case basis. Therefore, this chapter will focus on some general theory, which can then be applied to specific models as needed.\n1. Consistency Definition (Posterior consistency) Theorem (Doob\u0026rsquo;s consistency) Doob\u0026rsquo;s consistency result is indeed elegant, as it essentially states that, given a prior $\\Pi$, its posterior is almost always consistent with the true parameter $\\theta_0$​, under very few assumptions about the model. However, the issue lies in the fact that we don\u0026rsquo;t know where the $\\Pi$-null set (the set of parameters for which consistency may fail) is. In some cases, this null set can be quite large, potentially making the result not very useful in practice.\nTo address this, we need to impose some restrictions on the prior to ensure that the target parameter $\\theta_0$​ does not lie in the null set. One such restriction is the Kullback-Leibler (KL) property, which is often used to control the behavior of priors and guarantee good properties for posterior convergence.\nDefinition (KL property) This property ensures that prior assigns positive probability to any KL neighbourhood of the density $p_0:=p_{\\theta_0}$ determined by parameter $\\theta_0$. With this property in place, we can ensure that the posterior distribution is consistent at $\\theta_0$, meaning that as the number of observations increases, the posterior distribution converges to the true parameter $\\theta_0$. We state the following theorem with proof, a more detailed proof of this result could be given in a separate chapter if possible.\nTheorem (Schwartz\u0026rsquo;s) The theorem has two assumptions. The first is the Kullback-Leibler (KL) property, and the second is related to tests.\n$P_0^n\\theta_n$ can be understood as the Type I error in hypothesis testing, $P_\\theta^n (1-\\theta_n)$ can be viewed as the type II error. The second assumption is quite strong. These tests ensure that, under the given prior $\\Pi$, the true parameter $\\theta_0$​ is consistently detected, meaning that we can identify the true parameter $\\theta_{0}$ using any statistical method, not necessarily Bayesian methods.\nTo make the framework more flexible, we can relax the second condition. Instead of requiring that tests exist for every complement $\\mathcal{U}^\\complement$, we allow for the possibility that tests might not exist in some very small subset of the parameter space. Specifically, we can allow this to occur in a set with small prior probability.\nTheorem (Schwartz\u0026rsquo;s extension) Sketch of proof We need to prove that for any neighbourhood $\\mathcal{U}$ of $p_0$, we have $\\Pi_n(\\mathcal{U}^\\complement|X^{(n)})\\to 0$ a.s.. step 1: show that $\\Pi_n(\\mathcal{U}^\\complement\\cap \\mathcal{P}|X^{(n)})\\to 0$ a.s.\nstep 2: show that $\\Pi_n( \\mathcal{P}^\\complement|X^{(n)})\\to 0$ a.s.\n2. Tests As mentioned earlier, for posterior consistency, we need two conditions: the KL property and the existence of tests. The KL property is relatively straightforward to verify, but the existence of tests is not as intuitive. Therefore, in this section, we will discuss the existence of tests and why it\u0026rsquo;s important for establishing posterior consistency.\nTheorem (Convexity and tests) The theorem suggests that when the set of alternative hypotheses $\\mathcal{Q}$ is convex and separated from the \u0026ldquo;true\u0026rdquo; parameter $P$ (in terms of Hellinger distance), desired tests exist.\nThe assumption that alternatives is convex is a strong one, and in practice, it may not always hold. To address this, we can relax the convexity assumption. Instead of requiring $\\mathcal{Q}$ to be convex, we allow $\\mathcal{Q}$ to be covered by a collection of convex sets. In other words, even if the set of alternatives is not convex, as long as it can be decomposed into a union of convex sets, we can still construct valid tests. This idea is called convex covering or entropy.\nTheorem (Entropy) So far, by combining Schwartz\u0026rsquo;s extension and the existence of entropy under entropy, we can derive a more general posterior consistency result.\nTheorem (Posterior consistency under entropy) This is a very general theorem, which allows us to transform all posterior consistency problems into the problems related to covering number.\n3. Posterior Contraction Contraction is a more refined concept than consistency in Bayesian inference. While consistency only tells us whether the posterior distribution concentrates around the true parameter as the sample size grows, contraction goes further by quantifying the rate $\\epsilon$ at which the posterior distribution converges to the true parameter.\nDefinition (Posterior contraction) Theorrem (Posterior contraction) Sketch of proof Through condition (5.7), we restrict the covering number, so by Proposition 5.15, we can find appropriate tests, and then use bounded type I and type II errors to control the posterior. So far, we have developed a very powerful theory. For both consistency and contraction, we have transformed them into problems related to the covering number. For a specific non-parametric Bayesian problem, what we need to do is simply study the corresponding covering number.\nNext, we will discuss some concrete examples in a new chapter, and this chapter will conclude here.\nReferences [1] Van der Vaart, A. W. (2000). Asymptotic statistics. [2] Ghosal, S., \u0026amp; Van der Vaart, A. (2017). Fundamentals of nonparametric Bayesian inference ","permalink":"http://localhost:1313/posts/bayes/bayes_5_contraction/","summary":"\u003c!-- # Bayesian Statistics| Posterior Consistency and Contraction --\u003e\n\u003cp\u003eThis chapter discusses the theoretical question of whether non-parametric Bayes methods truly work. In other words, it addresses whether the posterior distribution really converges to the so-called \u0026ldquo;true\u0026rdquo; parameter $\\theta_0$. Contraction is a richer concept than consistency, as it also involves the rate of convergence.\u003c/p\u003e\n\u003cp\u003eDirichlet Process priors and Gaussian Process priors are common non-parametric Bayesian methods, along with their various variants. It would be too cumbersome to discuss the convergence issues of these methods on a case-by-case basis. Therefore, this chapter will focus on some general theory, which can then be applied to specific models as needed.\u003c/p\u003e","title":"Bayesian Statistics| Posterior Consistency and Contraction"},{"content":" The Dirichlet Process (DP) is widely used in Bayesian nonparametrics, where it is often treated as a default prior on spaces of probability measures. As a prior on probability measures, the DP reflects our prior belief about the potential distributions of data, with the assumption that the data might be drawn from an infinite mixture of possible underlying distributions. This is why the Dirichlet Process is often called a nonparametric prior: it does not assume a fixed, finite number of parameters but rather allows the model to grow with the complexity of the data.\n0. Parametric VS Non-parametric Non-parametric does not mean the absence of parameters; rather, it indicates that the parameter space $\\mathcal{\\Theta}$ is infinite-dimensional. On the other hand， a statistical model is called parametric if its parameter space $\\mathcal{\\Theta}$ is finite-dimensional.\n1. Definitions Having a definition does not guarantee the actual existence of such an entity, so its existence must be proven. A Dirichlet Process (DP) can be viewed as a stochastic process, with equation (4.3) representing its finite-dimensional distributions. According to Kolmogorov\u0026rsquo;s Extension Theorem, we can establish the existence of the process.\nTo understand the Dirichlet Process (DP) prior, for a stochastic process $(X_t:t\\in T)$, we have two main ways to interpret it:\nFor a fixed $t$, $X_t:\\Omega\\to\\mathbb{R}$ is a random variable. For a fixed $\\omega\\in \\Omega$, the mapping $t\\mapsto X_t(\\omega)$ is called a sample path of $(X_t:t\\in T)$. Therefore, the stochastic process $(X_t:t\\in T)$ can be understood as the collection of its sample paths, and this stochastic process also defines the distribution of these sample paths. For the Dirichlet Process (DP), a sample path is a probability measure, meaning that each realization of the Dirichlet Process is a distinct probability distribution over the sample space. Thus, the Dirichlet Process provides a prior $\\Pi$ over these probability measures. In Bayesian inference, this prior reflects our uncertainty about the underlying distribution from which the data are drawn. The Dirichlet Process acts as a distribution over potential distributions, offering a flexible way to model an infinite-dimensional space of possible probability measures, allowing for an infinite number of clusters or categories in a non-parametric model.\n2. Properties First, let\u0026rsquo;s introduce a simulation method known as stick-breaking. The stick-breaking process is a popular technique used to generate samples from a Dirichlet Process (DP). The idea is to iteratively break a \u0026ldquo;stick\u0026rdquo; into smaller pieces, with each piece representing the probability mass assigned to a cluster.\nThis means that we can simulate a Dirichlet Process (DP) using the base measure $\\alpha$ and the scaling parameter $M$.\nsketch of proof step 1: show that $P = \\sum_j W_j \\delta_{\\theta_j}$ is a random measure. step 2: show that $P \\sim DP(M\\bar{\\alpha})$. For $j\\geq 2$, define $W_{j-1}' = W_j/(1-Y_1), \\theta'_j = \\theta_{j+1}$, then\n$$ P= Y_1 \\delta_{\\theta_1} + (1-Y_1)\\sum_{j\\geq 1} W_j'\\delta_{\\theta'_j}. $$Notice that the random measure $P':= \\sum_{j\\geq 1} W_j'\\delta_{\\theta'_j}$ has the same structure as $P$, and hence has the same distribution. Thus we have the equation $$P = Y\\delta_{\\theta} + (1-Y) P.$$ The remaining task is to solve this equation, and it can be proven that $P\\sim DP(M\\bar{\\alpha}）$ is the unique solution. $\\Box$\nThe second very important property of the Dirichlet Process (DP) is called tail-freeness. The concept is quite complex (refer to Freedman, 1963), but simply put, it represents a form of independence. Without proof, we state the conclusion: $DP(\\alpha)$ is tail-free. The tail-free property is crucial for deriving the posterior of the Dirichlet Process. It ensures that the random measure generated by the DP exhibits a kind of independence in its behavior, making the computation of posterior distributions tractable. Now, without proof, we present a theorem related to the Dirichlet Process,\nwhere $N_\\epsilon := \\# \\{i: X_i\\in A_\\epsilon\\}$, i.e. the number of observations falling in $A_\\epsilon$.\nThe tail-free property of the Dirichlet Process is extremely useful because, with this property, when calculating the posterior, we no longer need to worry about the exact values of the observations $X_i$，Instead, we only need to know the number of observations $X_{i}$ that fall into a given set.\nTheorem (Conjugacy of DP) The posterior of DP is again a DP. where $MN$ indicates the multinomial distribution. And the Multinomial-Dirichlet distribution conjugacy is used in the proof.\nThis theorem tells us that, when updating the posterior of a Dirichlet Process, we only need to update the probabilities of the observed points (rather than the exact data values themselves). Let $\\mathbb{P}_n$ denote the empirical distribution of observations, then the posterior of DP can be written as $DP(\\alpha + n\\mathbb{P}_n)$. This is a key result in Bayesian nonparametrics, particularly in models where the number of clusters or components is unknown and can grow as more data are observed. It shows that, thanks to the tail-free property, we can focus on the distribution of observed values and their frequencies, making posterior updates computationally feasible.\n3. Consistency When we use the Dirichlet Process (DP) to estimate a probability measure, we are often concerned with its convergence properties, which are central to most learning algorithms. Specifically, we want to ensure that as we observe more data, the Dirichlet Process will converge to the true underlying distribution or probability measure. In this context, we can say that the DP prior does work, meaning that it provides a consistent way to estimate the true distribution as more data is observed, and we will now proceed to prove this result.\nTheorem (Consistency of DP) Suppose that observations $X_i\\sim P_0$ independently. sketch of proof Since Dirichlet Processes (DP) are discrete and not directly suitable for estimating continuous functions, transitioning to Gaussian Processes (GP) makes sense, as they are commonly used for estimating continuous functions in Bayesian nonparametrics.\nReferences [1] Ghosal, S. (2010). The Dirichlet process, related priors and posterior asymptotics [2] Ghosal, S., \u0026amp; Van der Vaart, A. (2017). Fundamentals of nonparametric Bayesian inference ","permalink":"http://localhost:1313/posts/bayes/bayes_3_dirichlet/","summary":"\u003c!-- # Bayesian Statistics| Dirichlet Process --\u003e\n\u003cp\u003eThe Dirichlet Process (DP) is widely used in \u003cstrong\u003eBayesian nonparametrics\u003c/strong\u003e, where it is often treated as a default prior on spaces of probability measures. As a \u003cstrong\u003eprior\u003c/strong\u003e on probability measures, the DP reflects our prior belief about the potential distributions of data, with the assumption that the data might be drawn from an infinite mixture of possible underlying distributions. This is why the Dirichlet Process is often called a \u003cstrong\u003enonparametric prior\u003c/strong\u003e: it does not assume a fixed, finite number of parameters but rather allows the model to grow with the complexity of the data.\u003c/p\u003e","title":"Bayesian Statistics| Dirichlet Process"},{"content":" From a non-Bayesian perspective, when we aim to estimate the parameters of a certain model, we often apply the Central Limit Theorem (CLT) and obtain a result similar to this.\n$$ \\sqrt{n} (\\hat{\\theta}_n - \\theta_0) \\to N(0,\\Sigma) $$From a Bayesian perspective, a similar conclusion can be drawn, often referred to as the Bayesian Central Limit Theorem. This is precisely the Bernstein-von Mises Theorem (BvM), which will be introduced in this article.\n0. Notation $I_\\theta$: Fisher information,\n$\\ell_\\theta(x) = \\log p_\\theta(x)$,\n$\\dot{\\ell}_\\theta(x) = \\frac{d}{d \\theta} \\ell_\\theta(x)$,\n$\\ddot{\\ell}_\\theta(x) = \\frac{d^2}{d\\theta^2} \\ell_\\theta(x)$,\n$\\Delta_{n,\\theta} = 1/\\sqrt{n} \\sum_{i=1}^n \\dot{\\ell}_\\theta(x_i)$,\n$I_{n,\\theta} = -1/n \\sum_{i=1}^n \\ddot{\\ell}_\\theta(x_i)$,\n$||\\cdot||_{TV}$: total variation,\n1. Preliminaries Bayes\u0026rsquo; formula By Bayes\u0026rsquo; formula, the posterior density of $\\vartheta$ is given by $$ \\begin{align*} \\pi(\\theta|X_1,...,X_n) = \\frac{\\prod_{i=1}^n p_\\theta(X_i) \\pi(\\theta )}{\\int \\prod_{i=1}^n p_\\theta(X_i) \\pi(\\theta)d\\theta}. \\end{align*} $$ Taylor expansion $$ \\log \\frac{p_{\\theta+h}}{p_\\theta} (x) = h^T \\dot{\\ell}_\\theta(x) + \\frac{1}{2} h^T \\ddot{\\ell}_\\theta(x) h + o_p(||h||^2). $$ 2. BvM Theorem (BvM) Under some conditions, $$ \\begin{equation} \\Vert \\Pi(\\sqrt{n}(\\vartheta - \\theta_0)\\in \\cdot| X_1,...,X_n) - N_d(I_{\\theta_0}^{-1}\\Delta_{n,\\theta_0}, I_{\\theta_0}^{-1}) \\Vert _{TV}\\to 0 \\; \\text{ in } P_{\\theta_0}^n. \\end{equation} $$ Here the sequence of variables $\\Delta_{n,\\theta_0}$ converges under $ \\theta_0 $ in distribution to $N_d(0, I_{\\theta_0})$. sketch pf proof step 1: Taylor expansion to apply CLT, Notice that $\\Delta_{n,\\theta} \\to \\Delta_\\theta:=N(0, I_\\theta)$ in distribution, and $I_{n,\\theta}\\to I_{\\theta}$ in probability by CLT.\nstep 2: Rewrite the posterior. In the first step, we observe that by performing a Taylor expansion on the log-likelihood, we can apply the CLT. Therefore, we aim to perform a Taylor expansion on the posterior distribution. Now that we have rewritten the posterior into the form from step 1, we can perform a Taylor expansion on it. The numerator is approximately equal to: $h^T\\Delta_{n,\\theta} + \\frac{1}{2} h^T I_{\\theta,n}h$.\nstep 3: Approximation posterior by Gaussian. Additionally, for a Gaussian distribution, we can derive through calculation that Let $\\Delta_\\theta = I_\\theta X$, we can see that， $(P_{\\theta+h/\\sqrt{n}}) \\to N(h,I_{\\theta}^{-1})$， so the asymptotical behavior of the posterior can be represented in terms of $N(h,I_{\\theta}^{-1})$ as where $X\\sim I_{\\theta_0}^{-1} N(0,I_{\\theta_0})$. $\\Box$\nReferences [1] Van der Vaart, A. W. (2000). Asymptotic statistics. [2] Ghosal, S., \u0026amp; Van der Vaart, A. (2017). Fundamentals of nonparametric Bayesian inference ","permalink":"http://localhost:1313/posts/bayes/bayes_2_bvm/","summary":"\u003c!-- # Bayesian Statistics| Bernstein-von Mises Theorem  --\u003e\n\u003cp\u003eFrom a non-Bayesian perspective, when we aim to estimate the parameters of a certain model, we often apply the Central Limit Theorem (CLT) and obtain a result similar to this.\u003c/p\u003e\n$$\n\\sqrt{n} (\\hat{\\theta}_n - \\theta_0) \\to N(0,\\Sigma)\n$$\u003cp\u003eFrom a Bayesian perspective, a similar conclusion can be drawn, often referred to as the Bayesian Central Limit Theorem. This is precisely the Bernstein-von Mises Theorem (BvM), which will be introduced in this article.\u003c/p\u003e","title":"Bayesian Statistics| Bernstein-von Mises Theorem"},{"content":"","permalink":"http://localhost:1313/posts/example/about/","summary":"","title":"About"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n.emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; }","permalink":"http://localhost:1313/posts/example/emoji-support/","summary":"\u003cp\u003eEmoji can be enabled in a Hugo project in a number of ways.\u003c/p\u003e","title":"Emoji Support"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code Code Blocks Code block with backticks \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; List Types Ordered List First item Second item Third item xxx Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/example/markdown-syntax/","summary":"\u003cp\u003eThis article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\u003c/p\u003e","title":"Markdown Syntax Guide"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTex globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTex on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Block math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$test in line $\\alpha + \\beta$.\ntest another in line math $\\varphi = \\frac{1+\\sqrt 5}{2}= 1.6180339887…$\n$$ \\begin{align} a\u0026=b+c \\cr d+e\u0026=f \\end{align} $$align test\n$$ \\begin{align} a\u0026=b+c \\cr d+e\u0026=f \\end{align} $$","permalink":"http://localhost:1313/posts/example/math-typesetting/","summary":"\u003cp\u003eMathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\u003c/p\u003e","title":"Math Typesetting"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo.\nRedit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\nExierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\nComas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et Vagus elidunt The Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","permalink":"http://localhost:1313/posts/example/placeholder-text/","summary":"\u003cp\u003eLorem est tota propiore conpellat pectoribus de pectora summo.\u003c/p\u003e","title":"Placeholder Text"},{"content":"这个系列主要内容是介绍贝叶斯理论，是我之前上课笔记的简单整理。整个体系是建立在测度论基础上的，所以默认读者已经熟悉 measure-theoretical probability 的基本内容了， 如果没有这方面基础，建议任意找本教材查阅。 这是第一章，我们的主要任务是定义先验概率(piror)和后验概率(posterior). 在非贝叶斯统计里，我们认为一个统计模型是一个概率分布的集合 $\\{ P_\\theta: \\theta\\in \\Theta \\}$. 然而，贝叶斯统计给与了参数$\\theta$一个先验分布，denoted by $\\Pi$, 这样分布$P_\\theta$就成了 data $X$ given $\\vartheta = \\theta$ 的条件概率, 从而 $(X,\\vartheta)$拥有联合概率 $$ \\begin{equation} \\Pr(X\\in A,\\vartheta\\in B) = \\int_B P_\\theta(A) d\\Pi(\\theta) . \\end{equation} $$ 观测到数据之后，我们可以对 prior 进行更新，这被称为posterior, given by $$ \\Pi(B|x) = \\Pr(\\vartheta\\in B| X =x). $$1. Definitions $\\mathfrak{X},\\mathscr{X}$\n","permalink":"http://localhost:1313/posts/bayes/bayes_1_priors/","summary":"\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e这个系列主要内容是介绍贝叶斯理论，是我之前上课笔记的简单整理。整个体系是建立在测度论基础上的，所以默认读者已经熟悉 measure-theoretical probability 的基本内容了， 如果没有这方面基础，建议任意找本教材查阅。\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e这是第一章，我们的主要任务是定义先验概率(piror)和后验概率(posterior). 在非贝叶斯统计里，我们认为一个统计模型是一个概率分布的集合 $\\{ P_\\theta: \\theta\\in \\Theta \\}$. 然而，贝叶斯统计给与了参数$\\theta$一个先验分布，denoted by $\\Pi$, 这样分布$P_\\theta$就成了 data $X$ given $\\vartheta = \\theta$ 的条件概率, 从而 $(X,\\vartheta)$拥有联合概率\n\u003c/p\u003e\n$$\n\\begin{equation} \n   \\Pr(X\\in A,\\vartheta\\in B) = \\int_B P_\\theta(A) d\\Pi(\\theta) .\n\\end{equation}\n$$\u003cp\u003e\n观测到数据之后，我们可以对 prior 进行更新，这被称为posterior, given by\n\u003c/p\u003e\n$$ \n \\Pi(B|x) = \\Pr(\\vartheta\\in B| X =x).\n$$\u003ch2 id=\"1-definitions\"\u003e1. Definitions\u003c/h2\u003e\n\u003cp\u003e$\\mathfrak{X},\\mathscr{X}$\u003c/p\u003e","title":"Bayesian Statistics| Pirors and Posteriors"},{"content":"这个专题主要介绍一些基础的 (theoretical) Bayesian statistics topics.\n目标人群：\n对Bayes有初步了解的，有一定 statistics，machine learning 基础的，且对理论有兴趣的人。整体上是基于 measure-theoretical probability， 所以读者最好有一定 measure theory 基础，基本上，具有数学本科三年级基础就足够了。\n主要内容:\nParametric Bayes and posterior consistency.\nNon-parametric Bayes, including Dirichlet process prior and Gaussian process prior.\nGeneral consistency theory for non-parametric Bayes.\nReproducing kernel Hilbert space (RKHS) theory.\n大部分内容将参考 Ghosal, S., \u0026amp; Van der Vaart, A. (2017). Fundamentals of nonparametric Bayesian inference，并结合一些其他论文。\n风格： 框架式的介绍，可以视为一种提纲，大部分技术性的、繁琐的证明细节将被省略，少部分结论会被证明，但也仅以粗略的形式给出。\n欢迎讨论。\n","permalink":"http://localhost:1313/posts/bayes/bayes_0_intro/","summary":"\u003cp\u003e这个专题主要介绍一些基础的 (theoretical) Bayesian statistics topics.\u003c/p\u003e\n\u003cp\u003e目标人群：\u003c/p\u003e\n\u003cp\u003e对Bayes有初步了解的，有一定 statistics，machine learning 基础的，且对理论有兴趣的人。整体上是基于 measure-theoretical probability， 所以读者最好有一定 measure theory 基础，基本上，具有数学本科三年级基础就足够了。\u003c/p\u003e\n\u003cp\u003e主要内容:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eParametric Bayes and posterior consistency.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNon-parametric Bayes, including Dirichlet process prior and Gaussian process prior.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGeneral consistency theory for non-parametric Bayes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eReproducing kernel Hilbert space (RKHS) theory.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e大部分内容将参考 Ghosal, S., \u0026amp; Van der Vaart, A. (2017). Fundamentals of nonparametric Bayesian inference，并结合一些其他论文。\u003c/p\u003e\n\u003cp\u003e风格：\n框架式的介绍，可以视为一种提纲，大部分技术性的、繁琐的证明细节将被省略，少部分结论会被证明，但也仅以粗略的形式给出。\u003c/p\u003e\n\u003cp\u003e欢迎讨论。\u003c/p\u003e","title":"Bayesian Statistics| Introduction"},{"content":"There is an additional structure on $B(X,Y)$ if $X,Y$ are Hilbert spaces, which enables us to obtain a simpler characterization of invertibility. This is the adjoint of an operator.\nDefinition and properties of adjoints. Adjoints and invertibility. Three important operators (normal, self-adjoint and unitary operators) that are related to the adjoint, and their properties. Spectrum of operators. 1. Adjoint In this section we define the adjoint and show its existence and uniqueness.\n","permalink":"http://localhost:1313/posts/analysis_3/functional_5/","summary":"\u003cp\u003eThere is an additional structure on $B(X,Y)$ if $X,Y$ are Hilbert spaces, which enables us to obtain a simpler characterization of invertibility. This is the \u003cspan style = \"color:orange\"\u003e\u003cem\u003eadjoint\u003c/em\u003e\u003c/span\u003e of an operator.\u003c/p\u003e\n\u003chr\u003e\n\u003cul\u003e\n\u003cli\u003eDefinition and properties of adjoints.\u003c/li\u003e\n\u003cli\u003eAdjoints and invertibility.\u003c/li\u003e\n\u003cli\u003eThree important operators (normal, self-adjoint and unitary operators) that are related to the adjoint, and their properties.\u003c/li\u003e\n\u003cli\u003eSpectrum of operators.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-adjoint\"\u003e1. Adjoint\u003c/h2\u003e\n\u003cp\u003eIn this section we define the adjoint and show its existence and uniqueness.\u003c/p\u003e","title":"Functional Analysis | 5 Linear Operators on Hilbert Spaces"},{"content":"1. Dual Spaces In this section we describe the dual of several of the standard spaces.\nTheorem 1 If $X$ is a finite-dimensional normed linear space with basis $\\{v_1,...,v_n\\}$, then $X'$ has a basis $\\{f_1,...,f_n\\}$ s.t. $f_j(v_k) = \\delta_{jk}$. In particular, $dim (X') = dim (X)$. proof Let $x\\in X$, we can represent is as $x = \\sum_{k=1}^n \\alpha_k v_k$. Define $f_j:X\\to\\mathbb{F}$ by $$f_j(x) = \\alpha_j.$$ It can be verified that $f_j$ is a linear transformation s.t. $f_j(v_k) = \\delta_{jk}$. Moreover, $f_j$ is continuous since $X$ is finite-dimensional, and thus $f_j\\in X'$. Next, we show that $\\{f_1,...,f_n\\}$ is a basis for $X'$. Suppose that $\\beta_1,...,\\beta_n$ are scalars s.t. $\\sum_{j=1}^n \\beta_j f_j = 0$. Then $$ 0 = \\sum_{j=1}^n \\beta_j f_j (v_k) = \\sum_{j=1}^n \\beta_j \\delta_{jk} = \\beta_k, $$ and so $\\{f_1,...,f_n\\}$ is linearly independent. Now for any $f\\in X'$, let $\\gamma_j = f(v_j)$. Then $$ \\sum_{j=1}^n \\gamma_j f_j(v_k) = \\gamma_k = f(v_k). $$ It follows that $f = \\sum_{j=1}^n \\gamma_j f_j $ since $v_1,...,v_n$ is a basis for $X$.\n$\\Box$\nTheorem 2 (Riesz-Frechet theorem) Let $\\mathcal{H}$ be a Hilbert space and let $f\\in \\mathcal{H}'$. Then there is a unique $y\\in \\mathcal{H}$ s.t. $f(x) = f_y(x) = \\langle x,y\\rangle$ for all $x\\in\\mathcal{H}$. Moreover, $\\|f\\| = \\|y\\|$. proof Existence. If $f= 0$, then $y=0$ is a suitable choice. Otherwise, $Ker f = \\{x: f(x) = 0\\}$ is a closed subspace of $\\mathcal{H}$ so that $(Ker f)^\\perp \\neq \\{0\\}$. Therefore there exists $z\\in (Ker f)^\\perp$ s.t. $f(z) = 1$. In particular, $z\\neq 0$ so we may define $y=\\frac{z}{\\|z\\|^2}$. Now let $x\\in \\mathcal{H}$ be arbitrary. Since $f$ is linear, $$ f(x - f(x)z) = 0, $$ and hence $x - f(x)z \\in Ker f$. However, $z\\in (Ker f)^\\perp$ so $$ \\langle x - f(x)z , z\\rangle = 0 . $$ It follows that $\\langle x, z\\rangle = f(x) \\|z\\|^2$. It suggests that $f(x) = \\langle x, y\\rangle$. Now, if $\\|x\\|\\leq 1$, by the Cauchy-Schwarz inequality, $\\|f\\|\\leq \\|y\\|$. On the other hand, if $x = y/\\|y\\|$ then $\\|x\\| = 1$, so $\\|f\\|\\geq |f(x)|= \\|y\\|$. Therefore, $\\|f\\| = \\|y\\|$.\nUniqueness. If $y,w$ are s.t. $f(x) = \\langle x, y\\rangle = \\langle x, w\\rangle$ for all $x\\in\\mathcal{H}$, then $\\langle x , y-w\\rangle=0$ for all $x$. Hence, we have $y-w = 0$.\n$\\Box$\nThis theorem gives a representation of all the elements of the dual space $\\mathcal{H}'$ of a general Hilbert space $\\mathcal{H}$.\nTheorem 3 Let $\\mathcal{H}$ be a Hilbert space, and define $T: \\mathcal{H}\\to \\mathcal{H}'$ by $T y = f_y , \\; y\\in\\mathcal{H}$. Then $T$ is a bijection, and for all $\\alpha,\\beta \\in\\mathbb{F}, y,z\\in\\mathcal{H}$: $T(\\alpha y + \\beta z) = \\bar{\\alpha} Ty + \\bar{\\beta}Tz$, $\\| Ty\\| = \\|y\\|$. In addition, the inner product $\\langle\\cdot,\\cdot\\rangle_{\\mathcal{H}'}$ can be defined on $\\mathcal{H}'$ by $$ \\begin{equation} \\langle Tz , Ty \\rangle_{\\mathcal{H}'} = \\langle y,z\\rangle_{\\mathcal{H}}. \\end{equation} $$ With this inner product, $\\mathcal{H}'$ is a Hilbert space.\nproof The bijectivity of $T$ and property $2$ follows immediately from Riesz-Frechet, Theorem 4.2. Next, for all $x\\in\\mathcal{H}$ we have $$ f_{\\alpha y + \\beta z} (x) = \\langle x, \\alpha y + \\beta z\\rangle = \\bar{\\alpha} f_y(x) + \\bar{\\beta} f_z(x). $$ Next, we show (1). Obviously, it is a norm, and $\\|f_y\\|^2 = \\|y\\|^2 = \\langle f_y, f_y\\rangle_{\\mathcal{H}'}$. $\\mathcal{H}'$ is complete because it is a Banach space.\n$\\Box$\n2. The Hahn-Banach Theorem Sometimes we have a linear functional $f_W:W\\to \\mathbb{F}$ defined on a subspace $W\\subset X$, but we want to extend this functional to the whole of $X$.\nDefinition 4 (Extension) Let $X$ be a vector space, $W$ a linear subspace of $X$, and $f_W$ a linear functional on $W$. A linear functional $f_X$ on $X$ satisfying $$ f_X(w) = f_W(w),\\;\\forall w\\in W, $$ is called an extension of $f_W$. Definition 5 (Sublinear functional) Let $X$ be a real vector space. A sublinear functional on $X$ is a function $p:X\\to \\R$ s.t. for all $\\alpha\\geq 0, x,y\\in X$, $p(x+y)\\leq p(x) + p(y)$, $p(\\alpha x) = \\alpha p(x)$ . Definition 6 (Seminorm) Let $X$ be a real or complex vector space. A seminorm on $X$ is a function $p:X\\to \\R$ s.t. for all $\\alpha\\in \\mathbb{F}, x,y\\in X$, $p(x+y)\\leq p(x) + p(y)$, $p(\\alpha x) = |\\alpha| p(x)$ . Theorem 7 (The Hahn-Banach theorem, real) Let $X$ be a real vector space, $W\\subset X$ a linear subspace of $X$, and $p$ a sublinear functional defined on $X$. If a linear functional $f_W:W\\to \\R$ satisfies $$ f_W(w)\\leq p(w), \\; w\\in W, $$ then $f_W$ has an extension $f_X$ on $X$ s.t. $$ f_X(x)\\leq p(x),\\; x\\in X. $$ proof Prove it in next section. $\\Box$\nLet $u$ be the real part of a complex-linear functional $f$ on $X$, then $u$ is real-linear and $$ \\begin{equation} f(x) = u(x) - i u(ix), \\; x\\in X, \\end{equation} $$ Theorem 8 (The Hahn-Banach theorem) Let $X$ be a real or complex vector space, $W\\subset X$ a linear subspace of $X$, and $p$ a seminorm on $X$. If a linear functional $f_W:W\\to \\R$ satisfies $$ |f_W(w)|\\leq p(w), \\; w\\in W, $$ then $f_W$ has an extension $f_X$ on $X$ s.t. $$ |f_X(x)|\\leq p(x),\\; x\\in X. $$\u0026gt; proof Let $u_W$ be the real part of $f_W$. Then $u_W$ has an extension $u_X$ s.t. $|u_X (x)|\\leq p(x)$ on $X_R$. The results now follows by (2). $\\Box$\nLemma 9 $W$ is a linear subspace of a real vector space $X$, $p$ is a sublinear functional on $X$. $f_W:W\\to\\R$ is linear and $f_W(w)\\leq p(w)$ on $W$. Suppose $x_1\\in X\\setminus W$, and let $$ M_1 = \\{\\alpha x_1 + w: \\alpha\\in R,w\\in W\\}. $$ Then there exists $f_1:M_1\\to\\R$ and $\\xi_1\\in\\R$ satisfying $$ \\begin{equation} f_1(\\alpha x_1 + w) = \\alpha\\xi_1 + f_W(w) \\leq p(\\alpha x_1 + w). \\end{equation} $$ Clearly, $f_1$ is linear and is an extension of $f_W$ on $M_1$.\nproof For any $x,y\\in W$, we have $$ f_W(x) + f_W(y) \\leq p(x+y) \\leq p(x-x_1) + p(x_1 + y) , $$ and so $$ f_W(x) - p(x-x_1) \\leq -f_W(y) + p(x_1+y). $$ Let $\\xi_1$ be the least upper bound of the left side, as $x$ ranges over $W$. Then $$ \\begin{equation} f_W (x) - \\xi_1 \\leq p(x-x_1), \\end{equation} $$ and $$ \\begin{equation} f_W (y) + \\xi_1 \\leq p(y+x_1). \\end{equation} $$ Define $f_1$ on $M_1$ by $f_1(\\alpha x_1 + w) = \\alpha \\xi_1 + f_W(w)$. $\\Box$\nTheorem 10 (H-B for normed spaces) $W$ is a linear subspace of a real or complex vector space $X$, $f_W\\in W'$, Then there exists an extension $f_X\\in X'$ of $f_W$ s.t. $\\|f_X\\| = \\|f_W\\|$. proof Let $p(x) = \\|f_W\\| \\|x\\|$ for $x\\in X$. Then $p$ is a siminorm on $X$. W.l.o.g. we assume that $W$ is not closed. We assume that $X$ is separable. If $X$ is real, we can find a sequence of unit vectors $\\{x_n\\}, x_n\\in X\\setminus M_{n-1}$,, where $M_0 = W$. Then we have $X = \\overline{M}_{\\infty}$ by separability. By Lemma 4.9, for $f_n$ there exists an extension $f_{n+1}\\in M'_{n+1}$ with $\\|f_{n+1} \\| = \\|f_n\\| = \\|f_W\\|$. We now show that there is an extension of these functionals to $M_\\infty$, and then to $X$. For any $x\\in M_\\infty$, there exists an integer $n_x$ s.t. $x\\in M_{m}$ and $f_m(x) = f_{n_x}(x)$ for all $m\\geq n_x$. Thus we may define $f_\\infty(x) = f_{n_x}(x)$. It is clear that $f_\\infty$ is an extension of $f_W$ and satisfies $\\|f_\\infty\\| = \\|f_W\\|$.\nFinally, since $X = \\overline{M}_{\\infty}$, $f_\\infty$ has an extension by continuity, $f_X\\in X'$, satisfying $\\|f_X\\| = \\|f_\\infty\\| = \\|f_W\\|$.\nSimilarly, we can extend the results to complex cases. It yields a complex linear functional $f_X\\in X'$ s.t. $$ |f_X(x)|\\leq p(x) = \\|f_W\\| \\|x\\|, \\; x\\in X. $$ Therefore $\\|f_X\\| = \\|f_W\\|$.\n$\\Box$\nWe present some immediate consequences of Theorem 4.10.\nTheorem 11 $W$ is a linear subspace of a real or complex vector space $X$, $f_W\\in W'$, For $x\\in X$, $\\delta:=\\inf_{w\\in W } \\|x - w\\| \u003e0$. Then there exits $f\\in X'$ s.t. $\\|f\\| = 1,f(x) =\\delta$ and $f(w) = 0$ for all $x\\in W$. proof W.l.o.g. we assume that $W$ is closed (if not, we replace it by its closure, which does not change the value of $\\delta$). Given $x\\in X$, we can define $M_1$ as before, and define a linear functional $f_1$ on $M_1$ by $$ f_1(\\alpha x + w) = \\alpha \\delta. $$ Clearly,\n$$ |f_1 (\\alpha x + w)| = |\\alpha| \\delta \\leq |\\alpha| \\| x - \\alpha^{-1} w\\| = \\|\\alpha x - w\\|, $$ which implies that $f_1 \\in M_1'$ and $\\|f_1\\|\\leq 1$. We now prove that $\\|f_1\\|\\geq 1$. For any $\\epsilon\\in (0,1)$, by Riesz\u0026rsquo; (Theorem 1.12), there exists $y_\\epsilon = \\alpha_\\epsilon x + w_\\epsilon \\in M_1$ s.t. $\\|y_\\epsilon\\| = 1$ and $\\|y_\\epsilon - w\\|\u003e1-\\epsilon$ for all $w\\in W$. Hence, $$ 1-\\epsilon\u003c \\|y_\\epsilon - w\\| = |\\alpha_\\epsilon| \\|x + \\alpha_\\epsilon^{-1}(w_\\epsilon -w)\\| \\leq |\\alpha_\\epsilon| \\delta , $$ which implies that $$ |f_1(y_\\epsilon)| = |\\alpha_\\epsilon| \\delta \u003e 1-\\epsilon. $$ Therefore, $\\|f_q\\|\\geq 1$.\nSo far we have proved that $\\|f_1\\|=1$, so by Theorem 4.10, $f_1$ has an extension $f\\in X'$ with $\\|f\\|=1$. By the construction, it is clear that $f(x) = \\delta, f(w) = 0, w\\in W$.\n$\\Box$\nCorollary 12 $x_1,...,x_n\\in X$ are linearly independent, then there exists $f_1,...,f_n\\in X'$ s.t. $f_j(x_k) = \\delta_{jk}$ for $j,k\\leq n$. proof Define $M = \\overline{sp}\\{x_1,...,x_n\\}$, which is a Hilbert space, By Theorem 4.1 there exists $f_1,...,f_n \\in M'$ s.t. $f_j(x_k) = \\delta_{jk}$. Then by Theorem 4.10, we can extend $f_1,...,f_n$ to $X'$. $\\Box$\nTheorem 13 (Dual and separability) $X'$ is separable then so is $X$. proof Let $B = \\{f\\in X': \\|f\\|=1\\}$. It follows immediately from the separability of $X'$ that we can choose a countable set of functionals $F= \\{f_1,f_2,...\\}\\subset B$ which is dense in $B$. For each $n$, we choose $w_n\\in X$ s.t. $\\|w_n\\| =1$ and $f_n(w_n)\\geq 1/2$, and let $W = \\overline{sp}\\{w_1,w_2,...\\}$. Clearly, $W\\subset X$. Now we suppose that $W\\neq X$. Then by Theorem 4.11 there exits $f\\in B$ s.t. $f(w) = 0$ for all $w\\in W$. However, this yields that $$ 1/2 \\leq |f_n(w_n)| = |f_n(w_n) - f(w_n)| \\leq \\|f- f_n\\| ,\\;n\\geq 1, $$ which contradicts the density of $F$ in $B$, and so proves that $W =X$. It follows that $\\{w_1,w_2,...\\}$ is a countable dense set in $X$, and so $X$ is separable. $\\Box$\n3. Proof of H-B Theorem. In this section we will prove Theorem 4.7.\nDefinition 14 (Ordered set) Suppose that $M$ is a non-empty set and $\\prec$ is an ordering on $M$. Then $\\prec$ is an partial order on $M$ if $x\\prec x$ for all $x\\in M$, $x\\prec y, y\\prec x \\Rightarrow x= y$, $x\\prec y, y\\prec z\\Rightarrow x\\prec z$, and then $M$ is a partially ordered set. If, in addition, $\\prec$ is defined for all pairs of elements (for any $x,y\\in M$, either $x\\prec y$ or $y\\prec x$), then $\\prec$ is a total order and $M$ is a totally ordered set.\nThe usual order $\\leq$ on $\\R$ is a total order.\nLemma 15 (Zorn\u0026rsquo;s) Let $M$ be a non-empty, partially ordered set s.t. every totally ordered subset of $M$ has an upper bound. Then there exists a maximal element in $M$. Proof of Theorem 4.7 Let $E$ denote the set of linear functionals $f$ on $X$ satisfying the conditions: $f$ is defined on a linear subspace $D_f$ s.t. $W\\subset D_f\\subset X$, $f(w) = f_W(w)$, $w\\in W$, $f(x)\\leq p(x)$, $x\\in D_f$. In other words, $E$ is the set of all extensions $f$ of $f_W$ to general subspaces $D_f\\subset X$. We apply Zorn\u0026rsquo;s to the set $E$ ans show that the resulting maximal element of $E$ is the desired functional.\nFirst, we verity that $E$ satisfies the hypotheses of Zorn\u0026rsquo;s. $E$ is non-empty since $f_W\\in E$. Define a relation $\\prec$ on $E$ as follows: for any $f,g\\in E$, $$ f\\prec g \\Leftrightarrow D_f \\subset D_g, f(x) = g(x) ,\\; \\forall x\\in D_f. $$ In other words, $f\\prec g$ iff $g$ is an extension of $f$. Now suppose that $G\\subset E$ is totally ordered. i.e. for any $f,g\\in G$, one of these functionals is an extension of the order. We will construct an upper bound for $G$ in $E$.\nDefine $Z_G = \\cup_{f\\in G} D_f$. It can be verified that $Z_G$ is a linear subspace of $X$. We now define a linear functional $f_G$ on $Z_G$ as follows:\nchoose $z\\in Z_G$, then there exists $\\xi\\in E$ s.t. $z\\in D_\\xi$, define $f_G(z) = \\xi(z)$ By the total ordering of $G$, we can see that $f_G$ is linear. And since $\\xi\\in E$, we have $f_G(z) = \\xi(z)\\leq p(z)$, and if $z\\in W$ then $f_G(z) = f_W(z)$. Hence $f_G\\in E$ and $f\\prec f_G$ for all $f\\in G$. Thus $f_G$ is an upper bound for $G$.\nSo far we have verifies that all conditions of Zorn\u0026rsquo;s are satisfied, then we can conclude that there exists a maximal element $f_{\\max}$ in $E$. If $D_{f_{max}}\\neq X$, by Lemma 4.9 $f_{max}$ has an extension, which also lies in $E$. However, this contradicts the maximality of $f_{max}$ in $E$, so $D_{f_{max}} = X$, and hence $f_X = f_{max}$ is the desired extension.\n$\\Box$\n4. The Second Dual Throughout this section $X$ will be a $\\color{orange}{\\text{normed linear space}}$. Given $X$ a normed linear space, by Lemma 3.7 we see that $X'$ is a Banach space, so $X'$ itself also has a dual space $(X')'$, denoted by $X''$, which is called the second dual of $X$.\nLemma 16 For any $x\\in X$, defined $F_x:X'\\to \\mathbb{F}$ by $$ F_x(f) = f(x),\\; f\\in X'. $$ Then $F_x\\in X''$ and $\\|F_x\\| = \\|x\\|$. proof Clearly, $F_x$ is linear. Moreover, $$ \\|F_x(f)\\| = \\|f(x)\\|\\leq \\|f\\| \\|x\\| , $$ and so $F_x\\in X''$ with $\\|F_x\\|\\leq \\|x\\|$. Finally by Theorem 4.11, $$ \\|x\\| = \\sup_{\\|f\\| = 1} |f(x)| = \\sup_{\\|f\\| = 1}|F_x(f)| \\leq \\|F_x\\|, $$ which completes the proof. $\\Box$\nThen we can define a linear transformation $J_X:X\\to X''$ by $$ J_X x = F_x. $$According to Lemma 4.16, $J_X$ is an isometry. Moreover, if $J_X(X) = X''$, we say that $X$ is replexive . In other words, $X$ is reflexive iff, for any $g\\in X''$, there exists $x_g\\in X$ s.t. $g = J_X x_g$, i.e. $g(f) = J_Xx_g(f) = f(x_g)$, for any $f\\in X'$. It follows that if $X$ is reflexive, then $J_X$ is an isometric isomorphism of $X$ onto $X''$ .\nFrom the definition of reflexive, we see that the only normed linear spaces that can be reflexive are Banach spaces. However, not all Banach spaces are reflexive.\nTheorem 17 (Reflexive and Banach) A Banach space $X$ is reflexive iff $X'$ is reflexive. proof Suppose that $X$ is reflexive. Let $\\rho\\in X'''$. Then $\\rho \\circ J_X\\in X'$. Let $f=\\rho\\circ J_X$ and let $g\\in X''$. Since $X$ is reflexive, there exists $x\\in X$ s.t. $g = J_X x$, and so $$ (J_{X'}f)(g) = g(f) = f(x) = \\rho\\circ J_X (x) = \\rho(g). $$ Hence $\\rho = J_{X'}f$, so $X'$ is reflexive. Conversely, suppose that $X'$ is reflexive but there exists $w\\in X''\\setminus J_X(X)$. Since $J_X(X)$ is closed, by Theorem 4.11 there exists $\\kappa\\in X'''$ s.t. $\\kappa(w) \\neq 0$ while $\\kappa(J_X x) = 0$ for all $x\\in X$. Since $X'$ is reflexive there exists $g\\in X'$ s.t. $\\kappa = J_{X'}(g)$. Thus $$ g(x) = (J_Xx)(g) = \\kappa(J_X x) = 0,; x\\in X, $$ and so $g = 0$. Since $w(g) = \\kappa(w)\\neq 0$, which is a contradiction, hence $X$ is reflexive.\n$\\Box$\nTheorem 18 Let $X,Y$ be two normed linear spaces and $T\\in B(X,Y)$. Then there exists a unique operator $T'\\in B(Y',X')$ s.t. $$ T'f(x) = f(Tx),\\; x\\in X, f\\in Y'. $$ proof For any $f\\in Y'$, define $T'f = f\\circ T$. Since $T,f$ are bounded, $T'f \\in X'$, so that $T'$ is a function from $Y'$ to $X'$ s.t. $T'f(x) = f(Tx)$ for all $x\\in X,f\\in Y'$. Furthermore, if $S\\in B(Y',X')$ satisfies $Sf(x) = f( Tx)$, then $Sf - T'f$ for all $y\\in Y'$. Therefore $T'$ is unique. It remains to show that $T'\\in B(Y', X')$. It clear that $T'$ is linear. And since $T'$ is a composition of two bounded operator, $T'$ is also bounded. The proof is complete.\n$\\Box$\nThe operator $T'\\in B(Y', X')$ constructed in this theorem is called the dual of $T$.\nExample 18 (Reflexive and Hilbert) Any Hilbert space $\\mathcal{H}$ is reflexive. proof By Theorem 4.3, $\\mathcal{H}'$ is a Hilbert space, so the mapping $T_{\\mathcal{H}'}:\\mathcal{H}'\\to \\mathcal{H}''$ is well defined, and is a bijection. In particular, for any $f\\in \\mathcal{H}'$ and $g\\in \\mathcal{H}''$, they have the form $f = T_{\\mathcal{H}}x$ and $g = T_{\\mathcal{H}'}(T_{\\mathcal{H}}y )$ for unique $x,y\\in \\mathcal{H}$ (see Riesz-Frechet). Now we have $$ J_{\\mathcal{H}}y(f) = f(y) = \\langle y,x\\rangle_{\\mathcal{H}} = \\langle T_{\\mathcal{H}}x, T_{\\mathcal{H}}y \\rangle_{\\mathcal{H}'} = \\langle f, T_{\\mathcal{H}}y\\rangle_{\\mathcal{H}'}, $$ which is equal to $T_{\\mathcal{H}'}(T_{\\mathcal{H}}y)(f) = g(f)$, so $J_{\\mathcal{H}}y = g$. Thus, $J_{\\mathcal{H}}$ is onto, and hence $\\mathcal{H}$ is reflexive. $\\Box$\n5. Weak and Weak-* Convergence We have seen that closed bounded sets are compact in finite dimensional spaces. Unfortunately, this is not true in infinite dimensional spaces. However, if this section we will show that a partial analogue of this result can be obtained in infinite dimensions if the adopt a weaker condition of the convergence.\nThroughout this section, $X$ will be a $\\color{orange}{\\text{Banach space}}$.\nLemma 19 $S = \\{s_\\alpha: \\alpha\\in A\\}\\subset X$, s.t. $\\overline{sp}S = X$. $\\{f_n\\}\\subset X'$ with $\\|f_n\\|\\leq C$ for some $C$ s.t. $f_n(s_\\alpha)$ converges for all $\\alpha\\in A$. Then there exists $f\\in X'$ s.t. $f_n (x)\\to f(x)$ for all $x\\in X$. proof By the completeness of $X$ and $X'$. $\\Box$\nThe following results provides a motivation for the new idea of convergence.\nLemma 20 $X$ is separable. $\\{s_k\\}$ is a dense sequence in $X$ with $s_k\\neq 0$ for all $k$. Then the function function $d_w:X'\\times X'\\to \\R$ defined by $$ d_w(f,g) = \\sum_{k} \\frac{1}{2^k} \\frac{|f(s_k) - g(s_k)|}{\\|s_k\\|},\\; f,g\\in X', $$ is a metric on $X'$. If $\\{f_n\\}\\subset X'$ and $f\\in X'$, the following are equivalent: (a). there exists $C\u003e0$ s.t. $\\|f_n\\|\\leq C$ for all $n$ and $d_w(f_n,f)\\to 0$. (b). $f_n(x)\\to f(x)$ for all $x\\in X$.\nproof The metric is easy to verity. If condition (a) holds, then it is clear that $f_n(s_k)\\to f(s_k)$. And by the density of $\\{s_k\\}$, we have $f_n(x)\\to f(x)$ for all $x\\in X$. Next, if (b) holds, the boundedness assertion is trivial as $\\|f\\|\\leq C$. Also, $d_w(f_n,f)\\to 0$ is easy. W.l.o.g. we assume that $f = 0$, then $$ d_w(f_n,f) \\leq \\sum_{k} \\frac{1}{2^k} \\|f_n\\|\\leq \\sum_{k} \\frac{1}{2^k} C\u003c\\infty. $$ It follows that there exists $K$ s.t. $\\sum_{k\\geq K} \\frac{1}{2^k} \\|f_n\\|\u003c\\epsilon.$ Together with $f_n \\to 0$, there exists $N$ s.t. $\\|f_n\\|\u003c\\epsilon/K$ whenever $n\\geq N$. It follows that $d_w(f_n,0)\\to 0$.\n$\\Box$\nDefinition 21 (*Weak and weak- convergence) For any Banach space $X$, let $\\{x_n\\},\\{f_n\\}$ be sequences in $X,X'$, respectively. $\\{x_n\\}$ is weakly convergent to $x\\in X$ if $f(x_n)\\to f(x)$ for all $f\\in X'$, denoted by $x_n\\rightharpoonup x$. $\\{f_n\\}$ is weak-* convergent to $f\\in X'$ if $f_n(x)\\to f(x)$ for all $x\\in X$, denoted by $f_n\\overset{*}{\\rightharpoonup}f$. Notice that the separability is not involved in this definition. We also remark at this point that the idea of weak convergence makes sence in $X'$, by using functionals in $X''$. If X $X$ is reflexive then weak and weak-*convergence in $X'$ coincide.\nWe have seen that an arbitrary Hilbert space $\\mathcal{H}$ is reflexive (eg reflexive Hilbert), and also $\\mathcal{H}$ can be identified with its dual $\\mathcal{H}'$, by Riesz-Frechet, so weak and weak-*convergence coincide.\nTheorem 22 (Compactness) If $x$ is separable and $B = \\{f\\in X': \\|f\\|\\leq 1\\}$, then any sequence in $B$ has a subsequence which is weak-* convergent to an element of $B$ (B is compact w.r.t. $d_w$). proof First, we show that any bounded sequence $\\{f_n\\}\\subset X'$ has a weak-* convergent subsequence if $X$ is separable. Let $\\{s_k\\}$ be a dense sequence in $X$. Since the sequence $\\{f_n(s_1)\\}$ is bounded, it has a convergent subsequence $\\{f_{n,1}(s_1)\\}$. Similarly, the the sequence $\\{f_{n,1}(s_2)\\}$ has a convergent subsequence $\\{f_{n,2}(s_2)\\}$, and so on. Finally, the diagonal subsequence $\\{f_{n,n}\\}$ in $X'$ is bounded and $\\{f_{n,n}(s_k)\\}$ converges for every $k$, so it is weak-* convergent.\nNext, for any sequence $\\{f_n\\}\\subset B$, it has a subsequence weak-*converging to $f'\\in X'$. Now, $$ |f(x)| = \\lim_{n\\to\\infty}|f_{n,1}(x) |\\leq \\|x\\|, \\; x\\in X . $$ so that $f\\in B$. Also, Lemma 4.20 (a) shows that $d_w(f_n,f)\\to 0$.\n$\\Box$\nTheorem 23(Reflexive and weak-*convergence) If $X$ is reflexive and $\\{x_n\\}$ is bounded sequence in $X$, then $\\{x_n\\}$ has weakly convergent subsequence. proof Let $Y= \\overline{sp}\\{x_1,x_2,...\\}$. Then $Y$ is separable and reflexive (closed subspace of reflexive is reflexive). Thus $Y''$ is separable ($J_Y(Y)= Y''$ is invertible), and hence $Y'$ is separable (Theorem 4.13). Now, $\\{J_Y x_n\\}$ is bounded sequence in $Y''$, so we may suppose that $\\{J_Y x_n\\}$ is weak-* convergent in $Y''$ (taking subsequence if necessary) to a limit of the form $J_Y y$ for some $y\\in Y$ (by reflexive). Now for any $f\\in X'$, the restriction $f_Y\\in Y'$, and so by the weak-* convergence in $Y''$, $$ \\lim_{n\\to\\infty} f(x_n) = \\lim_{n\\to\\infty} J_Yx_n(f_Y) = J_Yy(f_Y) = f(y),\\; f\\in X', $$ which shows that $x_n\\to y$. $\\Box$\n","permalink":"http://localhost:1313/posts/analysis_3/functional_4/","summary":"\u003ch2 id=\"1-dual-spaces\"\u003e1. Dual Spaces\u003c/h2\u003e\n\u003cp\u003eIn this section we describe the dual of several of the standard spaces.\u003c/p\u003e\n\u003cdl\u003e\n\u003cdt\u003eTheorem 1 \u003cspan id = \"c4_functional_norm\"\u003e\u003c/dt\u003e\n\u003cdd\u003eIf $X$ is a finite-dimensional normed linear space with basis $\\{v_1,...,v_n\\}$, then $X'$ has a basis $\\{f_1,...,f_n\\}$ s.t. $f_j(v_k) = \\delta_{jk}$. In particular, $dim (X') = dim (X)$.\u003c/dd\u003e\n\u003cdt\u003eproof\u003c/dt\u003e\n\u003cdd\u003eLet $x\\in X$, we can represent is as $x = \\sum_{k=1}^n \\alpha_k v_k$. Define $f_j:X\\to\\mathbb{F}$ by\n$$f_j(x) = \\alpha_j.$$\nIt can be verified that $f_j$ is a linear transformation s.t. $f_j(v_k) = \\delta_{jk}$. Moreover, $f_j$ is continuous since $X$ is finite-dimensional, and thus $f_j\\in X'$.\n\u003cp\u003eNext, we show that $\\{f_1,...,f_n\\}$ is a basis for $X'$. Suppose that $\\beta_1,...,\\beta_n$ are scalars s.t. $\\sum_{j=1}^n \\beta_j f_j = 0$. Then\n\u003c/p\u003e","title":"Functional Analysis | 4 Duality and Hahn-Banach Theorem"},{"content":"1. Continuous Linear Transformations Lemma 1 (Continuity) Let $X$ and $Y$ be two normed linear spaces and let $T:X\\to Y$ be a linear transformation. The following are equivalent: T is uniformly continuous, $T$ is continuous, $T$ is continuous at $0$, there exists a number $M$ s.t. $\\|T(x)\\|_Y\\leq M$ whenever $x\\in X$ with $\\|x\\|_X\\leq 1$, there exists a number $M$ s.t. $\\|T(x)\\|_Y \\leq M\\|x\\|_X$ for all $x\\in X$. proof $1 \\Rightarrow 2\\Rightarrow 3$ are trivial. We prove the left assertions only. $3\\Rightarrow 4$. As $T$ is continuous at $0$, there exist $\\delta$ s.t. $\\|T(x)\\|_Y \u003c 1$ whenveer $\\|x\\|_X \u003c\\delta$. Let $w\\in X$ and $\\|w\\|_X \\leq 1$. As $$ \\| \\delta w/2 \\|_X \\leq \\delta/2\u003c\\delta, $$ $\\|T(\\delta w/2)\\|_Y\u003c1$ and as $T$ is linear, it implies that $ \\|T(w)\\|_Y\u003c 2/\\delta$. We draw the conclusion by taking $M = 2/\\delta$. $4 \\Rightarrow 5$. Since $T(0) = 0$, it is clear that $\\|T(0)\\|_Y \\leq M \\|0\\|_X$. For any $w\\in X, w\\neq 0$, we have $\\|T(\\frac{w}{\\|w\\|_X}) \\|_Y \u003c M$. By linearity again we have $$ \\|T(W)\\|_Y \\leq M \\|w\\|_X. $$$5\\Rightarrow 1$. By linearity and assumption, $\\|T(x) - T(w)\\|_Y = \\| T(x-w)\\|_Y \\leq M\\|x-w\\|_X$. For any $\\epsilon$, we take $\\delta = \\epsilon/M$, then the result is clear.\n$\\Box$\nDefinition 2 (Bounded linear transformation) A linear transformation $T:X\\to Y$ is bounded if there exists a number $M$ s.t. $$ \\|T(x)\\|_Y \\leq M \\|x\\|_X, \\; \\forall x\\in X. $$ In view of Lemma 3.1, we see that continuous and bounded are equivalent for linear transformations. Let $X,Y$ be normed spaces. The set of all continuous linear transformations from $X$ to $Y$, denoted by $B(X,Y)$, is called the set of bounded linear operators or linear operators.\nTheorem 3 (Continuity and finite-dimensional normed space) Let $X$ be a finite-dimensional normed space, let $Y$ be any normed linear space, and let $T:X\\to Y$ be a linear transformation. Then $T$ is continuous. proof We define a norm $\\|\\cdot\\|_1 $ by $\\|x\\|_1 = \\|x\\| + \\|T(x)\\|_Y$. Because $X$ is finite-dimensional, $\\|\\cdot\\|$ and $\\|\\cdot\\|_1$ are equivalent, so that there exists $M$ s.t. $\\|x\\|_1 \\leq M\\|x\\|$. Hence $\\|T(x)\\|_Y\\leq M\\|x\\|$ for all $x\\in X$. $\\Box$\nThis theorem says that if the domain of a linear transformation is finite-dimensional then the linear transformation is continuous. However, if the range is finite-dimensional, the conclusion is not true.\nThe kernel of a linear transformation $T\\in L(X,Y)$ is defined by $Ker(T) = \\{x\\in X: T(x) = 0\\}$. Note that if $T\\in B(X,Y)$ then $Ker(T)$ is closed because $\\{0\\}$ is closed in $Y$.\nDefinition 4 (Graph) Let $X,Y$ be normed spaces, and let $T\\in L(X,Y)$. The graph of $T$ is the linear subspace $\\mathcal{G}(T)$ of $X\\times Y$ defined by $$ \\mathcal{G}(T) = \\{(x,Tx):x\\in X\\}. $$ We can show that $\\mathcal{G}(T)$ is closed if $T\\in B(X,Y)$. Let $(x_n,y_n)\\in \\mathcal{G}(T)$ s.t. $(x_n,y_n)\\to (x,y)$ for some $(x,y)\\in X\\times Y$. Then the conclusion is an immediate result of the continuity of $T$. Also, it is easily checked that $B(X,Y)$ is a liner subspace of $L(X,Y)$ and thus $B(X,Y)$ is a vector space.\n2. The Norm of $B(X,Y)$ In this Section we want to study if $B(X,Y)$ is a normed space.\nLemma 4 (Norm on B(X,Y)) Let $X,Y$ be normed spaces. The function $\\|\\cdot\\|: B(X,Y) \\to \\R$ defined by $$ \\|T\\| = \\sup_{x:\\|x\\|\\leq 1} \\|T x\\|_Y $$ is a norm on $B(X,Y)$. By Lemma 3.1, the norm can be rewritten as $\\|T\\| = \\inf\\{M: \\| Tx\\|_Y \\leq M \\|x\\|_X \\;\\forall x\\in X\\}$.\nproof Let $S,T\\in B(X,Y)$ and $\\lambda\\in\\mathbb{F}$. $\\|T\\|\\geq 0$ is clear, $\\| T \\| = 0 \\Leftrightarrow \\| Tx\\|_Y=0$ for all $x\\in X$ $\\Leftrightarrow$ $T$ is the zero linear transformation, as $\\| Tx\\|_Y \\leq \\|T\\| \\|x\\|_X$ by Lemma 3.1, we have $\\|\\lambda T \\| \\leq |\\lambda| \\|T\\|$. If $\\lambda= 0$, the equality is achieved. If $|\\lambda|\u003e0$, $$ \\| T\\| = \\|\\lambda^{-1} \\lambda T\\| \\leq \\|T\\|, $$ $\\| (S+T)x\\|_T \\leq \\|Sx\\|_Y + \\|Tx\\|_Y\\leq (\\|S\\| + \\|T\\|)\\|x\\|_X$ . Therefore, $\\|S+T\\|\\leq \\|S\\| + \\|T\\|$. $\\Box$\nExample 5 ($T: C([0,1])\\to \\mathbb{F}$) Let $f\\in C([0,1])$. The linear transformation $T f = f(0)$ is continuous, because $|T f|= |f(0)|\\leq \\sup_x |f(x)| = \\|f\\|$. Also, it follows that $\\|T\\|\\leq 1$ since\n$$ \\|T\\| = \\inf\\{M: |T f| \\leq M \\|f\\| \\;\\forall f\\in C[0,1]\\}. $$ On the other hand, the function $g$ on $[0,1]$ defined by $g(x) = 1$ is a member of $C[0,1]$, hence $\\|T\\|\\geq 1$. Therefore $\\|T\\| =1$. Since the norm of an operator is defined in terms of supremum, the norm can sometimes be hard to find, even if $X$ is finite-dimensional. But sometimes it is possible to use the norm of one operator to find that of another.\nTheorem 6 Let $X$ be a normed linear space and let $W\\subset X$ be a dense subspace. let $Y$ be a Banach space and let $S\\in B(W,Y)$. If $x\\in X$ and $\\{x_n\\} , \\{y_n\\}$ are sequences in $W$ converging to the same limit $x$, then $\\{S x_n\\}$ and $\\{S y_n\\}$ both converge to the same limit in $Y$. There exists $T\\in B(X,Y)$ s.t. $\\|T\\| = \\|S\\|$ and agrees with $S$ in $W$, i.e. $Tx = Sx$ for all $x\\in W$. proof $x_n$ is Cauchy, then so is $\\{S x_n\\}$, and it is convergent because $Y$ is a Banach space. Same results apply to $y_n$. We obtain the assertion by $$ \\| S x_n - S y_n\\| = \\| S (x_n - y_n) \\|\\leq \\|S\\| \\|x_n - y_n\\|\\to 0. $$ Since $W$ is dense, for any $x\\in X$, we can find a sequence in $W$ s.t. $x_n \\to x$. Now we define $T$ by $$ T x = \\lim_{n\\to\\infty} S x_n. $$ $\\|T\\|\\leq \\|S\\|$ since $\\|Tx\\| = \\lim_{n\\to\\infty} \\|S x_n\\|$. Moreover if $w\\in W$, then we can set the constant sequence $\\{w_n\\}$ in $W$ converging to $w$, and so $T w = S_w$. Thus $\\|S w\\| = \\| T w\\| \\leq \\|T\\| \\|w\\|$. Hence $\\|S\\|\\leq \\|T\\|$. $\\Box$ Lemma 7 (Complete codomain) If $X$ is a normed linear space and $Y$ is a Banach space, then $B(X,Y)$ is a Banach space. proof We need to show that $B(X,Y)$ is complete. Let $T_n$ be a Cauchy sequence in $B(X,Y)$, then $\\{T_n\\}$ is bounded, i.e. $\\| T_n\\|\\leq M$. Since $$ \\|T_n x - T_m x\\| = \\| (T_n -T_m)x\\|\\leq \\| T_n - T_m \\| \\| x\\|\\to 0, $$ $\\{T_n x\\}$ is also a Cauchy sequence in $Y$, so it converges to some $y\\in Y$. Next, we, we show that $y = Tx$ for some $T\\in B(X,Y)$ . Let $T x = \\lim_{n\\to\\infty} T_n x$. It can be checked that $T$ is linear and $\\|T x\\| \\leq M\\|x\\|$, so $T\\in B(X,Y)$.\nFinally, we show that $T_n \\to T$. $\\| Tx - T_m x\\| \\to 0$.\n$\\Box$\nThis Lemma shows that only the completeness of $Y$ matters.\nDefinition 7 (Functionals and dual space) Let $X$ be any normed space, and let $Y = \\mathbb{F}$. Then $T\\in L(X,Y)$ is called a linear functional, and $B(X,Y)$ is called the dual space of $X$, denoted by $X'$. Since $\\mathbb{F}$ is complete, $X'$ is a Banach space by Lemma 3.7.\nDefinition 8 (Isometry) Let $X,Y$ be normed linear spaces and let $T\\in L(X,Y)$. If $\\|T x\\| = \\|x\\|$ for all $x\\in X$, then $T$ is called an isometry. Moreover, if if the isometry $T$ is bijective, $X$ and $Y$ are called isometrically isomorphic, and $T$ is called an isometric isomorphism. On every normed space there is at least one isometry. And it is clear that $\\|T\\| = 1$ if $T$ is an isometry.\nTheorem 8 Let $\\mathcal{H}$ be an infinite-dimensional, separable Hilbert space over $\\mathbb{F}$ with an orthonormal basis $\\{e_n\\}$. Then is an isometry $T$ from $\\mathcal{H}$ to $\\ell^2$ s.t. $T e_n = \\tilde{e}_n$, where $\\tilde{e}_n$ is the standard orthonormal basis in $\\ell^2$ . proof For $x\\in\\mathcal{H}$, it can be represented as $x = \\sum_{n=1}^\\infty \u003c x, e_n\u003e e_n$. Write $\\alpha_n := \\langle x, e_n \\rangle$, then $\\{\\alpha_n\\}\\in\\ell^2$ by Bessel\u0026rsquo;s inequality. So we can define a linear transformation $T$ by $T x = \\{a_n\\}$. Now $$ \\|T x\\|^2 = \\|x\\|^2. $$ So $T$ is an isometry. Also by definition $T e_n = \\tilde{e}_n$. $\\Box$\n3. Inverses of Operators In this Section we introduce some methods of determining invertibility of an operator, one is based on the norm, the second one is open mapping theorem, and the third one is based on denseness.\nDefinition 9 (Inverse operator) Let $X,Y$ be normed linear spaces. An operator $T\\in B(X,Y)$ is said to be invertible if there exists $S\\in B(Y,X)$ s.t. $ST = I_X$ and $TS = I_X$, in which case $S$ is the inverse of $T$, denoted by $T^{-1}$. Definition 10 (Isomorphic) Let $X,Y$ be normed linear spaces. If there exists $T\\in B(X,Y)$ that is invertible, then $X,Y$ are isomorphic, and $T$ is an isomorphism. Lemma 11 If the normed linear spaces $X,Y$ are isomorphic, then $dim(X) \u003c\\infty \\Leftrightarrow dim(Y)\u003c\\infty$, in which case $dim(X) = dim(Y)$. $X$ is separable iff $Y$ is separable. $X$ is complete iff $Y$ is complete. Theorem 12 (1st, norm) Let $X$ be a Banach space. If $T\\in B(X)$ is an operator with $\\|T\\| \u003c 1$, then $I- T$ is invertible and the inverse is given by $$ (I-T)^{-1} = \\sum_{n=0}^\\infty T^n. $$ proof Because $X$ is a Banach space, so is $B(X)$ by Lemma 3.7. Since $\\| T\\|\u003c1$, the series $\\sum \\| T \\|^n$ converges, and as $\\| T^n \\| \\leq \\| T\\| ^n$ the series $\\sum \\| T^n \\|$ also converges. Therefore the series $\\sum T^n$ converges. Let $S = \\sum T^n$ and $S_k = \\sum_{n=0}^k T^n$. We see that $S_k \\to S$. Now $\\|(I-T)S_k - I \\| = \\| I - T^{k+1} - I \\| \\leq \\| T\\|^{k+1}\\to 0$. It follows that $(I-T)S = I$. Similarly, $S(I-T) = I$.\n$\\Box$\nCorollary 13 Let $X,Y$ be Banach spaces. The set $\\mathcal{A}$ of invertible operators in $B(X,Y)$ is open. proof Let $T\\in \\mathcal{A}$, and let $\\eta = \\|T^{-1}\\|^{-1}$. It suffices to show that if $\\|T -S\\|\u003c\\eta$ then $S\\in \\mathcal{A}$. $$ \\| (T-S) T^{-1}\\| \\leq \\| T -S\\| \\|T^{-1}\\|\u003c 1 $$ By Theorem 3.12, $I_Y - (T-S) T^{-1} = ST^{-1}$ is invertible. So $S = ST^{-1} T$ is invertible. $\\Box$\nTheorem 14 (2nd, Open mapping theorem) Suppose that $X,Y$ are Banach spaces and $T\\in B(X,Y)$ is surjective. Let $$ L = \\{Tx: x\\in X , \\|x\\|\\leq 1\\}, $$ with closure $\\overline{L}$. Then there exists $r\u003e0$ s.t. $\\{y\\in Y: \\|y\\|\\leq r\\}\\subset \\overline{L}$, $\\{y\\in Y: \\|y\\|\\leq r/2\\} \\subset L$, if, in addition, $T$ is bijective then $T$ is invertible. proof Let $U(r), V(r)$ be open balls with center $0$ and radius $r$ in the space $X,Y$, respectively. For any $y\\in Y$, there exists $x\\in X$ s.t. $T x = y$. Thus $y\\in \\|x\\| L$ and so $$ Y = \\cup_n n \\overline{L} $$ Therefore, by Baire category theorem, there is $N\\in\\N$ s.t. $N\\overline{L}$ contains an open ball, and hence $\\overline{L}$ also contains an open ball. Therefore, there exists $p\\in \\overline{L}$ and $t\u003e0$ s.t. $$ p + V(t) \\subset \\overline{L}. $$ Then for any $y\\in V(t)$, we have $y\\in \\overline{L}$ because $y+p,y-p \\in \\overline{L}$. This suggests that $V(t)\\subset \\overline{L}$. We finish the proof by taking $r = t/2$.\nlet $y\\in \\overline{V}(r/2)$. Since $\\overline{V}(r)\\subset \\overline{L}$, there exits $w_1\\in L$ s.t. $$ \\|2y - w_1 \\|\u003c r/2 $$ Since $2^2 y - 2 w_1 \\in \\overline{V}(r)\\subset \\overline{L}$, there exits $w_2\\in L$ s.t. $$ \\| w^2 y - 2w_1 - w_2 \\| \u003c r/2. $$ Continuing in this way, we obtain a sequence $\\{w_n\\}$ is $L$ s.t. $$ \\| w^2 y - w^{n-1}w_1 - \\cdots - w_n\\| \u003c r/2. $$ Therefore, $\\| y - \\sum_{j=1}^n 2^{-j} w_j\\|\u003c 2^{-n-1} r\\to 0$. Hence $y = \\sum_{j=1}^\\infty 2^{-j} w_j$. Also, there exists $x_n\\in \\overline{U}(1)$ s.t. $w_n = Tx_n$. As $\\sum_{j=1}^n 2^{-j} x_j$ is Cauchy, it converges to $x\\in \\overline{U}(1)$. Also, $$ Tx = T \\sum_{j=1}^\\infty 2^{-j} x_j = \\sum_{j=1}^\\infty 2^{-j} Tx_j = y. $$ Therefore $y\\in L$. The proof is complete.\nThere exists a unique $S\\in L(Y,X)$ s.t. $S T = I_X, TS = I_Y$. We need to show that $S$ is bounded. Let $y\\in Y$ and $\\|y\\|\\leq 1$, and let $w = ry/2$. As $w \\in \\overline{V}(r/2)$, by part 2. we have that $y = T (2x/r)$ for some $x\\in X, \\|x\\|\\leq 1$. So $\\|Sy\\|\\leq 2/r$. Thus $S$ is bounded.\n$\\Box$\nCorollary 15 (Closed graph theorem) Let $X,Y$ be Banach spaces, and $T\\in L(X,Y)$ s.t. the graph $\\mathcal{G}(T)$ is closed, then $T\\in B(X,Y)$. proof As $X\\times Y$ is Banach, $\\mathcal{G}(T)$ is Banach since it is a closed subspace of $X\\times Y$. let $R:\\mathcal{G}(T)\\to X$ be defined by $$ R(x,Tx) = x. $$ We can see that $R$ is bijective. Since $$ \\|R(x, Tx) \\| = \\|x\\| \\leq \\|(x,Tx)\\|. $$ we see that $R$ is bounded and $\\|R\\|\\leq 1$. By the opening mapping theorem (3), there exists $S\\in B(X,\\mathcal{G}(T))$ s.t. $S = T^{-1}$, in particular $Sx = (x, Tx)$. As $$ \\|Tx\\|\\leq \\|x\\| + \\|Tx\\| = \\|S x\\| \\leq \\|S\\| \\|x\\|, $$ it follows that $T$ is bounded. $\\Box$\nLemma 16 Let $X$ be a Banach space, $Y$ be a normed space, and let $T\\in B(X,Y)$. If there exits $\\alpha\u003e0$ s.t. $\\|T x\\|\\geq \\alpha \\|x\\|$ for all $x\\in X$, then $T(X)$ is closed. proof Let $y_n$ be a sequence in $T(X)$ converging to $y\\in Y$. There exists a sequence $x_n$ s.t. $T x_n = y_n$. Since $y_n$ converges, $$ 0\\leftarrow \\|y_n - y_m\\| = \\| T(x_n - x_m)\\| \\geq \\alpha \\|x_n - x_m\\| . $$ It follows that $x_n$ converges as well, write the limit $x\\in X$. By the continuity of $T$, we have $Tx = y\\in T(X)$. $\\Box$\nTheorem 17 (3rd, dense) Let $X,Y$ be Banach spaces, and $T\\in B(X,Y)$. The following are equivalent: $T$ is invertible, $T(X)$ is dense in $Y$, and there exists $\\alpha\u003e0$ s.t. $\\|Tx\\|\\geq \\alpha \\|x\\|$ for all $x\\in X$. proof $1\\Rightarrow 2$ is easy because $\\|x\\| = \\|T^{-1}T x\\|\\leq \\|T^{-1}\\| \\|Tx\\|$. $2\\Rightarrow 1$. By Lemma 3.16 $T(X)$ is closed and dense, hence $T(X) = Y$ (onto). For $x\\in Ker(T)$, $Tx = 0$, so $$ 0 =\\|Tx\\| \\geq \\alpha \\|x\\| \\Rightarrow x = 0. $$ It follows that $Ker(T) = \\{0\\}$ (one-to-one), so $T$ is bijective and thus invertible. $\\Box$\n","permalink":"http://localhost:1313/posts/analysis_3/functional_3/","summary":"\u003ch2 id=\"1-continuous-linear-transformations\"\u003e1. Continuous Linear Transformations\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eLemma 1 (\u003cstrong\u003eContinuity\u003c/strong\u003e) \u003cspan id = \"c3_lem_continuity\"\u003e\u003c/dt\u003e\n\u003cdd\u003eLet $X$ and $Y$ be two normed linear spaces and let $T:X\\to Y$ be a linear transformation. The following are equivalent:\n\u003col\u003e\n\u003cli\u003eT is uniformly continuous,\u003c/li\u003e\n\u003cli\u003e$T$ is continuous,\u003c/li\u003e\n\u003cli\u003e$T$ is continuous at $0$,\u003c/li\u003e\n\u003cli\u003ethere exists a number $M$ s.t. $\\|T(x)\\|_Y\\leq M$ whenever $x\\in X$ with $\\|x\\|_X\\leq 1$,\u003c/li\u003e\n\u003cli\u003ethere exists a number $M$ s.t. $\\|T(x)\\|_Y \\leq M\\|x\\|_X$ for all $x\\in X$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/dd\u003e\n\u003cdt\u003eproof\u003c/dt\u003e\n\u003cdd\u003e$1 \\Rightarrow 2\\Rightarrow 3$ are trivial. We prove the left assertions only.\n$3\\Rightarrow 4$. As $T$ is continuous at $0$, there exist $\\delta$ s.t. $\\|T(x)\\|_Y \u003c 1$ whenveer $\\|x\\|_X \u003c\\delta$. Let $w\\in X$ and $\\|w\\|_X \\leq 1$. As\n$$ \n   \\| \\delta w/2 \\|_X \\leq \\delta/2\u003c\\delta,\n  $$\n$\\|T(\\delta w/2)\\|_Y\u003c1$ and as $T$ is linear, it implies that $ \\|T(w)\\|_Y\u003c 2/\\delta$. We draw the conclusion by taking $M = 2/\\delta$.\n\u003cp\u003e$4 \\Rightarrow 5$. Since $T(0) = 0$, it is clear that $\\|T(0)\\|_Y \\leq M \\|0\\|_X$. For any $w\\in X, w\\neq 0$, we have $\\|T(\\frac{w}{\\|w\\|_X}) \\|_Y \u003c M$. By linearity again we have\n\u003c/p\u003e","title":"Functional Analysis | 3 Linear Operators"},{"content":"1. Inner Product spaces Definition 1 (Inner product) Let $X$ be a vector space. An inner product on $X$ is a function $\\langle\\cdot,\\cdot\\rangle:X\\times X\\to \\mathbb{F}$, s.t. for all $x,y,z\\in X$, $\\alpha,\\beta\\in\\mathbb{F}$, $\\langle x,x \\rangle\\geq 0$, $\\langle x,x \\rangle = 0 \\Leftrightarrow x = 0$ , $\\langle \\alpha x + \\beta y ,z\\rangle = \\alpha \\langle x,z \\rangle + \\beta \\langle y,z \\rangle$, $ \\langle x,y \\rangle = \\overline{\\langle y,x \\rangle}$. A vector space $X$ with an inner product $\\langle \\rangle$ is called an inner product space.\nWith an inner product, we can define a norm by $\\| x\\| = \\sqrt{\\langle x, x\\rangle}$, and hence we can define a metric $d$.\nLemma 2 (Continuity of inner product) let $X$ be an inner product space, and suppose that $\\{x_n\\}, \\{y_n\\}$ are convergent sequences $X$ with $x_n \\to x, y_n\\to y$. Then $$ \\langle x_n, y_n \\rangle \\to \\langle x, y\\rangle.\r$$ proof $$\r\\begin{align*}\r| \\langle x_n , y_n\\rangle - \\langle x , y\\rangle | \u0026\\leq | \\langle x_n , y_n\\rangle - \\langle x_n , y\\rangle | + | \\langle x_n , y\\rangle - \\langle x , y\\rangle | \\cr\r\u0026\\leq \\|x_n\\| \\|y_n - y\\| + \\|y\\| \\|x_n - x\\|\\to 0 \\end{align*}\r$$2. Hilbert Spaces Definition 3 (Hilbert space) An inner product space is called a Hilbert space if it is complete w.r.t. the metric associated with the norm induced by the inner product. Lemma 4 Let $\\mathcal{H}$ be a Hilbert and $Y\\subset \\mathcal{H}$ is a linear subspace, then $Y$ is a Hilbert space iff $Y$ is closed. proof A subset of a complete metric space is complete iff it is closed. $\\Box$\nLemma 5 Let $Y$ be a linear subspace of an inner product space $X$. Then $$ x\\in Y^\\perp \\Leftrightarrow \\| x-y\\| \\geq \\|x\\|,\\;\\forall y\\in Y.\r$$ proof ($\\Rightarrow$) $\\| x - y\\|^2 = \\|X\\|^2 + \\|y\\|^2$ if $x\\perp y$. $\\Leftarrow$ $\\|x - \\alpha y\\|^2 - \\|x\\|^2$ is minimized at $\\alpha = $.\n$\\Box$\nTheorem 6 (Projection theorem) Let $A$ be a closed linear subspace of a $\\mathcal{H}$. Then for any $x\\in \\mathcal{H}$, there exists a unique $x_0 \\in A$ s.t. $$ \\|x-x_a\\| = \\inf_{y\\in A} \\| x- y\\|,\r$$ and $x - x_a \\in A^\\perp$. proof Suppose $\\gamma = \\inf_{y\\in A} \\| x- y\\|$. We first prove the existence of $x_0$. For each $n$, there exists $x_n\\in A$ s.t. $$ \\|x - x_n\\|^2 \u003c \\gamma^2+ 1/n.\r$$ Applying the parallelogram rule to $x-x_n, x - x_m$, we can show that $\\{x_n\\}$ is Cauchy. So $x_n \\to x_0 , x_0 \\in A$. Then we obtain the equality by the continuity of inner product. Uniqueness. Suppose that $x'\\in A$ makes the equality. Then $(x_0, x')/2 \\in A$ by linearity, and $\\|x - (x_0, x')/2 \\in A\\|\\geq \\gamma$. Applying the parallelogram rule to $x - x', x-x_0$, we obtain that $\\|x_0 - x'\\|^2 = 0$.\nFinally, $x-x_0\\in A^\\perp$ follows from Lemma 2.5.\n$\\Box$\nLet $\\{e_n\\}$ a orthonormal system in an infinite-dimensional Hilbert space $X$, then for any $x\\in X$, $$ x = \\sum_{n=1}^\\infty \\langle x, e_n\\rangle e_n.\r$$ proof By Bessel\u0026rsquo;s inequality, we obtain that $ \\sum_{n=1}^\\infty |\\langle x, e_n\\rangle|^2\\langle\\infty$. Also we can show that $y_k = \\sum_{n=1}^k \\langle x, e_n\\rangle e_n$ is Cauchy and converges to $x$. $\\Box$\nLemma 7 (Bessel\u0026rsquo;s inequality) Let $X$ be an inner product space and let $\\{e_n\\}$ a orthonormal system in $X$. Then for any $x\\in X$ the series $\\sum_{n=1}^\\infty |\\langle x, e_n\\rangle|^2$ converges and $$ \\sum_{n=1}^\\infty |\\langle x, e_n\\rangle|^2 \\leq \\|x \\|^2.\r$$ proof Let $y_k = \\sum_{n=1}^k \\langle x, e_n\\rangle e_n$. Then $$ \\|x - y_k\\|^2 = \\|x\\|^2 - \\|y_k\\|^2.\r$$ It shows that $\\|y_k\\|^2 \\leq \\|x\\|^2$ for all $k\\in\\N$. In addition $\\|y_k\\|^2$ is increasing and bounded, so it is convergent. $\\Box$\nDefinition (Separability) A Hilbert space $\\mathcal{H}$ is called seperable if there is an at most countable orthonormal system $\\{e_n\\}$ s.t. $$ \\mathcal{H} = \\overline{sp}(\\{e_n\\}) .\r$$ ","permalink":"http://localhost:1313/posts/analysis_3/functional_2/","summary":"\u003ch2 id=\"1-inner-product-spaces\"\u003e1. Inner Product spaces\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 1 (\u003cstrong\u003eInner product\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eLet $X$ be a vector space. An inner product on $X$ is a function $\\langle\\cdot,\\cdot\\rangle:X\\times X\\to \\mathbb{F}$, s.t. for all $x,y,z\\in X$, $\\alpha,\\beta\\in\\mathbb{F}$,\n\u003col\u003e\n\u003cli\u003e$\\langle x,x \\rangle\\geq 0$,\u003c/li\u003e\n\u003cli\u003e$\\langle x,x \\rangle = 0 \\Leftrightarrow x = 0$ ,\u003c/li\u003e\n\u003cli\u003e$\\langle \\alpha x + \\beta y ,z\\rangle = \\alpha \\langle x,z \\rangle + \\beta \\langle y,z \\rangle$,\u003c/li\u003e\n\u003cli\u003e$ \\langle x,y \\rangle = \\overline{\\langle y,x \\rangle}$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eA vector space $X$ with an inner product $\\langle \\rangle$ is called an \u003ccode\u003einner product space\u003c/code\u003e.\u003c/p\u003e","title":"Functional Analysis | 2 Hilbert Spaces"},{"content":"1. Finite-dimensional Normed Spaces Definition 1 (Norm) Let $X$ be a vector space over $\\mathbb{F}$. A norm on $X$ is a function $\\|\\cdot\\|:X\\to\\R$ s.t. for all $x,y\\in X$ and $\\alpha\\in\\mathbb{F}$, $\\| x\\|\\geq 0$ , $\\|x\\|=0$ iff $x=0$, $\\| \\alpha x\\| = |\\alpha| \\| x\\|$, $\\|x+y\\|\\leq \\|x\\| + \\|y\\|$. A vector $X$ equipped with a norm is called a normed vector space or just a normed space. In particular, a unit vector in $X$ is a vector $x$ s.t. $\\|x\\|=1$.\nExample 2 (Norms on $\\mathbb{F}^n$) Let $x=(x_1,...,x_n)$, $\\|x \\| = \\left(\\sum_{i=1}^n |x_i|^2\\right)^{1/2}$ is a norm. Example 3 (Norms on $C(M)$) Let $M$ be a compact metric space and let $C(M)$ be the vector space of functions defined on $M$. Then the function $\\|\\cdot\\|:C(M)\\to\\R$ given by $$ \\|f\\| = \\sup_{x\\in M} |f(x)| $$ is a norm on $C(M)$. Example 4 (Norms on $L^p$) Let $(X,\\Sigma,\\mu)$ be a measure space. If $1\\leq p \u003c\\infty$, then $\\|f\\|_p = \\left(\\int |f|^p \\;d\\mu\\right)^{1/p}$ is a norm on $L^p(X)$. If $p=\\infty$, then $\\|f\\|_\\infty = \\mathrm{ess}\\sup |f|$ is a norm on $L^\\infty(X)$, where the essential supremum $\\mathrm{ess}\\sup$ of a measurable function $f$ is defined by $$ \\mathrm{ess}\\sup f = \\inf \\{b: f(x)\\leq b \\; a.e.\\} . $$ Lemma 5 (Norm and metric) Let $X$ be a vector space equipped with norm $\\|\\cdot\\|$. If $d: X\\times X\\to\\R$ is defined by $d(x,y) = \\|x-y\\|$, then $(X,d)$ is a metric space. Notice that there can be many different norms on each finite-dimensional space.\nDefinition 6 (Equivalence) Let $X$ be a vector space and let $\\|\\cdot\\|_1$ and $\\|\\cdot\\|_2$ be two norms on $X$. We say the norm $\\|\\cdot\\|_2$ is equivalent to the norm $\\|\\cdot\\|_1$ if there exists $M,m\u003e0$ s.t. for all $x\\in X$ $$ m\\|x\\|_1 \\leq \\|x\\|_2 \\leq M\\|x\\|_1. $$ Theorem 7 Let $X$ be finite-dimensional vector space with norm $\\|\\cdot\\|$ and let $\\{e_1,...,e_n\\}$ be a basis for $X$. We define another norm $\\|\\cdot\\|_1$ on $X$ by $$ \\begin{equation} \\lVert \\sum_{j=1}^n \\lambda_j e_j \\rVert_1 = \\left(\\sum_{i=1}^n |\\lambda_i|^2\\right)^{1/2}. \\end{equation} $$ Then norms $\\|\\cdot\\|$ and $\\|\\cdot\\|_1$ are equivalent. proof Let $M = \\left(\\sum_{i=1}^n \\|e_i\\|^2\\right)^{1/2}$. Then we have $$ \\begin{align*} \\| \\sum_{i=1}^n \\lambda_i e_i\\| \u0026\\leq \\sum_{i=1}^n \\|\\lambda_i e_i\\| \\cr \u0026=\\sum_{i=1}^n |\\lambda_i | \\|e_i\\| \\cr \u0026\\leq \\left(\\sum_{i=1}^n |\\lambda_i|^2\\right)^{1/2} \\left(\\sum_{i=1}^n \\|e_i\\|^2\\right)^{1/2} \\cr \u0026= M\\| \\sum_{j=1}^n \\lambda_j e_j\\|_1 \\end{align*} $$ Next, we define a function $f$ by $$ f(\\lambda_1,...,\\lambda_n) = \\| \\sum_{i=1}^n \\lambda_i e_i\\|. $$ The function $f$ is continuous w.r.t. the standard metric. Now we define $S$ be the unit circle, i.e. $$ S = \\{(\\lambda_1,...,\\lambda_n): \\sum_{i=1}^n |\\lambda_i|^2 = 1\\}. $$ Since $S$ is compact, there exists $(u_1,...,u_n)\\in S $s.t. $f(u_1,...,u_n)\\leq f(\\lambda_1,...,\\lambda_n)$ for all $(\\lambda_1,...,\\lambda_n)\\in S$. Let $m = f(u_1,...,u_n)$. Notice that $m\u003e0$. By the definition of $\\|\\cdot\\|_1$, if $\\|x\\|_1=1$ then $\\|x\\|\\geq m$. Therefore $y\\in X\\setminus\\{0\\}$, we have $\\| \\frac{y}{\\|y\\|_1}\\|\\geq m$ since $\\| \\frac{y}{\\|y\\|_1}\\|_1 = 1$. It follows that $$ \\|y\\|\\geq m \\|y\\|_1. $$ For $y=0$, it still holds.\n$\\Box$\nCorollary 8 (Equivalent norms \u0026amp; finite-dimensional) If $\\|\\cdot\\|$ and $\\|\\cdot\\|_2$ are any two norms on a finite-dimensional vector space $X$, then they are equivalent proof By Theorem 1.7, we can see that both $\\|\\cdot\\|$ and $\\|\\cdot\\|_2$ are equivalent to $\\|\\cdot\\|_1$. $\\Box$\nLemma 9 (Complete metric space \u0026amp; finite-dimensional) Let $X$ be finite-dimensional vector space with norm $\\|\\cdot\\|_1$ that is defined as (1) and let $\\{e_1,...,e_n\\}$ be a basis for $X$. Then $X$ is a complete metric space. proof Let $\\{x_m\\}$ be a Cauchy sequence. It implies when $k,m\\geq N$, $$ \\sum_{j=1}^n |\\lambda_{j,k} - \\lambda_{j,m}|^2 = \\|x_k - x_m \\|_1^2 \u003c\\epsilon. $$ Hence $\\{\\lambda_{j,m}\\}$ is a Cauchy sequence for $j\\leq n$. And since $\\mathbb{F}$ is complete, there exists $\\lambda_j\\in\\mathbb{F}$ s.t. $\\lambda_{j,m}\\to \\lambda$. Define $x = \\sum_{j=1}^n \\lambda_je_j$, we have $\\|x_m - x \\|_1^2\\to 0$. $\\Box$\nThis lemma means that for any norm on a finite-dimensional space $X$, $X$ is a complete metric space.\nCorollary 10 If $Y$ is a finite-dimensional subspace of a normed vector space $X$, then $Y$ is closed. proof Y is a complete metric space by Lemma 1.10. Hence $Y$ is closed since any complete subset of a metric space $X$ is closed. $\\Box$\n2. Banach Spaces In this Section we turn to infinite-dimensional vector spaces.\nLemma 11 If $X$ is a normed vector space and $S$ is a linear subspace of $X$ then $\\bar{S}$ is also a linear subspace of $X$. proof Let $x,y \\in\\bar{S}$, so we can find sequences $\\{x_n\\},\\{y_n\\}$ in $S$ s.t. $$ x_n\\to x, y_n\\to y. $$ Then $x+y\\in\\bar{S}$ and $\\alpha x \\in\\bar{S}$. $\\Box$\nLet $X$ be a normed vector space and $E\\subset X,E\\neq \\empty$. The closed linear span of $E$, denoted by $\\overline{sp}(E)$, is the intersection of all closed linear subspaces of $x$ which contain $E$.\nTheorem 12 (Riesz\u0026rsquo;s) Let $X$ is a normed vector space, $Y$ is closed linear subspace of $X$ s.t. $Y\\neq X$, and $\\alpha\\in(0,1)$. Then there exists $x_\\alpha\\in X$ s.t. $$ \\|x_\\alpha\\| = 1, \\quad \\|x_\\alpha - y\\|\u003e\\alpha, \\;\\forall y\\in Y. $$ proof Let $x\\in X\\setminus Y$. Also, since $Y$ is closed, $$ d = \\inf\\{\\|x-z\\|: z\\in Y\\} \u003e0. $$ Thus there exists $z\\in Y$ s.t. $\\|x-z\\|\u003c d \\alpha^{-1}$. Let $x_\\alpha = \\frac{x-z}{\\|x-z\\|}$. Then for any $y\\in Y$ $$ \\begin{align*} \\|x_\\alpha - y\\| \u0026= \\frac{1}{\\|x-z\\|} \\| x - (z - \\|x-z\\|y)\\|\\cr \u0026\u003e\\alpha d^{-1} d. \\end{align*} $$ $\\Box$\nTheorem 13 (Compactness \u0026amp; infinite-dimensional) Let $X$ be an infinite-dimensional normed vector space, then neither $D =\\{x\\in X: \\|x\\|\\leq 1\\}$ nor $K =\\{x\\in X: \\|x\\|= 1\\}$ is compact. proof Let $x_1\\in K$, then $sp(x_1)$ is closed. By Riesz\u0026rsquo;s, there exists $x_2\\in K$ s.t. $$ \\| x_2 - a x_1\\| \\geq 3/4, \\;\\forall a\\in\\mathbb{F}. $$ Go on with $sp(x_1,x_2)$, we have $x_3\\in K$ s.t. $$ \\| x_3 - a x_1 - b x_2\\|\\geq 3/4,\\; \\forall a,b\\in\\mathbb{F}. $$ Continuing this way, we obtain a sequence $\\{x_n\\}$ in $K$ s.t. $\\|x_n - x_m\\|\\geq 3/4$ when $n\\neq m$. This cannot have a convergent subsequence. $\\Box$\nRecall that in a finite-dimensional normed space, any closed bounded set is compact, but this is not true for infinite-dimensional spaces.\nDefinition 14 (Banach space) A Banach space is a normed vector space which is complete relative to the metric associated with the norm. Theorem 15 Any finite-dimensional normed vector space is a Banach space. if $X$ is a compact metric space then $C(X)$ is a Banach space. if $(X,\\Sigma,\\mu)$ is a measure space, then $L^p(X)$ is a Banach space for $1\\leq p\\leq \\infty$. $\\ell^p$ is a Banach space for $1\\leq p\\leq \\infty$. If $X$ is a Banach space and $Y$ is a linear subspace of $X$, then $Y$ is a Banach space $\\Leftrightarrow$ $Y$ is closed in $X$. proof Any finite-dimensional normed vector space is complete. $C(M)$ is complete. $L^p(X)$ is complete. $\\ell^p$ is a special case of $L^p$ by taking counting measure on $\\N$. A subset of a complete metric space is complete iff it is closed. $\\Box$ ","permalink":"http://localhost:1313/posts/analysis_3/functional_1/","summary":"\u003ch2 id=\"1-finite-dimensional-normed-spaces\"\u003e1. Finite-dimensional Normed Spaces\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 1 (Norm)\u003c/dt\u003e\n\u003cdd\u003eLet $X$ be a vector space over $\\mathbb{F}$. A \u003ccode\u003enorm\u003c/code\u003e on $X$ is a function $\\|\\cdot\\|:X\\to\\R$ s.t. for all $x,y\\in X$ and $\\alpha\\in\\mathbb{F}$,\n\u003col\u003e\n\u003cli\u003e$\\| x\\|\\geq 0$ ,\u003c/li\u003e\n\u003cli\u003e$\\|x\\|=0$ iff $x=0$,\u003c/li\u003e\n\u003cli\u003e$\\| \\alpha x\\| = |\\alpha| \\| x\\|$,\u003c/li\u003e\n\u003cli\u003e$\\|x+y\\|\\leq \\|x\\| + \\|y\\|$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eA vector $X$ equipped with a norm is called a \u003ccode\u003enormed vector space\u003c/code\u003e or just a \u003ccode\u003enormed space\u003c/code\u003e. In particular, a \u003ccode\u003eunit vector\u003c/code\u003e in $X$ is a vector $x$ s.t. $\\|x\\|=1$.\u003c/p\u003e","title":"Functional Analysis | 1 Normed Spaces"},{"content":"Functional Analysis | 0 Preliminaries Notation:\n$\\R$: the set of real numbers, $\\mathbb{C}$: the set of complex numbers, $\\N$: the set of positive integers. $\\mathbb{F} = \\{\\R,\\mathbb{C}\\}$, $\\Re_z$: the real part of complex number $z$, $\\Im_z$: the imaginary part of complex number $z$, $sp(A)$: span of set $A$. $dim V$: dimension of vector space $V$. 1. Linear Algebra Definition 1 (Vector space) A vector space over $\\mathbb{F}$ is a non-empty set $V$ together with two function $x+y:V\\times V\\to V, \\alpha x: \\mathbb{F}\\times V\\to V$ satisfying for all $x,y,z\\in V$ and all $\\alpha,\\beta\\in\\mathbb{F}$, $x+y = y+x$, there exists a unique $0\\in V$ s.t. $x+0 = x$, there exists a unique $-x\\in V$ s.t. $x + (-x) = 0$, $1x = x$, $\\alpha(x+y) = \\alpha x + \\alpha y, (\\alpha+\\beta)x = \\alpha x + \\beta x$. Definition 2 (Linear subspace) Let $V$ be a vector space. A non-empty set $U\\subset V$ is a linear subspace of $V$ if $U$ is itself a vector space, which is equivalent to the condition $$ \\alpha x + \\beta y \\in U, \\;\\forall \\alpha,\\beta\\in \\mathbb{F}, x,y\\in U.\r$$ Definition 3 Let $S$ be a set and let $V$ be a vector space over $\\mathbb{F}$. We denote the set of functions $f:S\\to V$ by $F(S,V)$. With the definition of scalar multiplication and vector addition, $F(S,V)$ is a vector space over $\\mathbb{F}$. Definition 4 (Linear transformation) Let $V,W$ be vector spaces over $\\mathbb{F}$. A function $T: V\\to W$ is called a linear transformation if, for all $\\alpha,\\beta\\in \\mathbb{F}$ and $x,y\\in V$ $$ T(\\alpha x + \\beta y) = \\alpha T(x) + \\beta T(y).\r$$ The set of all linear transformations $T:V\\to W$ is denoted by $L(V,W)$. With the definition of scalar multiplication and vector addition, $L(V,W)$ is a vector space. 2. Metric Spaces Definition 5 (Metric space) A metric on a set $M$ is a function $d:M\\times M\\to \\R$ with the following properties, $d(x,y)\\geq 0$, $d(x,y)= 0 \\Leftrightarrow x=y$, $d(x,y) = d(y,x)$, $d(x,z)\\leq d(x,y) + d(y,z)$. If $d$ is a metric on $M$, then the pair $(M,d)$ is called a metric space.\nGiven a metric space $(M,d)$ and a subset $U\\subset M$, the restriction of $d$ to the subset $N$, denoted by $d_N$, is called the the metric induced on $N$ by $d$.\nDefinition 6 (Continuity) Let $(M,d_M), (N,d_N)$ be two metric spaces and let $f:M\\to N$ be a function. $f$ is continuous at $x\\in M$, if for any $\\epsilon\u003e0$, there exists $\\delta\u003e0$ s.t. $y\\in M, d(x,y)\u003c\\delta$ implies $$ d_N(f(x),f(y))\u003c\\epsilon.\r$$ $f$ is continuous on $M$ if it is continuous at each $x\\in M$. $f$ is uniformly continuous on $M$, if for any $\\epsilon\u003e0$, there exists $\\delta\u003e0$ s.t. $d(x,y)\u003c\\epsilon$ implies $$ d_N(f(x),f(y))\u003c\\epsilon.\r$$ Definition 6 (Completeness) A metric space $(M,d)$ is complete if every Cauchy sequence in $(M,d)$ is convergent. A subset $A\\subset M$ is complete if every Cauchy sequence lying in $A$ converges to an element of $A$. Theorem 7 (Baire\u0026rsquo;s category theorem) If $(M,d)$ is a complete metric space and $M=\\cup_{j\\in\\N}A_j$, where $A_j\\subset M$, $j=1,2,...$, is closed, then at least one of the sets $A_j$ contains an open ball. Definition 8 (Compactness) Let $(M,d)$ be a metric space. A set $A\\subset M$ is compact if every sequence $\\{x_n\\}$ in $A$ contains a subsequence that converges to an element of $A$. The set $A$ is relatively compact if the closure $\\bar{A}$ is compact. If he set $M$ itself is compact, then we say that $(M,d)$ is a compact metric space. Compactness can also be defined in terms of open coverings, which is more appropriate in more general topological spaces. But in metric spaces both definitions are equivalent.\nTheorem 8 Suppose that $(M,d)$ is a metric space and $A\\subset M$. Then if $A$ is complete, then $A$ is closed, if $M$ is complete,then $A$ is complete $\\Leftrightarrow$ $A$ is closed, if $A$ is compact, then $A$ is closed and bounded, (Bolzano-Weierstrass theorem) every closed, bounded subset of $\\mathbb{F}$ is compact. A subset $A\\subset M$ is bounded if $d(x,y)\u003c M$ for all $x,y\\in A$.\nTheorem 9 Let $(M,d)$ be a compact metric space, and let $f:M\\to\\mathbb{F}$ be a continuous function. Then there exists $M\u003e0$ s.t. $f(x)\\leq M$ for all $x\\in M$ ($f$ is bounded), in particular, if $\\mathbb{F}=\\R$ then $\\sup_x f(x),\\inf_x f(x)$ exist and are finite, furthermore, there exist $x_s,x_i\\in M$ s.t. $f(x_s)=\\sup_x f(x),f(x_i)=\\inf_x f(x)$. Definition 9 Let $(M,d)$ be a compact metric space. The set of continuous functions $f:M\\to \\mathbb{F}$ will be denoted by $C(M)$. We define a metric on $C(M)$ by $$ d(f,g) = \\sup_{x\\in M} |f(x) - g(x)|.\r$$ The metric space $C(M)$ is complete.\nDefinition 10 (separable) A metric space $(M,d)$ is separable if it contains a countable, dense subset. Theorem 11 Suppose that $(M,d)$ is a metric space and $A\\subset M$. If $A$ is compact then it is separable. If $A$ is separable and $B\\subset A$ then $B$ is separable. ","permalink":"http://localhost:1313/posts/analysis_3/functional_0/","summary":"\u003ch1 id=\"functional-analysis--0-preliminaries\"\u003eFunctional Analysis | 0 Preliminaries\u003c/h1\u003e\n\u003chr\u003e\n\u003cp\u003eNotation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\R$: the set of real numbers,\u003c/li\u003e\n\u003cli\u003e$\\mathbb{C}$: the set of complex numbers,\u003c/li\u003e\n\u003cli\u003e$\\N$: the set of positive integers.\u003c/li\u003e\n\u003cli\u003e$\\mathbb{F} = \\{\\R,\\mathbb{C}\\}$,\u003c/li\u003e\n\u003cli\u003e$\\Re_z$: the real part of complex number $z$,\u003c/li\u003e\n\u003cli\u003e$\\Im_z$: the imaginary part of complex number $z$,\u003c/li\u003e\n\u003cli\u003e$sp(A)$: span of set $A$.\u003c/li\u003e\n\u003cli\u003e$dim V$: dimension of vector space $V$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-linear-algebra\"\u003e1. Linear Algebra\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 1 (\u003cstrong\u003eVector space\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eA \u003ccode\u003evector space\u003c/code\u003e over $\\mathbb{F}$ is a non-empty set $V$ together with two function $x+y:V\\times V\\to V, \\alpha x: \\mathbb{F}\\times V\\to V$ satisfying for all $x,y,z\\in V$ and all $\\alpha,\\beta\\in\\mathbb{F}$,\n\u003col\u003e\n\u003cli\u003e$x+y = y+x$,\u003c/li\u003e\n\u003cli\u003ethere exists a unique $0\\in V$ s.t. $x+0 = x$,\u003c/li\u003e\n\u003cli\u003ethere exists a unique $-x\\in V$ s.t. $x + (-x) = 0$,\u003c/li\u003e\n\u003cli\u003e$1x = x$,\u003c/li\u003e\n\u003cli\u003e$\\alpha(x+y) = \\alpha x + \\alpha y, (\\alpha+\\beta)x = \\alpha x + \\beta x$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/dd\u003e\n\u003cdt\u003eDefinition 2 (\u003cstrong\u003eLinear subspace\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eLet $V$ be a vector space. A non-empty set $U\\subset V$ is a \u003ccode\u003elinear subspace\u003c/code\u003e of $V$ if $U$ is itself a vector space, which is equivalent to the condition\n$$ \r\n   \\alpha x + \\beta y \\in U, \\;\\forall \\alpha,\\beta\\in \\mathbb{F}, x,y\\in U.\r\n$$\u003c/dd\u003e\n\u003cdt\u003eDefinition 3\u003c/dt\u003e\n\u003cdd\u003eLet $S$ be a set and let $V$ be a vector space over $\\mathbb{F}$. We denote the set of functions $f:S\\to V$ by $F(S,V)$. With the definition of scalar multiplication and vector addition, $F(S,V)$ is a vector space over $\\mathbb{F}$.\u003c/dd\u003e\n\u003cdt\u003eDefinition 4 (Linear transformation)\u003c/dt\u003e\n\u003cdd\u003eLet $V,W$ be vector spaces over $\\mathbb{F}$. A function $T: V\\to W$ is called a \u003ccode\u003elinear transformation\u003c/code\u003e if, for all $\\alpha,\\beta\\in \\mathbb{F}$ and $x,y\\in V$\n$$ \r\n T(\\alpha x + \\beta y) = \\alpha T(x) + \\beta T(y).\r\n$$\nThe set of all linear transformations $T:V\\to W$ is denoted by $L(V,W)$. With the definition of scalar multiplication and vector addition, $L(V,W)$ is a vector space.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003ch2 id=\"2-metric-spaces\"\u003e2. Metric Spaces\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 5 (Metric space)\u003c/dt\u003e\n\u003cdd\u003eA \u003ccode\u003emetric\u003c/code\u003e on a set $M$ is a function $d:M\\times M\\to \\R$ with the following properties,\n\u003col\u003e\n\u003cli\u003e$d(x,y)\\geq 0$,\u003c/li\u003e\n\u003cli\u003e$d(x,y)= 0 \\Leftrightarrow x=y$,\u003c/li\u003e\n\u003cli\u003e$d(x,y) = d(y,x)$,\u003c/li\u003e\n\u003cli\u003e$d(x,z)\\leq d(x,y) + d(y,z)$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIf $d$  is a metric on $M$, then the pair $(M,d)$ is called a \u003ccode\u003emetric space\u003c/code\u003e.\u003c/p\u003e","title":"Functional Analysis | 0 Preliminaries"},{"content":"1. Fourier Series Definition 1 (Trigonometric polynomial) A Trigonometric polynomial is a finite sum of the form $$ f(x) = a_0 + \\sum_{n=1}^N (a_n \\cos nx + b_n \\sin nx),\r$$ where $a_i,b_i$ are complex numbers. It can be written in the form $$ \\begin{equation}\rf(x) = \\sum_{n=-N}^N c_n e^{inx},\r\\end{equation}\r$$ which is more convenient for most purposes. It is clear that every Trigonometric polynomial has period $2\\pi$. Notice that $$ \\frac{1}{2\\pi}\\int_{-\\pi}^\\pi e^{inx} dx = 1_{n=0},\r$$ so we have $$ \\begin{equation}\rc_m = \\frac{1}{2\\pi}\\int_{-\\pi}^\\pi f(x)e^{-imx} dx\r\\end{equation}\r$$ for $|m|\\leq N$.\nDefinition 2 (Fourier series) In agreement with (1), we define a trigonometric series to be a series of the form $$ \\begin{equation}\r\\sum_{n=-\\infty}^\\infty c_n e^{inx}.\r\\end{equation}\r$$ If $f$ is integrable, the numbers $c_m$ defined by (2) for all integers $m$ are called the Fourier coefficients of $f$, and the series (3) formed with these coefficients is called a Fourier series of $f$.\nA natural question is whether the Fourier series of $f$ converges to $f$.\nDefinition 3 (Orthogonal system of functions) Let $\\{\\phi_n\\}$ be a sequence of complex functions on $[a,b]$, s.t. $$ \\int_a^b \\phi_n(x) \\bar{\\phi}_m(x) dx = 0,\\; (n\\neq m) .\r$$ Then $\\{\\phi_n\\}$ is said to be an orthogonal system of functions on $[a,b]$. In particular, if $$ \\int_a^b |\\phi_n(x)|^2 dx = 1\r$$ for all $n$, then $\\{\\phi_n\\}$ is said to be orthonormal. Let $\\\\{\\phi_n\\\\}$ be orthonormal on $[a,b]$. If $c_n = \\int_a^b f(x) \\bar{\\phi}_n(x) dx$ for all $n$, we call $c_n$ the n-th Fourier coefficient of $f$ relative to $\\\\{\\phi_n\\\\}$. Write $f(x) \\sim \\sum_{n=1}^\\infty c_n \\phi_n(x)$ and call this series the Fourier series of $f$ relative to $\\\\{\\phi_n\\\\}$.\nTheorem 4 Let $\\{\\phi_n\\}$ be orthonormal on $[a,b]$. Let $$ s_n(x) = \\sum_{m=1}^n c_m \\phi_m(x)\r$$ be the n-th partial sum of the Fourier series of $f$, and define $$ t_n (x) = \\sum_{m=1}^n \\gamma_m \\phi_m(x) .\r$$ Then $$ \\int_a^b |f - s_n|^2 dx \\leq \\int_a^b |f - t_n|^2 dx,\r$$ and the equality holds iff $c_m = \\gamma_m$, $m=1,...,n$. proof Omit scripts $a,b,n$, we write $\\int,\\sum$. Then $$ \\int f \\bar{t}_n = \\int f \\sum \\bar{\\gamma}_m \\bar{\\phi}_m = \\sum \\bar{\\gamma}_m c_m\r$$ by the definition of $c_m$. And $$ \\int |t_n|^2 = \\sum |\\gamma_m|^2\r$$ since $\\{\\phi_n\\}$ is orthonormal, and so $$\r\\begin{align*}\r\\int |f-t_n|^2 \u0026= \\int |f|^2 - \\int f \\bar{t}_n - \\int \\bar{f}t_n + \\int |t_n|^2 \\cr\r\u0026= \\int |f|^2 - \\sum \\bar{\\gamma}_m c_m - \\sum {\\gamma}_m \\bar{c}_m + \\sum |\\gamma_m|^2 \\cr\r\u0026=\\int |f|^2 + \\sum |\\gamma_m - c_m|^2 - \\sum |c_m|^2.\r\\end{align*}\r$$ which is minimized by $\\gamma_m = c_m$.\n$\\Box$\nThis theorem says that among all functions $t_n$, $s_n$ gives the best mean-square approximation to $f$.\nTheorem 5 Let $\\{\\phi_n\\}$ be orthonormal on $[a,b]$. If $f(x)\\sim \\sum_{n=1}^\\infty c_n \\phi_n(x)$, then $$ \\sum_{n=1}^\\infty |c_n|^2 \\leq \\int_a^b |f(x)|^2 dx.\r$$ In particular, $\\lim_{n\\to\\infty} c_n = 0$. proof Given $n$, we have $$ \\int |s_n|^2 = \\sum_{m=1}^n |c_m|^2 \\leq \\int |f|^2.\r$$ We draw the conclusion by letting $n\\to\\infty$. $\\Box$ 2. Convergence From now on we shall deal only with the trigonometric system. We shall consider functions $f$ that have period $2\\pi$ and that are Riemann-integrable on $[-\\pi,\\pi]$. We define the N-th partial sum of the Fourier series of $f$ by $$ S_N(x) = S_N(f;x) = \\sum_{n=-N}^N c_n e^{inx}.\r$$ Definition 6 (Dirichlet kernel) The Dirichlet kernel is defined by $$ \\begin{equation}\rD_N(x) := \\sum_{n=-N}^N e^{inx} = \\frac{\\sin(N+1/2)x}{\\sin(x/2)}.\r\\end{equation}\r$$ The second equality follows from the fact $$ (e^{ix} - 1) D_N(x) = e^{i(N+1)x} - e^{-iNx},\r$$ and multiply $e^{-ix/2}$ on both sides. Now we can rewrite $S_N(f;x)$ as $$ \\begin{align*}\rS_N(f;x) \u0026= \\sum_{n=-N}^N \\frac{1}{2\\pi} \\int f(t)e^{-int} dt\\; e^{inx} \\cr\r\u0026= \\frac{1}{2\\pi} \\int f(t) \\sum_{n=-N}^N e^{in(x-t)} dt \\cr\r\u0026= \\frac{1}{2\\pi} \\int f(t) D_N(x-t) dt \\cr\r\u0026= \\frac{1}{2\\pi} \\int f(x-t) D_N(t) dt.\r\\end{align*} $$ Theorem 7 (Convergence) If, for some $x$, there are constants $\\delta\u003e0$ and $M\u003c\\infty$ s.t. $$ |f(x+t) - f(x)|\\leq M|t|\r$$ for all $|t|\u003c\\delta$, then $$ S_N(f;x)\\to f(x) \\text{ as } N\\to\\infty.\r$$ proof Define $$ g(t) = \\frac{f(x-t) - f(x)}{\\sin(t/2)}\r$$ for $0\u003c|t|\\leq \\pi$, and let $g(0) = 0$. We have $$ \\begin{align*}\rS_N(f;x) - f(x) \u0026= \\frac{1}{2\\pi} \\int (f(x-t) - f(x)) D_N(t) \\;dt \\cr\r\u0026=\\frac{1}{2\\pi} \\int g(t) \\sin(N+1/2)t \\;dt \\cr\r\u0026=\\frac{1}{2\\pi} \\int g(t) \\cos(t/2)\\sin(Nt) \\;dt + \\frac{1}{2\\pi} \\int g(t) \\sin(t/2)\\cos(Nt) \\;dt\r\\end{align*}\r$$ By assumption, $g(t) \\cos(t/2),g(t) \\sin(t/2)$ are bounded. By Theorem 13.5, the last two integrals tend to 0 as $N\\to\\infty$. $\\Box$\nTheorem 8 If $f$ is continuous (with period $2\\pi$), then given $\\epsilon\u003e0$ there exists a trigonometric polynomial $P$ s.t. $$ |P(x) - f(x)|\u003c \\epsilon,\\;\\forall x.\r$$ proof $f$ can be regarded as a function on the unit circle $T$, by means of mapping $x\\mapsto e^{ix}$. Notice that the trigonometric polynomials form a self-adjoint algebra $\\mathcal{A}$, which seperates points on $T$, and vanishes at no point of $T$. For $T$ a unit circle. Since $T$ is compact, we have that $\\mathcal{A}$ is dense in $C(T)$. Theorem 9 (Parseval\u0026rsquo;s theorem) Suppose $f$ and $g$ are Riemann-integrable functions with period $2\\pi$, and $$ f(x)\\sim \\sum_{-\\infty}^\\infty c_n e^{inx},\\quad g(x) \\sim \\sum_{-\\infty}^\\infty \\gamma_n e^{inx}.\r$$ Then $$ \\begin{align}\r\\lim_{N\\to\\infty} \\frac{1}{2}\\int_{-\\pi}^\\pi |f(x) - S_N(f;x)|^2 dx \u0026= 0 ,\\cr\r\\frac{1}{2}\\int_{-\\pi}^\\pi f(x)\\bar{g}(x) dx \u0026= \\sum_{-\\infty}^\\infty c_n \\bar{\\gamma}_n, \\cr\r\\frac{1}{2}\\int_{-\\pi}^\\pi |f(x)|^2 dx \u0026= \\sum_{-\\infty}^\\infty |c_n|^2.\\cr\r\\end{align}\r$$ proof Define the norm $$\r\\|h\\|_2 = \\left( \\frac{1}{2\\pi}\\int_{-\\pi}^\\pi |h(x)|^2 dx \\right)^{1/2}.\r$$ We can choose a continuous $2\\pi$-periodic function $h$ s.t. $$\r\\| f-h\\|_2 \u003c \\epsilon.\r$$ By Theorem 13.8, there is a trigonometric polynomial $P$ s.t. $|P(x) - h(x)|\u003c\\epsilon$ for all $x$. Hence $\\|P - h \\|_2\u003c\\epsilon$. Suppose that $P$ has degree $N_0$, Theorem 13.4 shows that $$\r\\|h - S_N(h) \\|_2\\leq \\|h - P \\| \u003c\\epsilon\r$$ for all $N\\geq N_0$. With $h-f$ in place of $f$, $$ \\|s_N(h) - s_N(f) \\|_2 = \\| s_N(h-f)\\|_2\\leq \\| h-f\\|_2\u003c\\epsilon.\r$$ By the triangle inequality, $$ \\|f-s_N(f) \\|_2\u003c 3\\epsilon,\\; (N\\geq N_0).\r$$ Complements Definition (Self-adjoint) An algebra $\\mathcal{A}$ of complex functions is said to be self-adjoint if $f\\in\\mathcal{A}\\Rightarrow \\bar{f}\\in\\mathcal{A}$. Definition (Seperate points on E) Let $\\mathcal{A}$ be a family of functions on a set $E$. Then $\\mathcal{A}$ is said to seperate ponits on $E$ if to every pair of distinct points $x_1,x_2\\in E$, there corresponds a function $f\\in\\mathcal{A}$ s.t. $f(x_1)\\neq f(x_2)$. Definition (Vanish at no point) If to each $x\\in E$ there corresponds a function $g\\in\\mathcal{A}$ s.t. $g(x)\\neq0$, we say that $\\mathcal{A}$ vanish at no point of $E$. Theorem Suppose $\\mathcal{A}$ is self-adjoint algebra of complex continuous functions on a compact set $K$, $\\mathcal{A}$ separates points on $K$, vanishes at no point of $K$. Then the uniform closure $\\mathcal{B}$ of $\\mathcal{A}$ consists of all complex continuous functions on $K$. In other words, $\\mathcal{A}$ is dense $C(K)$. ","permalink":"http://localhost:1313/posts/analysis_1/math_analysis_13/","summary":"\u003ch2 id=\"1-fourier-series\"\u003e1. Fourier Series\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 1 (\u003cstrong\u003eTrigonometric polynomial\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eA \u003ccode\u003eTrigonometric polynomial\u003c/code\u003e is a finite sum of the form\n$$ \r\n f(x) = a_0 + \\sum_{n=1}^N (a_n \\cos nx  + b_n \\sin nx),\r\n$$\nwhere $a_i,b_i$ are complex numbers.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003cp\u003eIt can be written in the form\n\u003c/p\u003e\n$$ \r\n\\begin{equation}\r\nf(x) = \\sum_{n=-N}^N c_n e^{inx},\r\n\\end{equation}\r\n$$\u003cp\u003e\nwhich is more convenient for most purposes. It is clear that every Trigonometric polynomial has period $2\\pi$. Notice that\n\u003c/p\u003e","title":"Mathematical Analysis | 13 Fourier Series"},{"content":"In this Chapter we shall the power series formed by the sequence of power functions $\\{c_n (x-x_0)^n\\}$, i.e. of the form $$ \\sum_{n=0}^\\infty c_n (x-x_0)^n. \\tag{1}\r$$ W.l.o.g. we shall focus on the case where $x_0 = 0$, i.e. $$ \\sum_{n=0}^\\infty c_n x^n. \\tag{2}\r$$1. Power Series First, we study the convergence of series (2).\nLemma 1 Suppose the series (2) converges for $|x|\u003c R$, then it is absolutely convergent on every $[-R+\\epsilon, R-\\epsilon]$. proof Let $a_n = |c_n x^n|$, which goes to zero and is bounded. We have $\\lim\\sup_{n\\to\\infty}\\sqrt[n]{a_n} = 0 \u003c1$. By root test we complete the proof. $\\Box$ Theorem 2 Suppose the series (2) converges for $|x|\u003c R$, and define $$ f(x) = \\sum_{n=0}^\\infty c_n x^n, \\quad (|x|\u003c R). \\tag{3}\r$$ Then (2) converges uniformly on every $[-R+\\epsilon, R-\\epsilon]$. The function $f$ is continuous and differentiable in $(-R,R)$, and $$ f'(x) = \\sum_{n=1}^\\infty n c_n x^{n-1} , \\quad (|x|\u003c R). \\tag{4}\r$$ proof For $|x|\u003c R-\\epsilon$, we have $$ |c_n x^n|\\leq |c_n (R-\\epsilon)^n|\r$$ and since $\\sum c_n (R-\\epsilon)^n$ converges absolutely by Lemma 1, the series (2) converges uniformly on $[-R+\\epsilon, R-\\epsilon]$. Next, we have $\\lim\\sup_{n\\to\\infty} \\sqrt[n]{n|c_n|} = \\lim\\sup_{n\\to\\infty} \\sqrt[n]{|c_n|}$ since $\\sqrt[n]{n} = 1$, which implies that $f$ and $f'$ have the same interval of convergence. As (4) is also a power series, it converges uniformly in $[-R+\\epsilon, R-\\epsilon]$. We can apply Theorem 11.10 (Differentiation) and obtain the assertion (4). Continuity of $f$ follows from the existence of $f'$. $\\Box$ Theorem 3 Suppose $\\sum c_n$ converges, Put $$ f(x) = \\sum_{n=0}^\\infty c_n x^n, \\quad (|x|\u003c 1).\r$$ Then $$ \\lim_{x\\to 1_-} f(x) = \\sum_{n=0}^\\infty c_n.\r$$ proof Let $f_n(x) = \\sum_{j=0}^n c_j x^j$. $f_n \\to f$ at $x=1$, so $f_n\\to f$ uniformly on $[0,1]$. Since $f_n$ are continuous, then $f$ is continuous on $[0,1]$ (left continuous at $x = 1$). $\\Box$ 2. Power Series Expansion If (1) converges for $|x-x_0|\u003c R$, and a function $f$ can be represented as $f(x) = \\sum_{n=0}^\\infty c_n (x-x_0)^n$, then we say $f$ is expanded in a power series about the point $x=x_0$.\nTheorem 4 (Taylor expansion) Suppose $f(x) = \\sum_{n=0}^\\infty c_n x^n$, the series converging in $|x|\u003c R$. If $a\\in(-R,R)$, then $f$ can be expanded in a power series about the point $x = a$, which converges in $|x-a|\u003c R - |a|$, i.e. $$ f(x) = \\sum_{n=0}^\\infty \\frac{f^{(n)}(a)}{n!}(x-a)^n.\r$$ proof Notice that $f^{(k)}(x) = \\sum_{n=k}^\\infty n (n-1)\\cdots (n-k+1) c_n x^{n-k}$. And $$\r\\begin{align*}\rf(x) \u0026= \\sum_{n=0}^\\infty c_n (x-a+a )^n \\cr\r\u0026= \\sum_{n=0}^\\infty c_n \\sum_{m=0}^n \\binom{n}{m} a^{n-m} (x-a )^m \\cr\r\u0026= \\sum_{m=0}^\\infty \\sum_{n=m}^\\infty \\binom{n}{m} c_n a^{n-m} (x-a )^m\r\\end{align*}\r$$ We can exchange the order of summation because the series is convergent. $\\Box$\nTheorem 5 Suppose the series $\\sum a_n x^n$ and $\\sum b_n x^n$ converge in the segment $S = (-R,R)$. Let $E$ be the set of all $x\\in S$ where $$ \\sum_{n=0}^\\infty a_n x^n = \\sum_{n=0}^\\infty b_n x^n . \\tag{5}\r$$ If $E$ has a limit point in $S$, then $a_n = b_n$ for $n=0,1,2,...$. Hence the equation (5) holds for all $x\\in S$. proof Let $c_n = a_n - b_n$ and $f(x) = \\sum_{n=0}^\\infty c_n x^n $, then $f(x) = 0$ on $E$. Let $A$ be the set of limit points of $E$ in $S$, and $B = S\\setminus A$. It is clear that $B$ is open. Suppose we can prove that $A$ is open, because $S = A\\cup B$ and $S$ is connected (check definiiton), then it follows that $B$ is empty, thus $A = S$. Since $f$ is continuous in $S$, $A\\subset E$. Thus $E=S$, and $c_n = 0$ for $n=0,1,...$. Now we prove that $A$ is open. If $x_0\\in A$, then we can expand $f$ at $x=x_0$ by $$ f(x) = \\sum_{n=0}^\\infty d_n (x - x_0)^n,\\quad |x - x_0|\u003c R- |x_0|.\r$$ By Theorem 12.4, $d_n = 0$ for all $n$. So that $f(x)=0$ for all $x:|x-x_0|\u003c R- |x_0|$, i.e. in a neighbourhood of $x_0$. This shows that $A$ is open.\n$\\Box$\n","permalink":"http://localhost:1313/posts/analysis_1/math_analysis_12/","summary":"\u003cp\u003eIn this Chapter we shall the power series formed by the sequence of power functions $\\{c_n (x-x_0)^n\\}$, i.e. of the form\n\u003c/p\u003e\n$$ \r\n \\sum_{n=0}^\\infty c_n (x-x_0)^n. \\tag{1}\r\n$$\u003cp\u003e\nW.l.o.g. we shall focus on the case where $x_0 = 0$, i.e.\n\u003c/p\u003e\n$$ \r\n   \\sum_{n=0}^\\infty c_n x^n. \\tag{2}\r\n$$\u003ch2 id=\"1-power-series\"\u003e1. Power Series\u003c/h2\u003e\n\u003cp\u003eFirst, we study the convergence of series (2).\u003c/p\u003e\n\u003cdl\u003e\n\u003cdt\u003eLemma 1\u003c/dt\u003e\n\u003cdd\u003eSuppose the series (2) converges for $|x|\u003c R$, then it is absolutely convergent on every  $[-R+\\epsilon, R-\\epsilon]$.\u003c/dd\u003e\n\u003cdt\u003eproof\u003c/dt\u003e\n\u003cdd\u003eLet $a_n = |c_n x^n|$, which goes to zero and is bounded. We have $\\lim\\sup_{n\\to\\infty}\\sqrt[n]{a_n} = 0 \u003c1$. By root test we complete the proof.\n$\\Box$\u003c/dd\u003e\n\u003cdt\u003eTheorem 2\u003c/dt\u003e\n\u003cdd\u003eSuppose the series (2) converges for $|x|\u003c R$, and define\n$$ \r\n f(x) =   \\sum_{n=0}^\\infty c_n x^n, \\quad (|x|\u003c R). \\tag{3}\r\n$$\nThen (2) converges uniformly on every $[-R+\\epsilon, R-\\epsilon]$. The function $f$ is continuous and differentiable in $(-R,R)$, and\n$$ \r\n f'(x) = \\sum_{n=1}^\\infty n c_n x^{n-1}  , \\quad (|x|\u003c R). \\tag{4}\r\n$$\u003c/dd\u003e\n\u003cdt\u003eproof\u003c/dt\u003e\n\u003cdd\u003eFor $|x|\u003c R-\\epsilon$, we have\n$$ \r\n |c_n x^n|\\leq |c_n (R-\\epsilon)^n|\r\n$$\nand since $\\sum c_n (R-\\epsilon)^n$ converges absolutely by Lemma 1, the series (2) converges uniformly on $[-R+\\epsilon, R-\\epsilon]$.\nNext, we have $\\lim\\sup_{n\\to\\infty} \\sqrt[n]{n|c_n|} = \\lim\\sup_{n\\to\\infty} \\sqrt[n]{|c_n|}$ since $\\sqrt[n]{n} = 1$, which implies that $f$ and $f'$ have the same interval of convergence. As (4) is also a power series, it converges uniformly in $[-R+\\epsilon, R-\\epsilon]$. We can apply Theorem 11.10 (Differentiation) and obtain the assertion (4).\nContinuity of $f$ follows from the existence of $f'$.\n$\\Box$\u003c/dd\u003e\n\u003cdt\u003eTheorem 3\u003c/dt\u003e\n\u003cdd\u003eSuppose $\\sum c_n$ converges, Put\n$$ \r\n f(x) =   \\sum_{n=0}^\\infty c_n x^n, \\quad (|x|\u003c 1).\r\n$$\nThen\n$$ \r\n \\lim_{x\\to 1_-} f(x) = \\sum_{n=0}^\\infty c_n.\r\n$$\u003c/dd\u003e\n\u003cdt\u003eproof\u003c/dt\u003e\n\u003cdd\u003eLet $f_n(x) = \\sum_{j=0}^n c_j x^j$. $f_n \\to f$ at $x=1$, so $f_n\\to f$ uniformly on $[0,1]$. Since $f_n$ are continuous, then $f$ is continuous on $[0,1]$ (left continuous at $x = 1$).\n$\\Box$\u003c/dd\u003e\n\u003c/dl\u003e\n\u003c!-- Let $s_n = \\sum_{j=0}^n c_j$. Then \r\n$$ \r\n   \\sum_{n=0}^m c_n x^n = \\sum_{n=0}^m (s_n - s_{n-1}) x^n = (1-x)\\sum_{n=0}^m s_n x^n + s_m x^m.\r\n$$\r\nFor $|x|\u003c1$, we let $m\\to\\infty$ and obtain\r\n$$ \r\n f(x) = (1-x)  \\sum_{n=0}^\\infty s_n x^n.\r\n$$\r\nsuppose $\\lim_{n\\to\\infty} s_n= s$. Let $\\epsilon\u003e0$ and choose $N$ so that $n\u003eN$ implies\r\n$$ \r\n |s_n - s|\u003c\\epsilon/2.  \r\n$$\r\nSince $(1-x)  \\sum_{n=0}^\\infty  x^n = 1 , |x|\u003c1$, we obtain\r\n$$ \r\n |f(x) - s| = |(1-x)  \\sum_{n=0}^\\infty (s_n -s) x^n|\\leq |1-x|  \r\n$$ --\u003e\r\n\u003ch2 id=\"2-power-series-expansion\"\u003e2. Power Series Expansion\u003c/h2\u003e\n\u003cp\u003eIf (1) converges for $|x-x_0|\u003c R$, and a function $f$ can be represented as $f(x) = \\sum_{n=0}^\\infty c_n (x-x_0)^n$, then we say $f$ is \u003ccode\u003eexpanded in a power series\u003c/code\u003e  about the point $x=x_0$.\u003c/p\u003e","title":"Mathematical Analysis | 12 Power Series"},{"content":"We have studied how to define a number by convergent sequences or series. In this chapter we discuss how to define a function by a sequence or series of functions, and study the properties of this function.\n1. Uniform Convergence Definition 1 (Pointwise convergence) Suppose $\\{f_n\\}$ be a sequence of functions defined on a set $E$, and suppose that the sequence of numbers $\\{f_n(x)\\}$ converges for every $x\\in E$. We can define a function $f$ by $$ f(x) = \\lim_{n\\to\\infty} f_n(x). \\tag{1} $$ We say that $\\{f_n\\}$ converges pointwise to $f$ on $E$. Similarly, if $\\sum f_n (x)$ converges for every $x\\in E$, we can define $f$ by $$ f(x) = \\sum_{n=1}^\\infty f_n(x). \\tag{2} $$ Definition 2 (Uniform convergence) We say that a sequence of functions $\\{f_n\\}$ converges uniformly to a function $f$ on $E$ if for every $\\epsilon\u003e0$ there exists an integer $N$ s.t. $n\\geq N$ implies $$ |f_n(x) - f(x)|\u003c\\epsilon \\tag{3} $$ for all $x\\in E$. It is clear that every uniformly convergent sequence is pointwise convergent.\nTheorem 3 (Cauchy criterion) The sequence of functions $\\{f_n\\}$ defined on $E$ converges uniformly on $E$ iff for every $\\epsilon\u003e0$ there exists an integer $N$ s.t. $m,n\\geq N$ implies $$ |f_n(x) - f_m(x)|\u003c\\epsilon, \\tag{4} $$ for all $x\\in E$. proof \u0026lsquo;$\\Rightarrow$\u0026rsquo; is trivial. We show \u0026lsquo;$\\Leftarrow$\u0026rsquo; only. Suppose the Cauchy condition holds, we have that $\\{f_n(x)\\}$ converges for every $x\\in E$ to a limit function $f$. We shall prove the convergence is uniform. Let $\\epsilon\u003e0$ be given, we choose $N$ s.t. (4) holds. Fix $n$ and let $m\\to\\infty$, we have $$ |f_n(x) - f(x)|\u003c\\epsilon $$ for every $n\\geq N$ and every $x\\in E$. $\\Box$ Theorem 4 The sequence of functions $\\{f_n\\}$ defined on $E$ converges uniformly to $f$ on $E$ iff $$ \\lim_{n\\to\\infty}\\sup_{x\\in E} |f_n(x) - f(x)| = 0. $$ proof We show \u0026lsquo;$\\Leftarrow$\u0026rsquo; only. For any $\\epsilon\u003e0$, choose $N$ s.t. $n\\geq N$ implies $$ \\sup_{x\\in E}|f_n(x) - f(x)| \u003c \\epsilon. $$ It follows that for every $x\\in D$, $|f_n(x) - f(x)|\\leq \\sup_{x\\in E}|f_n(x) - f(x)| \u003c \\epsilon$. $\\Box$ Theorem 5 Suppose $\\{f_n\\}$ is a sequence of functions defined on $E$, and suppose for every $x\\in E$ and $n=1,2,...$ $$ |f_n(x)|\\leq M_n. $$ Then $\\sum f_n$ converges uniformly on $E$ if $\\sum M_n$ converges. proof $|\\sum_{k=n}^m f_k(x)|\\leq \\sum_{k=n}^m M_k \u003c \\epsilon$. $\\Box$ 2. Continuity Theorem 6 Suppose $f_n\\to f$ uniformly on a set $E$. Let $x_0$ a limit point of $E$, and suppose that, for $n=1,2,...$ $$ \\lim_{x\\to x_0}f_n(x) = a_n. $$ Then $\\{a_n\\}$ converges, and $$ \\lim_{x\\to x_0} f(x) = \\lim_{n\\to\\infty} a_n. $$ In other words, $\\lim_{x\\to x_0}\\lim_{n\\to\\infty} f_n(x) = \\lim_{n\\to\\infty}\\lim_{x\\to x_0} f_n(x)$. proof By the uniform convergence of $\\{f_n\\}$, for every $\\epsilon\u003e0$, there is $N$ s.t. for any $m,n\\geq N$ and any $x\\in E$, $$ |f_n(x) - f_m(x)|\u003c\\epsilon. $$ Now let $x\\to x_0$, we obtain $|a_n - a_m|\\leq \\epsilon$. So $\\{a_n\\}$ converges, say to $A$. Next, $|f(x) - A|\\leq |f(x) - f_n(x)| + |f_n(x) - a_n| + |a_n - A|$. We first choose $n$ s.t $$ |f(x) - f_n(x)|\u003c\\epsilon/3, \\quad \\forall x\\in E \\quad (\\text{uniform convergence}), $$ and s.t. $$ |a_n - A| \u003c \\epsilon/3 , \\quad (\\text{convergent}). $$ Then, for this $n$, we choose a neighbourhood $V$ of $x_0$ where $$ |f_n(x) - a_n| \u003c\\epsilon/3. $$ Overall, for $x \\in E\\cap V, x\\neq x_0$, we have $|f(x) - A|\u003c\\epsilon.$ $\\Box$ Theorem 7 (Continuity) If $\\{f_n\\}$ is a sequence of continuous functions on $E$, and if $f_n\\to f$ uniformly on $E$, then $f$ is continuous on $E$. proof For $x\\in E$, we have $\\lim_{x\\to x_0} f_n(x) = f_n(x_0)$. By the Theorem 6, we know that $\\lim_{x\\to x_0} f(x) = \\lim_{n\\to\\infty}f_n(x_0) = f(x_0)$. $\\Box$ Theorem 8 Suppose $K$ is compact, and $\\{f_n\\}$ is a sequence of continuous functions on $K$, $\\{f_n\\}$ converges pointwise to a continuous function $f$ on $K$, $f_n(x)\\geq f_{n+1}(x)$ for all $x\\in E, n = 1,2,...$. Then $f_n \\to f$ uniformly on $K$.\nproof Let $g_n = f_n -f $, then $g_n$ is continuous, $g_n \\to 0$ pointwise, and $g_n\\geq g_{n+1}$. It suffices to show that $g_n \\to 0$ uniformly on $K$. Given $\\epsilon\u003e0$, let $K_n=\\{x\\in K: g_n(x)\\geq \\epsilon\\}$. Since $g_n$ is continuous, $K_n$ is closed and hence compact. Since $g_n\\geq g_{n+1}$, $K_n\\supset K_{n+1}$. For any $x\\in K$, since $g_n(x)\\to 0$, we see that $x\\notin K_n$ for some $n$, hence $x\\notin \\cap K_n$. In other words, $\\cap K_n=\\emptyset$, and hence $K_N = \\emptyset$ for some $N$. It follows that $0\\geq g_n(x) \u003c\\epsilon $ for all $x\\in K$ and for all $n\\geq N$. 3. Integration Theorem 9 (Integration) Let $\\alpha$ be a monotonically increasing on $[a,b]$. Suppose $f_n \\in \\mathcal{R}(\\alpha)$ on $[a,b]$ for $n=1,2,...,$ and suppose $f_n\\to f$ uniformly on $[a,b]$. Then $f\\in \\mathcal{R}(\\alpha)$ and $$ \\int_a^b f d\\alpha = \\lim_{n\\to\\infty}\\int_a^b f_n d\\alpha . $$ proof Put $\\epsilon_n = \\sup_x |f_n(x) - f(x)|$. Then, $$ f_n - \\epsilon_n \\leq f \\leq f_n + \\epsilon, $$ so that $$ \\int_a^b (f_n - \\epsilon_n) d\\alpha \\leq \\sup_T L(T,f,\\alpha)\\leq \\inf_T U(T,f,\\alpha) \\leq \\int_a^b (f_n + \\epsilon_n) d\\alpha. \\tag{5} $$ Hence $\\inf_T U(T,f,\\alpha) - \\sup_T L(T,f,\\alpha)\\leq 2\\epsilon_n[\\alpha(b) - \\alpha(a)]\\to 0$ by uniform convergence. Thus $f\\in \\mathcal{R}(\\alpha)$ and (5) yields $$ \\left| \\int_a^b fd \\alpha - \\int_a^b f_n d\\alpha \\right| \\;\\leq \\epsilon_n[\\alpha(b) - \\alpha(a)]\\to 0 $$ $\\Box$ 4. Differentiation Theorem 10 (Differentiation) Suppose $\\{f_n\\}$ is a sequence of functions, differentiable on $[a,b]$ and $\\{f_n(x_0)\\}$ converges for some point $x_0\\in[a,b]$. If $\\{f_n'\\}$ converges uniformly on $[a,b]$, then $\\{f_n\\}$ converges uniformly on $[a,b]$ to a function $f$, and $$ f'(x) = \\lim_{n\\to\\infty}f_n'(x) ,\\quad \\forall x\\in[a,b]. $$ proof Let $\\epsilon\u003e0$ be given. By the convergence, we choose $N$ s.t. $m,n\\geq N$ implies $$ |f_n(x_0) - f_m(x_0)|\u003c\\epsilon/2, $$ and $$ |f_n'(x) - f_m'(x)|\u003c \\frac{\\epsilon}{2(b-a)}, \\quad x\\in[a,b] . \\tag{6} $$ Apply the mean value theorem to the function $f_n - f_m$, (6) yields that $$ |f_n(x) - f_m(x) - f_n(t) + f_m(t)|\\leq \\frac{|x-t|\\epsilon}{2(b-a)} \u003c \\epsilon/2. \\tag{7} $$ Then by triangle inequality, we have $$ |f_n(x) - f_m(x)|\\leq |f_n(x) - f_m(x) - f_n(x_0) + f_m(x_0)| + |f_n(x_0) - f_m(x_0)|\u003c\\epsilon. $$ Thus we obtain the uniform convergence, and we write $f_n\\to f$ uniformly. Next, fix a point $x\\in [a,b]$ and define $$ \\phi_n(t) = \\frac{f_n(t) - f_n(x)}{t -x}, \\quad \\phi(t) = \\frac{f(t) - f(x)}{t -x}. $$ Then $\\lim_{t\\to x} \\phi_n(t) = f_n'(x)$ by differentiability. We shall show that $\\{\\phi_n\\}$ converges uniformly. The first inequality in (7) gives $$ |\\phi_n(t) - \\phi_m(t)|\\leq \\frac{\\epsilon}{2(b-a)}, \\quad (n,m\\geq N,t\\in[a,b]\\setminus\\{x\\}). $$ So the uniform convergence of $\\{\\phi_n\\}$ is obtained for $t\\neq x$. Since $\\{f_n\\}$ converges to $f$, we conclude that $$ \\lim_{t\\to\\infty}\\phi_n(t) = \\phi(t) $$ By Theorem 6, we have $$ \\lim_{t\\to x} \\phi(t) = \\lim_{n\\to\\infty}f_n'(x). $$ Complements Definition (Open cover) By an open cover of a set $E$ in a metric space $X$, we mean a collection $\\{G_\\alpha\\}$ of open subsets (contain interior points only) of $x$ s.t. $E\\subset \\cup_\\alpha G_\\alpha$. Definition (Compact) A subset $K$ of a metric space $X$ is said to be compact relative to $X$ if every open cover of $K$ contains a finite subcover. Theorem Suppose $K\\subset Y\\subset X$. Then $K$ is compact relative to $X$ iff $K$ is compact relative to $Y$. Theorem Compact subsets of metric spaces are closed (contains all limit points). proof We shall prove that the complement of $K$ is an open subset of $X$. Suppose $p\\in X, p\\notin K$. If $q\\in K$, let $V_q$ and $W_q$ be neighbourhoods of $p$ and $q$, relatively, of radius less than $\\frac{1}{2}d(p,q)$. Since $K$ is compact, we can find finitely many $q_1,...,q_n \\in K$ s.t. $$ K \\subset \\cup_{j=1}^n W_{q_j}:= W. $$ If $V = \\cap_{i=1}^n V_{q_i}$, then $V$ is a neighbourhood of $p$ and $V\\cap W =\\emptyset$. Hence $V\\subset K^\\complement$, so $p$ is an interior point of $K^\\complement$. $\\Box$ Theorem Closed subsets of compact sets are compact. Corollary If $F$ is closed and $K$ is compact, then $F\\cap K$ is compact. Theorem If $\\{K_\\alpha\\}$ is a collection of compact subsets of a metric space $X$ s.t. the intersection of every finite subcollection of $\\{K_\\alpha\\}$ is nonempty, then $\\cap K_\\alpha$ is nonempty. Corollary If $\\{K_n\\}$ is a collection of nonempty compact sets s.t. $K_n\\supset K_{n+1}$, then $\\cap K_n$ is not empty. ","permalink":"http://localhost:1313/posts/analysis_1/math_analysis_11/","summary":"\u003cp\u003eWe have studied how to define a number by convergent sequences or series. In this chapter we discuss how to define a function by a sequence or series of functions, and study the properties of this function.\u003c/p\u003e\n\u003ch2 id=\"1-uniform-convergence\"\u003e1. Uniform Convergence\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 1 (\u003cstrong\u003ePointwise convergence\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eSuppose $\\{f_n\\}$ be a sequence of functions defined on a set $E$, and suppose that the sequence of numbers $\\{f_n(x)\\}$ converges for every $x\\in E$. We can define a function $f$ by\n$$ \n f(x) = \\lim_{n\\to\\infty} f_n(x).  \\tag{1}\n$$\nWe say that $\\{f_n\\}$ \u003ccode\u003econverges pointwise\u003c/code\u003e to $f$ on $E$. Similarly, if $\\sum f_n (x)$ converges for every $x\\in E$, we can define $f$ by\n$$ \n f(x) = \\sum_{n=1}^\\infty f_n(x).  \\tag{2}\n$$\u003c/dd\u003e\n\u003cdt\u003eDefinition 2 (\u003cstrong\u003eUniform convergence\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eWe say that a sequence of functions $\\{f_n\\}$ \u003ccode\u003econverges uniformly\u003c/code\u003e to a function $f$ on $E$ if for every $\\epsilon\u003e0$ there exists an integer $N$ s.t. $n\\geq N$ implies\n$$ \n |f_n(x) - f(x)|\u003c\\epsilon \\tag{3}\n$$\nfor all $x\\in E$.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003cp\u003eIt is clear that every uniformly convergent sequence is pointwise convergent.\u003c/p\u003e","title":"Mathematical Analysis | 11 Sequences and Series of Functions"},{"content":"1. Convergence Definition 1 (Series) Given a sequence $\\{a_n\\}$, we define a sequence $\\{s_n\\}$ by $$ s_n = \\sum_{i=1}^n a_i.\r$$ We call $\\sum_{i=1}^\\infty a_i$ an infinite series or just a series. The numbers $s_n$ are called the partial sums of the series. Definition 2 (Convergence of series) If the sequence $\\{s_n\\}$ converges to $s$, we say that the series converges, and write $\\sum_{n=1}^\\infty a_n = s$. If $\\{s_n\\}$ diverges, the series is said to be diverge. Theorem 3 (Cauchy criterion) $\\sum a_n$ converges iff for any $\\epsilon\u003e0$, there exists an integer $N$ s.t. $$ |\\sum_{k=n}^m a_k|\u003c \\epsilon\r$$ if $m\\geq n\\geq N$. In other words, if $\\sum a_n$ converges, then $\\lim_{n\\to\\infty} a_n = 0$.\nTheorem 4 A series of nonnegative terms converges iff its partial sums form a bounded sequence. Theorem 5 If $|a_n|\\leq c_n$ for $n\\geq N$, and if $\\sum c_n$ converges, then $\\sum a_n$ converges. If $a_n\\geq d_n\\geq 0$ for $n\\geq N$, and if $\\sum d_n$ diverges, then $\\sum a_n$ diverges. proof Given $\\epsilon\u003e0$, there exists $N$ s.t. $m\\geq n\\geq N$ implies $$ \\sum_{k=n}^m c_k \u003c\\epsilon,\r$$ by Cauchy criterion. Hence $$ |\\sum_{k=n}^m a_k|\\leq \\sum_{k=n}^m |a_k|\\leq \\sum_{k=n}^m c_k \u003c\\epsilon.\r$$ $\\Box$\n2. The Root and Ratio Tests Theorem 6 (Root test) Given $\\sum a_n$, put $\\alpha = \\lim\\sup_{n\\to\\infty} \\sqrt[n]{|a_n|}$. Then if $\\alpha\u003c1$, $\\sum a_n$ converges; if $\\alpha\u003e1$, $\\sum a_n$ diverges; if $\\alpha =1$, the test gives no information. proof If $\\alpha\u003c1$, we can choose $\\alpha\u003c\\beta\u003c1$, and an integer $N$ s.t. $$ \\sqrt[n]{|a_n|}\u003c\\beta\r$$ for $n\\geq N$. It follows that $|a_n|^n\u003c\\beta^n$. Since $\\sum \\beta^n$ converges, by Theorem 5 (i), $\\sum a_n$ converges. If $\\alpha \u003e1$, then there is a sequence $\\{n_k\\}$ s.t. $\\sqrt[n_k]{|a_{n_K}|}\\to \\alpha$. Hence $a_n \\to 0$ does not hold. $\\Box$ Theorem 7 (Ratio test) The series $\\sum a_n$, converges if $\\lim\\sup_{n\\to\\infty}|a_{n+1}/a_n|\u003c1$, diverges if $|a_{n+1}/a_n|\\geq 1$ for all $n\\geq N$. proof Under condition (a), we can find $\\beta\u003c1$ and integer $N$ s.t. $$ \\left| \\frac{a_{n+1}}{a_n} \\right|\u003c\\beta\r$$ for $n\\geq N$. It follows that $|a_n|\u003c |a_N|\\beta^{n-N}$. Then (a) follows from the fact that $\\sum \\beta^n$ converges. If $|a_{n+1}|\\geq |a_n|$ for $n\\geq N$, it is easily seen that $a_n\\to 0$ does not hold. $\\Box$\nNotice that the ratio test is easier to apply than the root test, since it easier to compute ratios than roots. However, the root test has wider scope. More precisely, whenever the ratio test shows convergence, the root test does too; whenever the root test is inconclusive, the ratio test is too.\n3. Absolute Convergence The series $\\sum a_n$ is said to be absolutely convergent if the series $\\sum |a_n|$ converges.\nTheorem 8 if $\\sum a_n$ converges absolutely, then $\\sum a_n$ converges. proof $|\\sum_{k=n}^m a_k|\\leq \\sum_{k=n}^m |a_k|$. $\\Box$\n","permalink":"http://localhost:1313/posts/analysis_1/math_analysis_10/","summary":"\u003ch2 id=\"1-convergence\"\u003e1. Convergence\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 1 (Series)\u003c/dt\u003e\n\u003cdd\u003eGiven a sequence $\\{a_n\\}$, we define a sequence $\\{s_n\\}$ by\n$$ \r\n s_n = \\sum_{i=1}^n a_i.\r\n$$\nWe call $\\sum_{i=1}^\\infty a_i$ an \u003ccode\u003einfinite series\u003c/code\u003e or just a \u003ccode\u003eseries\u003c/code\u003e. The numbers $s_n$ are called the \u003ccode\u003epartial sums\u003c/code\u003e of the series.\u003c/dd\u003e\n\u003cdt\u003eDefinition 2 (Convergence of series)\u003c/dt\u003e\n\u003cdd\u003eIf the sequence $\\{s_n\\}$ converges to $s$, we say that the series converges, and write $\\sum_{n=1}^\\infty a_n = s$. If $\\{s_n\\}$ diverges, the series is said to be diverge.\u003c/dd\u003e\n\u003cdt\u003eTheorem 3 (Cauchy criterion)\u003c/dt\u003e\n\u003cdd\u003e$\\sum a_n$ converges iff for any $\\epsilon\u003e0$, there exists an integer $N$ s.t.\n$$ \r\n |\\sum_{k=n}^m a_k|\u003c \\epsilon\r\n$$\nif $m\\geq n\\geq N$.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003cp\u003eIn other words, if $\\sum a_n$ converges, then $\\lim_{n\\to\\infty} a_n = 0$.\u003c/p\u003e","title":"Mathematical Analysis | 10 Numerical Series"},{"content":"1. Definitions Definition 1 (Partition) Let $[a,b]$ be a given interval, by a partition $T$ of $[a,b]$, we mean a finite set of points $x_0,x_1,...,x_n$, where $$ a = x_0\\leq x_1\\leq \\cdots\\leq x_n = b\r$$ We write $\\Delta x_i = x_i - x_{i-1}$, and denote $|T| = \\max_i \\Delta x_i$ as the mesh of the partition $T$. Definition 2 (Riemann sum) Let $f$ be a given function defined on $[a,b]$, and $T$ be a partition of $[a,b]$. For any points $x_{i-1}\\leq\\xi\\leq x_i$, $i=1,2,...,n$, the summation $$ \\sum_{i=1}^n f(\\xi_i)\\Delta x_i,\r$$ is called the Riemaan sum of $f$ on $[a,b]$. Definition 3 (Riemann integrable) Let $f$ be a given function defined on $[a,b]$, and $J$ be a fixed number. If for any $\\epsilon\u003e0$, there exists $\\delta\u003e0$, s.t. for every partition $T$ of $[a,b]$ with $|T|\u003c\\delta$ and for any choice of $\\{\\xi_i\\}$ we have $$ |\\sum_{i=1}^n f(\\xi_i)\\Delta x_i - J| \u003c\\epsilon,\\tag{1}\r$$ we say that the function $f$ is Riemann integrable on the interval $[a,b]$, we write $f\\in\\mathcal{R}$ and we denote the $J$ by $J = \\int_a^b f(x) dx$. Definition 4 (Stieltjes integral) Let $\\alpha$ be an increasing and bounded function on $[a,b]$, $f:[a,b]\\to \\mathbb{R}$. For each partition $T$ of $[a,b]$, we write $\\Delta \\alpha_i = \\alpha(x_i) - \\alpha(x_{i-1})$. If for any $\\epsilon\u003e0$, there exists $\\delta\u003e0$, s.t. for every partition $T$ of $[a,b]$ with $|T|\u003c\\delta$ and for any choice of $\\{\\xi_i\\}$ we have $$ |\\sum_{i=1}^n f(\\xi_i)\\Delta \\alpha_i - J| \u003c\\epsilon,\\tag{2}\r$$ we say that the function $f$ is Stieltjes integrable w.r.t. $\\alpha$ on the interval $[a,b]$, and write $f\\in \\mathcal{R}(\\alpha)$, denoted by $J = \\int_a^b f(x) d\\alpha(x)$. By taking $\\alpha(x) = x$, the Riemann integral can be viewed as a special case of the Stieltjes integral.\n2. Existence Theorem 5 (Necessary condition) If $f\\in \\mathcal{R}$ on $[a,b]$, then $f$ is bounded on $[a,b]$. proof By contradiction. If $f$ is unbounded on $[a,b]$, then for any partition $T$ of $[a,b]$, there exists an interval $[x_{k-1},x_k]$ where $f$ is unbounded. Let $G = |\\sum_{i\\neq k} f(\\xi_i) \\Delta x_i|$, then for any $M\u003e0$, there exists $\\xi_k\\in [x_{k-1},x_k]$ s.t. $$ |f(\\xi_k)\\Delta x_k|\u003e \\frac{M+ G}{\\Delta x_k}.\r$$ So $|\\sum_{i=1}^n f(\\xi_i) \\Delta x_i| \\geq |f(\\xi_k)\\Delta x_k| - G \u003e M$. $\\Box$\nSuppose $T$ is a partition of $[a,b]$, and $f$ is bounded on $[a,b]$. Then we define $m_i,M_i$ as the lower bound and upper bound of $f$ on $[x_{i-1},x_i]$, respectively. Also we make summations, $$ L(T,f) = \\sum_{i=1}^n M_i \\Delta x_i, \\quad U(T,f) \\sum_{i=1}^n m_i \\Delta x_i.\r$$ They are called the upper sum and lower sum of $f$ on $[a,b]$ with partition $T$, respectively. Similarly, we can define $$ L(T,f,\\alpha) = \\sum_{i=1}^n M_i \\Delta \\alpha_i, \\quad U(T,f,\\alpha) \\sum_{i=1}^n m_i \\Delta \\alpha_i.\r$$ Theorem 6 $f\\in\\mathcal{R}(\\alpha)$ on $[a,b]$ iff for any $\\epsilon\u003e0$, there exists a partition $T$ s.t. $$ U(T,f,\\alpha) - L(T,f,\\alpha)\u003c \\epsilon.\\tag{3}\r$$ proof For every $T$, we have that $$ L(T,f,\\alpha) \\leq \\underline{\\int} f d\\alpha \\leq \\overline{\\int}fd\\alpha\\leq U(T,f,\\alpha),\r$$ where $\\underline{\\int} f d\\alpha = \\sup_T L(T,f,\\alpha)$ and $\\overline{\\int}fd\\alpha =\\inf_T U(T,f,\\alpha)$. Then the assumption (3) implies that $$ 0\\leq \\overline{\\int}fd\\alpha - \\underline{\\int} f d\\alpha \u003c\\epsilon,\r$$ and hence $\\overline{\\int}fd\\alpha = \\underline{\\int} f d\\alpha$. Conversely, if $f\\in\\mathcal{R}(\\alpha)$, then for every $\\epsilon\u003e0$, there exist partitions $T_1,T_2$ s.t. $$ U(T_2,f,\\alpha) - \\int_a^b f d\\alpha\u003c\\epsilon/2, \\quad \\int_a^b f d\\alpha - L(T_1,f,\\alpha)\u003c\\epsilon/2.\\tag{4}\r$$ We choose $T = T_1\\cup T_2$, then we have $$ U(T,f,\\alpha)\\leq U(T_2,f,\\alpha) \u003c \\int_a^b f d\\alpha + \\epsilon/2 \u003c L(T_1,f,\\alpha) + \\epsilon \\leq L(T,f,\\alpha) + \\epsilon.\r$$$\\Box$\nCorollary 7 If $f$ is continuous on $[a,b]$, then $f\\in\\mathcal{R}(\\alpha)$ on $[a,b]$. proof For any $\\epsilon\u003e0$, we can choose $\\eta\u003e0$ s.t. $$ |\\alpha(b) -\\alpha(a)|\\eta \u003c\\epsilon.\r$$ Since $f$ is continuous on the closed interval $[a,b]$, it is uniformly continuous on $[a,b]$. There exists $\\delta\u003e0$ s.t. for any $x_1,x_2\\in[a,b]$ satisfying $|x_1 - x_2|\u003c\\delta$, $$ |f(x_1) - f(x_2)|\u003c\\eta .\r$$ Given a partition $T$ s.t. $|T|\u003c\\delta$, we have that $U(T,f,\\alpha) - L(T,f,\\alpha)\u003c\\epsilon$.\n$\\Box$\nTheorem 8 If $f$ is monotonic on $[a,b]$, and $\\alpha$ is continuous on $[a,b]$,then $f\\in\\mathcal{R}(\\alpha)$ on $[a,b]$. proof For any $\\epsilon\u003e0,n$, we choose a partition $T$ s.t. $$ \\Delta \\alpha_i = \\frac{\\alpha(b)- \\alpha(a)}{n}.\r$$ W.l.o.g. we assume that $f$ is increasing, then $M_i = f(x_i), m_i = f(x_{i-1})$, and thus $U(T,f,\\alpha) - L(T,f,\\alpha) = \\frac{\\alpha(b)- \\alpha(a)}{n} (f(b) - f(a))\u003c\\epsilon$, by letting $n$ large enough. $\\Box$\nTheorem 9 Suppose $f$ is bounded on $[a,b]$, and has only finitely many points of discontinuity, and $\\alpha$ is continuous at each point where $f$ is discontinuous. Then $f\\in\\mathcal{R}(\\alpha)$ on $[a,b]$. proof Given any $\\epsilon\u003e0$, put $M = \\sup_x |f(x)|$, and let $E$ be the set of points of discontinuity. Then we can cover $E$ by finitely many disjoint intervals $[u_i,v_i]\\subset [a,b]$ s.t. $\\sum |\\alpha(v_i) - \\alpha(u_i)|\u003c\\epsilon$. Let $K:= [a,b]\\setminus \\cup_i (u_i,v_i)$, we can see $K$ is a union of finitely many closed intervals, thus we can find a partition $T_K$ of $K$ s.t. $$ U(T_K,f,\\alpha) - L(T_K,f,\\alpha)\u003c\\epsilon.\r$$ Consider intervals $(u_i,v_i)$, we can defined a partition $T$ of $[a,b]$ by $T = T_K\\cup\\{u_1,v_1,...,u_k,v_k\\}$. It follows that $$ U(T,f,\\alpha) - L(T,f,\\alpha)\\leq U(T_K,f,\\alpha) - L(T_K,f,\\alpha) + 2M\\epsilon\u003c\\epsilon + 2M\\epsilon.\r$$$\\Box$\nTheorem 10 (Fundamental theorem of calculus) If $f\\in\\mathcal{R}$ on $[a,b]$ and if $f$ possesses a primary function $F$, then $$ \\int_a^b f(x)dx = F(b) - F(a).\r$$ proof For any $\\epsilon\u003e0$, choose a partition $T$ s.t. $U(T,f) - L(T,f)\u003c\\epsilon$. Then by the mean value theorem, we have $\\eta_i\\in[x_{i-1},x_i]$ $$ F(x_i) - F(x_{i-1}) = f(\\eta_i)\\Delta x_i.\r$$ Thus $\\sum_{i}^n f(\\eta_i)\\Delta x_i = F(b) - F(a)$. Because $\\sum_{i}^n f(\\xi_i)\\Delta x_i \\in [L,U]$, we obtain that for any $\\xi_i \\in[x_{i-1},x_i]$ $$ |F(b) - F(a) - \\sum_{i}^n f(\\xi_i)\\Delta x_i|\u003c\\epsilon .\r$$ $\\Box$\n3. Properties Theorem 11 (Integral mean value I) If $f$ is continuous on $[a,b]$, then there exists $\\xi\\in[a,b]$ s.t. $$ \\int_a^b f(x) dx = f(\\xi)(b-a).\r$$ proof $m(b-a)\\leq \\int_a^b f(x) dx \\leq M(b-a)$. It follows that $$ m\\leq \\frac{1}{b-a} \\int_a^b f(x) dx \\leq M.\r$$ By the continuity of $f$, we can find $\\xi\\in[a,b]$ s.t. $f(\\xi) = \\frac{1}{b-a} \\int_a^b f(x) dx$. $\\Box$\nTheorem 12 If $f\\in\\mathcal{R}$ on $[a,b]$. For $x\\in[a,b]$, put $$ F(x) = \\int_a^x f(t)dt.\r$$ Then $F$ is continuous on $[a,b]$. Furthermore, if $f$ is continuous at $x_0$, then $F$ is differentiable at $x_0$, with $$ F'(x_0) = f(x_0) .\r$$ proof We know that $f$ is bounded $|f|\\leq M$. Then $|F(y) - F(x)|\\leq M |y-x|$. Given $\\epsilon\u003e0$, we see $|F(y) - F(x)|\u003c\\epsilon$, provided $|y - x|\u003c\\epsilon/M$. This shows the continuity of $F$. Now suppose that $f$ is continuous at $x_0$. We can choose $\\delta\u003e0$ s.t. $|f(t) - f(x_0)|\u003c\\epsilon$ if $|t-x_0|\u003c\\delta$. Hence for any $s,t\\in [a,b]$ s.t. $x_0\\in [s,t]\\subset U(x_0,\\delta)$, we have that $F(t) - F(s) = (t-s) f(\\xi)$ for some $\\xi\\in[s,t]$ by the Integral mean value theorem I. Then, $$ \\bigg|\\frac{F(t) - F(s)}{t-s} - f(x_0)\\bigg| = |f(\\xi) - f(x_0)| \u003c\\epsilon.\r$$$\\Box$\nTheorem 13 (Integral mean value II) Suppose that $f\\in\\mathcal{R}$ on $[a,b]$. If $g$ is decreasing on $[a,b]$ and $g\\geq 0$, then there exists $\\xi\\in [a,b]$ s.t. $$ \\int_a^b f(x)g(x) dx = g(a)\\int_a^\\xi f(x)dx.\r$$ If $g$ is increasing on $[a,b]$ and $g\\geq 0$, then there exists $\\eta\\in [a,b]$ s.t. $$ \\int_a^b f(x)g(x) dx = g(b)\\int_\\eta^b f(x)dx.\r$$ proof We have shown that $F(x) = \\int_a^x f(t)dt$ is continuous on $[a,b]$, and thus bounded. $g(a) = 0$ is trivial so we show the case $g(a)\u003e0$ only. We shall find $\\xi\\in[a,b]$ s.t. $$ F(\\xi) = \\frac{1}{g(a)} \\int_a^b f(x)g(x) dx.\r$$ Now, by the intermediate value theorem, it suffices to show that $$ m\\leq \\frac{1}{g(a)} \\int_a^b f(x)g(x) dx \\leq M.\r$$","permalink":"http://localhost:1313/posts/analysis_1/math_analysis_9/","summary":"\u003ch2 id=\"1-definitions\"\u003e1. Definitions\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 1 (\u003cstrong\u003ePartition\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eLet $[a,b]$ be a given interval, by a \u003ccode\u003epartition\u003c/code\u003e $T$ of $[a,b]$, we mean a finite set of points $x_0,x_1,...,x_n$, where\n$$ \r\n a = x_0\\leq x_1\\leq \\cdots\\leq x_n = b\r\n$$\nWe write $\\Delta x_i = x_i  - x_{i-1}$, and denote $|T| = \\max_i \\Delta x_i$ as the \u003ccode\u003emesh\u003c/code\u003e of the partition $T$.\u003c/dd\u003e\n\u003cdt\u003eDefinition 2 (\u003cstrong\u003eRiemann sum\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eLet $f$ be a given function defined on $[a,b]$, and $T$ be a partition of $[a,b]$. For any points $x_{i-1}\\leq\\xi\\leq x_i$, $i=1,2,...,n$, the summation\n$$ \r\n \\sum_{i=1}^n f(\\xi_i)\\Delta x_i,\r\n$$\nis called the \u003ccode\u003eRiemaan sum\u003c/code\u003e of $f$ on $[a,b]$.\u003c/dd\u003e\n\u003cdt\u003eDefinition 3 (\u003cstrong\u003eRiemann integrable\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eLet $f$ be a given function defined on $[a,b]$, and $J$ be a fixed number. If for any $\\epsilon\u003e0$, there exists $\\delta\u003e0$, s.t. for every partition $T$ of $[a,b]$ with $|T|\u003c\\delta$ and for any choice of $\\{\\xi_i\\}$ we have\n$$ \r\n |\\sum_{i=1}^n f(\\xi_i)\\Delta x_i - J|  \u003c\\epsilon,\\tag{1}\r\n$$\nwe say that the function $f$ is \u003ccode\u003eRiemann integrable\u003c/code\u003e on the interval $[a,b]$, we write $f\\in\\mathcal{R}$ and we denote the $J$ by $J = \\int_a^b f(x) dx$.\u003c/dd\u003e\n\u003cdt\u003eDefinition 4 (Stieltjes integral)\u003c/dt\u003e\n\u003cdd\u003eLet $\\alpha$ be an increasing and bounded function on $[a,b]$, $f:[a,b]\\to \\mathbb{R}$. For each partition $T$ of $[a,b]$, we write $\\Delta \\alpha_i = \\alpha(x_i) - \\alpha(x_{i-1})$. If for any $\\epsilon\u003e0$, there exists $\\delta\u003e0$, s.t. for every partition $T$ of $[a,b]$ with $|T|\u003c\\delta$ and for any choice of $\\{\\xi_i\\}$ we have\n$$ \r\n |\\sum_{i=1}^n f(\\xi_i)\\Delta \\alpha_i - J|  \u003c\\epsilon,\\tag{2}\r\n$$\nwe say that the function $f$ is \u003ccode\u003eStieltjes integrable\u003c/code\u003e w.r.t. $\\alpha$ on the interval $[a,b]$, and write $f\\in \\mathcal{R}(\\alpha)$, denoted by $J = \\int_a^b f(x) d\\alpha(x)$.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003cp\u003eBy taking $\\alpha(x) = x$, the Riemann integral can be viewed as a special case of the Stieltjes integral.\u003c/p\u003e","title":"Mathematical Analysis | 9 Riemann Integrals"},{"content":"1. Definitions Definition 1 (Primary function) Let $f$ and $F$ be functions defined on $I$. If $F'(x) = f(x)$ for every $x\\in I$, we say that $F$ is the primary function of $f$ on $I$. Theorem 2 (Existence) If $f$ is continuous on $I$, then $f$ possesses a primary function $F$ on $I$. Definition 3 (Indefinite integral) The set of all primary functions of $f$ on $I$ is called the indefinite integral of $f$ on $I$, denoted by $\\int f(x) dx= F(x) + C$. 2. Change of Variable and Integration by Parts Theorem 4 (Change of variable) Let $f(x)$ defined on $I$, $\\phi(t)$ differentiable on $J$ and $\\phi(J)\\subset I$. If the indefinite integral $\\int f(x) dx = F(x) + C$ exists, then the indefinite integral $\\int f(\\phi(t))\\phi'(t) dt$ exists too, and $\\int f(\\phi(t))\\phi'(t) dt = F(\\phi(t))+C$ If the function $x = \\phi(t)$ has inverse function $t=\\phi^{-1}(x)$,and $\\int f(x) dx = F(x) + C$ exists, then then the indefinite integral $\\int f(\\phi(t))\\phi'(t) dt = G(t) + C$ exists too, and $\\int f(x) dx =G(\\phi^{-1}(x)) + C$. Theorem 5 (Integration by Parts) If $u(x)$ and $v(x)$ are differentiable, and $\\int v(x)u'(x) dx$ exists, then $\\int v'(x) u(x) dx$ exists too, and $\\int v'(x) u(x) dx = v(x)u(x) - \\int v(x) u'(x) dx$. proof $[u(x)v(x)]' = u'(x)v(x) + v'(x)u(x)$. $\\Box$\n","permalink":"http://localhost:1313/posts/analysis_1/math_analysis_8/","summary":"\u003ch2 id=\"1-definitions\"\u003e1. Definitions\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 1 (\u003cstrong\u003ePrimary function\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eLet $f$ and $F$ be functions defined on $I$. If $F'(x) = f(x)$ for every $x\\in I$, we say that $F$ is the \u003ccode\u003eprimary function\u003c/code\u003e of $f$ on $I$.\u003c/dd\u003e\n\u003cdt\u003eTheorem 2 (\u003cstrong\u003eExistence\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eIf $f$ is continuous on $I$, then $f$ possesses a primary function $F$ on $I$.\u003c/dd\u003e\n\u003cdt\u003eDefinition 3 (\u003cstrong\u003eIndefinite integral\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eThe set of all primary functions of $f$ on $I$ is called the \u003ccode\u003eindefinite integral\u003c/code\u003e of $f$ on $I$, denoted by $\\int f(x) dx= F(x) + C$.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003ch2 id=\"2-change-of-variable-and-integration-by-parts\"\u003e2. Change of Variable and Integration by Parts\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eTheorem 4 (\u003cstrong\u003eChange of variable\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eLet $f(x)$ defined on $I$, $\\phi(t)$ differentiable on $J$ and $\\phi(J)\\subset I$.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003col\u003e\n\u003cli\u003eIf the indefinite integral $\\int f(x) dx = F(x) + C$ exists, then the indefinite integral $\\int f(\\phi(t))\\phi'(t) dt$ exists too, and $\\int f(\\phi(t))\\phi'(t) dt = F(\\phi(t))+C$\u003c/li\u003e\n\u003cli\u003eIf the function $x = \\phi(t)$ has inverse function $t=\\phi^{-1}(x)$,and $\\int f(x) dx = F(x) + C$ exists, then then the indefinite integral $\\int f(\\phi(t))\\phi'(t) dt = G(t) + C$ exists too, and $\\int f(x) dx =G(\\phi^{-1}(x)) + C$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdl\u003e\n\u003cdt\u003eTheorem 5 (\u003cstrong\u003eIntegration by Parts\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eIf $u(x)$ and $v(x)$ are differentiable, and $\\int v(x)u'(x) dx$ exists, then $\\int v'(x) u(x) dx$ exists too, and $\\int v'(x) u(x) dx = v(x)u(x) - \\int v(x) u'(x) dx$.\u003c/dd\u003e\n\u003cdt\u003eproof\u003c/dt\u003e\n\u003cdd\u003e$[u(x)v(x)]' = u'(x)v(x) + v'(x)u(x)$.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003cp\u003e$\\Box$\u003c/p\u003e","title":"Mathematical Analysis | 8 Indefinite Integrals"},{"content":"1. Nested Intervals Theorem Definition 1 (Nested intervals) A sequence of closed intervals $\\{[a_n,b_n]\\}$ is called a sequence of nested closed intervals if $[a_{n+1}, b_{n+1}]\\subset [a_n, b_n]$, $\\lim_{n\\to\\infty}(b_n - a_n) = 0$. Theorem 2 (Nested intervals theorem) Let $\\{[a_n,b_n]\\}$ be a sequence of nested closed intervals, there exist a unique real number $\\xi$ s.t. $\\xi\\in [a_n,b_n]$ for every $n$. proof Notice that $\\{a_n\\}$ is an increasing and bounded sequence, and thus it has limit $\\xi$ s.t. $\\xi\\geq a_n$ for all $n$. Similarly, the sequence $\\{b_n\\}$ also has a limit. Combing (ii) of the definition, we have $$ \\lim_{n\\to\\infty}a_n = \\lim_{n\\to\\infty}b_n = \\xi, $$ and $a_n\\leq \\xi\\leq b_n$ for all $n$. The left work is to show the uniqueness. Suppose that there exists another $\\xi'$ s.t. $a_n\\leq \\xi'\\leq b_n$, we have $$ |\\xi - \\xi'|\\leq b_n - a_n \\to 0, $$ which means that $\\xi = \\xi'$. $\\Box$\n2. Finite Open Covers Definition 3 (Closure point) Let $S$ be a set of real numbers, $\\xi$ be a fixed point, if for any neighbourhood of $x_0$, $U(x_0,\\delta)$, we have $$ |U(x_0,\\delta)\\cap S| = \\infty, $$ we say that $\\xi$ is a closure point of $S$. The definition is equivalent to\n3\u0026rsquo; for any neighbourhood of $x_0$, $U(x_0,\\delta)$, we have $U^o(x_0,\\delta)\\cap S \\neq \\emptyset$.\n3\u0026rsquo;\u0026rsquo; there exists a convergent series $\\{x_n\\}\\subset S$ with distinct elements s.t. $\\lim_{n\\to\\infty} x_n = \\xi$.\nproof 3 $\\Rightarrow$ 3\u0026rsquo; is trivial. 3\u0026rsquo;\u0026rsquo; $\\Rightarrow$ 3 is trivial. We only show 3\u0026rsquo; $\\Rightarrow$ 3\u0026rsquo;\u0026rsquo;. Let $\\delta_1 = 1$, we can find $x_1\\in U^o(\\xi,\\delta_1)\\cap S$, then we let $\\delta_2 = \\min\\{1/2, |\\xi - x_1|\\}$, we can find $\\xi_2\\in U^o(\\xi,\\delta_2)\\cap S$ and $x_1\\neq x_2$, $\\ldots$, let $\\delta_2 = \\min\\{1/n, |\\xi - x_{n-1}|\\}$, we can find $\\xi_2\\in U^o(\\xi,\\delta_2)\\cap S$ and $x_n$ is different from $x_1,...,x_{n-1}$. Continue this procedure, we obtain a sequence of distinct numbers $\\{x_n\\}$ with $|\\xi - x_n|\u003c\\delta_n\\leq 1/n\\to 0$.\nTheorem 4 (Bolzano–Weierstrass theorem) Every bonded sequence of real numbers has at least one closure point. proof method 1: suppose $\\{x_n\\}$ is bounded in $[-M,M] = [a_1,b_1]$, we can separate it into two equally long intervals and at least one of them contains infinitely many elements of the sequence. Apply this procedure iteratively, we can obtain a sequence of nested closed intervals $\\{[a_n,b_n]\\}$ in which every interval contains infinitely many elements of the sequence. By the nested intervals theorem, there exists a unique $\\xi\\in [a_n,b_n]$, $n=1,2,...$, then $\\xi$ is a closure point.\nmethod 2: we can take a subsequence with distinct elements of the sequence. Because it is bounded, we can find a further subsequence convergent to $x_0$, then $x_0$ is a closure point.\n$\\Box$\nDefinition 5 (Open covers) Let $S$ be a set of real numbers, $H$ be a set of open intervals. If $S\\subset H$, we say that $H$ is an open cover of $S$. Furthermore, if $|H|\u003c\\infty$, we say that $H$ is a finite open cover of $S$. Theorem 6 (Heine-Borel finite open cover) Let $H$ be an open cover of a closed interval $[a,b]$, then there exists a finite subset $H_1\\subset H$ that covers $S$. proof By contradiction. If such subset does not exist, we can separate $[a,b]$ equally into two intervals s.t. at least one of them cannot covered by a finite subset of $H$, denote such interval by $[a_1,b_1]$. It obvious that $[a_1,b_1]\\subset [a,b]$ and $b_1 - a_1 = 1/2(b-a)$. Apply this procedure for infinitely times, we can obtain a sequence of closed intervals $\\{[a_n,b_n]\\}$ a.t. $b_n - a_n = 1/2^n(b-a)\\to 0$ and every $[a_n,b_n]$ cannot be covered by a finite subset of $H$.\nBy the nested intervals theorem, there exists a unique $\\xi\\in [a_n,b_n]$ for $n=1,2,...$. Since $H$ is an open cover of $S$, there exists an open interval $(\\alpha,\\beta)\\in H$ s.t. $\\xi \\in (\\alpha,\\beta)$. Then there exists a integer $N$ s.t. for any $n\\geq N$, $[a_n,b_n]\\subset (\\alpha,\\beta)$, which contradicts with the fact that every $[a_n,b_n]$ cannot be covered by any finite subset of $H$.\n$\\Box$\nNote that this theorem can be only applied to closed intervals $[a,b]$.\n3. Equivalence So far we have studied six fundamental theorems about the completeness of the real numbers.\nLeast-upper-bound property. If $S$ has an upper bound, then $\\sup S$ exists. If $S$ has an lower bound, then $\\inf S$ exists. Monotone convergence theorem. If $\\{a_n\\}$ is monotonic and bounded sequence of real numbers, then $\\{a_n\\}$ is convergent. Nested intervals theorem. Heine-Borel finite open cover. Bolzano–Weierstrass theorem. Cauchy completeness. Every Cauchy sequence of real numbers converges. proof $1\\Rightarrow 2\\Rightarrow 3 \\Rightarrow 4$ has been shown. For $4\\Rightarrow 5$, we can show it by contradiction. If it has not closure points, we cannot find an open interval containing infinitely many points of $S$. For $5\\Rightarrow 6$, a Cauchy sequence is bounded. $6\\Rightarrow 1$. Construct a Cauchy sequence of upper bounds of $S$.\n$\\Box$\n","permalink":"http://localhost:1313/posts/analysis_1/math_analysis_7/","summary":"\u003ch2 id=\"1-nested-intervals-theorem\"\u003e1. Nested Intervals Theorem\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 1 (\u003cstrong\u003eNested intervals\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eA sequence of closed intervals $\\{[a_n,b_n]\\}$ is called a sequence of \u003ccode\u003enested closed intervals\u003c/code\u003e if\u003c/dd\u003e\n\u003c/dl\u003e\n\u003col\u003e\n\u003cli\u003e$[a_{n+1}, b_{n+1}]\\subset [a_n, b_n]$,\u003c/li\u003e\n\u003cli\u003e$\\lim_{n\\to\\infty}(b_n - a_n) = 0$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdl\u003e\n\u003cdt\u003eTheorem 2 (\u003cstrong\u003eNested intervals theorem\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eLet $\\{[a_n,b_n]\\}$ be a sequence of nested closed intervals, there exist a unique real number $\\xi$ s.t. $\\xi\\in [a_n,b_n]$ for every $n$.\u003c/dd\u003e\n\u003cdt\u003eproof\u003c/dt\u003e\n\u003cdd\u003eNotice that $\\{a_n\\}$ is an increasing and bounded sequence, and thus it has limit $\\xi$ s.t. $\\xi\\geq a_n$ for all $n$. Similarly, the sequence $\\{b_n\\}$ also has a limit. Combing (ii) of the definition, we have\n$$ \n \\lim_{n\\to\\infty}a_n =   \\lim_{n\\to\\infty}b_n = \\xi,\n$$\nand $a_n\\leq \\xi\\leq b_n$ for all $n$. The left work is to show the uniqueness. Suppose that there exists another $\\xi'$ s.t. $a_n\\leq \\xi'\\leq b_n$, we have\n$$ \n |\\xi - \\xi'|\\leq b_n - a_n \\to 0,\n$$\nwhich means that $\\xi = \\xi'$.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003cp\u003e$\\Box$\u003c/p\u003e","title":"Mathematical Analysis | 7 Completeness of the Real Numbers"},{"content":"1. Rolle\u0026rsquo;s Theorem Theorem 1 (Rolle\u0026rsquo;s theorem) If a function $f$ satisfies $f$ is continuous on a closed interval $[a,b]$, $f$ is differentiable on $(a,b)$, $f(a) = f(b)$, then there exists a number $\\xi\\in(a,b)$ s.t. $f'(\\xi)= 0$.\nproof Because $f$ is continuous on $[a,b]$, $f$ has maximum and minimum, denoted by $M$ and $m$ respectively. If $m= M$, then $f$ is constant, hence the assertion is true. In the case $m","permalink":"http://localhost:1313/posts/analysis_1/math_analysis_6/","summary":"\u003ch2 id=\"1-rolles-theorem\"\u003e1. Rolle\u0026rsquo;s Theorem\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eTheorem 1 (Rolle\u0026rsquo;s theorem)\u003c/dt\u003e\n\u003cdd\u003eIf a function $f$ satisfies\u003c/dd\u003e\n\u003c/dl\u003e\n\u003col\u003e\n\u003cli\u003e$f$ is continuous on a closed interval $[a,b]$,\u003c/li\u003e\n\u003cli\u003e$f$ is differentiable on $(a,b)$,\u003c/li\u003e\n\u003cli\u003e$f(a) = f(b)$,\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ethen there exists a number $\\xi\\in(a,b)$ s.t. $f'(\\xi)= 0$.\u003c/p\u003e\n\u003cdl\u003e\n\u003cdt\u003eproof\u003c/dt\u003e\n\u003cdd\u003eBecause $f$ is continuous on $[a,b]$, $f$ has maximum and minimum, denoted by $M$ and $m$ respectively. If $m= M$, then $f$ is constant, hence the assertion is true. In the case $m\u003cM$, the assumption $f(a) = f(b)$ implies that at least one of the maximum and minimum is achieved at $\\xi\\in(a,b)$. Then the Fermat\u0026rsquo;s theorem completes the proof.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003cp\u003e$\\Box$\u003c/p\u003e","title":"Mathematical Analysis | 6 Mean Value Theorem"},{"content":"1. Derivatives Definition 1 (Derivative at $x_0$) Given a function $f$ defined on an neighbourhood of $x_0$ $U(x_0,\\delta)$, we say that $f$ is differentiable at $x_0$ if the following limit exists $$ \\lim_{x\\to x_0}\\frac{f(x)- f(x_0)}{x-x_0}, $$ denoted by $f'(x_0)$. According to the definition, we can see that if $f$ has derivative at $x_0$, then it is continuous at $x_0$. But the converse is not true.\nDefinition 2 (One-sided derivatives) Given a function $f$ defined on $[x_0,x_0+\\delta)$, then it has a right derivative at $x_0$, if the right limit exists $$ \\lim_{x\\to x_0^+}\\frac{f(x)- f(x_0)}{x-x_0}, $$ denoted by $f'_{+}(x_0)$. Theorem 3 $f'(x_0)$ exists iff both $f'_{+}(x_0)$ and $f'_{-}(x_0)$ exist. Definition 4 (Local maximum) If there is a neighbourhood $U(x_0,\\delta)$ where for any $x\\in U(x_0,\\delta)$ s.t. $$ f(x_0)\\geq f(x), $$ we say that $f$ attains its local maximum at $x_0$. Theorem 5 (Fermat\u0026rsquo;s Theorem) If $f$ has a local extremum at $x_0$ and if it is differentiable at $x_0$, then $f'(x_0) = 0$. 2. Differentials Definition 5 (Differential) We define an increment of $f$ by $\\Delta y = f(x_0 + \\Delta x) - f(x_0)$. If there exists a constant $A$ s.t. $$ \\Delta y = A\\Delta x + o(\\Delta x), $$ we say $f$ has a differential at $x_0$, denoted by $d y|_{x_0} = A\\Delta x$. Theorem 6 $f$ has a differential at $x_0$ iff $f$ is differentiable at $x_0$ and $A = f'(x_0)$. ","permalink":"http://localhost:1313/posts/analysis_1/math_analysis_5/","summary":"\u003ch2 id=\"1-derivatives\"\u003e1. Derivatives\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 1 (Derivative at $x_0$)\u003c/dt\u003e\n\u003cdd\u003eGiven a function $f$ defined on an neighbourhood of $x_0$ $U(x_0,\\delta)$, we say that $f$ is \u003ccode\u003edifferentiable\u003c/code\u003e at $x_0$ if the following limit exists\n$$ \n\\lim_{x\\to x_0}\\frac{f(x)- f(x_0)}{x-x_0}, \n$$\ndenoted by $f'(x_0)$.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003cp\u003eAccording to the definition, we can see that if $f$ has derivative at $x_0$, then it is continuous at $x_0$. But the converse is not true.\u003c/p\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 2 (One-sided derivatives)\u003c/dt\u003e\n\u003cdd\u003eGiven a function $f$ defined on $[x_0,x_0+\\delta)$, then it has a right derivative at $x_0$, if the right limit exists\n$$ \n\\lim_{x\\to x_0^+}\\frac{f(x)- f(x_0)}{x-x_0}, \n$$\ndenoted by $f'_{+}(x_0)$.\u003c/dd\u003e\n\u003cdt\u003eTheorem 3\u003c/dt\u003e\n\u003cdd\u003e$f'(x_0)$ exists iff both $f'_{+}(x_0)$ and $f'_{-}(x_0)$ exist.\u003c/dd\u003e\n\u003cdt\u003eDefinition 4 (Local maximum)\u003c/dt\u003e\n\u003cdd\u003eIf there is a neighbourhood $U(x_0,\\delta)$ where for any $x\\in U(x_0,\\delta)$ s.t.\n$$ \n f(x_0)\\geq f(x),\n$$\nwe say that $f$ attains its local maximum at $x_0$.\u003c/dd\u003e\n\u003cdt\u003eTheorem 5 (Fermat\u0026rsquo;s Theorem)\u003c/dt\u003e\n\u003cdd\u003eIf $f$ has a local extremum at $x_0$ and if it is differentiable at $x_0$, then $f'(x_0) = 0$.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003ch2 id=\"2-differentials\"\u003e2. Differentials\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 5 (Differential)\u003c/dt\u003e\n\u003cdd\u003eWe define an increment of $f$ by $\\Delta y = f(x_0 + \\Delta x) - f(x_0)$. If there exists a constant $A$ s.t.\n$$ \n \\Delta y = A\\Delta x + o(\\Delta x),\n$$\nwe say $f$ has a differential at $x_0$, denoted by $d y|_{x_0} = A\\Delta x$.\u003c/dd\u003e\n\u003cdt\u003eTheorem 6\u003c/dt\u003e\n\u003cdd\u003e$f$ has a differential at $x_0$ iff $f$ is differentiable at $x_0$ and $A = f'(x_0)$.\u003c/dd\u003e\n\u003c/dl\u003e","title":"Mathematical Analysis | 5 Derivatives and Differentiations"},{"content":"1. Definitions Definition 1 (Continuity at $x_0$) Given a function $f$ defined on $U(x_0,\\delta)$. If $\\lim_{x\\to x_0} = f(x_0)$, $f$ is said to be continuous at $x_0$. That $f$ is continuous at $x_0$ implies that we can interchange the order of $\\lim$ and $f$, $$ \\lim_{x\\to x_0} = f(\\lim{x\\to x_0} x).\r$$We introduce a new terminology \u0026ldquo;increment\u0026rdquo;, which is defined by $\\Delta x = x - x_0$ and $\\Delta y = y- y_0 = f(x_0 + \\Delta x) - f(x_0)$. Now the continuity of function $f$ at $x_0$ is equivalent to $$ \\lim_{\\Delta x\\to 0} \\Delta y = 0.\r$$ Definition 2 (Discontinuity at $x_0$) Given a function $f$ defined on $U^o(x_0,\\delta)$. If $f$ is not defined at $x_0$ or $f$ is well defined at $x_0$ but not continuous at $x_0$, $f$ is said to be discontinuous at $x_0$ and $x_0$ is called a discontinuity of $f$. According to the definition, if $x_0$ is a discontinuity of $f$, then one of the following cases must happen:\n$f$ is not defined at $x_0$. $f$ is defined at $x_0$, but the limit of $f$ at $x_0$ does not exist. $f$ is defined at $x_0$ and the limit $\\lim_{x\\to x_0}f(x)$ exists, but $\\lim_{x\\to x_0}f(x) \\neq f(x_0)$. Therefore, a discontinuity of $f$ can be classified into one of the three types:\nremovable discontinuity. Suppose $\\lim_{x\\to x_0}f(x) = A$, then $x_0$ is called a removable discontinuity of $f$ if $f$ is not defined on $x_0$ or $f(x_0)\\neq A$. jump discontinuity. The two one-sided limits of $f$ at $x_0$ exist but they are not equal. essential discontinuity. At least one of the two one-sided limits does not exist. Definition 3 (Continuity on an interval) Given a interval $I$, if function $f$ is continuous at every point in $I$,then $f$ is said to be continuous on $I$. 2. Properties Proposition 4 (Continuous at $x_0$) locally bounded. If $f$ is continuous at $x_0$, then $f$ is bouned in some neighbourhood $U(x_0,\\delta)$.\nlocally signed. If $f$ is continuous at $x_0$, and $f(x_0)\u003e0$, then for any $0","permalink":"http://localhost:1313/posts/analysis_1/math_analysis_4/","summary":"\u003ch2 id=\"1-definitions\"\u003e1. Definitions\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 1 (\u003cstrong\u003eContinuity at $x_0$\u003c/strong\u003e)\u003c/dt\u003e\n\u003cdd\u003eGiven a function $f$ defined on $U(x_0,\\delta)$. If $\\lim_{x\\to x_0} = f(x_0)$, $f$ is said to be \u003cem\u003econtinuous at $x_0$\u003c/em\u003e.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003cp\u003eThat $f$ is continuous at $x_0$ implies that we can interchange the order of $\\lim$ and $f$,\n\u003c/p\u003e\n$$ \r\n \\lim_{x\\to x_0} = f(\\lim{x\\to x_0} x).\r\n$$\u003cp\u003eWe introduce a new terminology \u0026ldquo;increment\u0026rdquo;, which is defined by $\\Delta x = x - x_0$ and $\\Delta y = y- y_0 = f(x_0 + \\Delta x) - f(x_0)$. Now the continuity of function $f$ at $x_0$ is equivalent to\n\u003c/p\u003e","title":"Mathematical Analysis | 4 The Continuity of Functions"},{"content":"1. Limit of Functions Definition 1 (Limit at $\\infty$) Given a function $f$ defined on $[a,\\infty)$ and a constant $A$, if for any $\\epsilon\u003e0$ there exists $M$ s.t. for any $x\\geq M$, $$ |f(x) - A|\u003c\\infty, $$ we say that $f$ has limit $A$ as $x\\to+\\infty$, denoted by $\\lim_{x\\to +\\infty}f(x) = A$. If $\\lim_{x\\to +\\infty}f(x)= \\lim_{x\\to -\\infty}f(x) = A$, we deonte it by $\\lim_{x\\to \\infty}f(x) = A$.\nDefinition 2 (Limit at $x_0$) Given a function $f$ that is well defined on $U^o(x_0,\\delta')$ and a constant $A$, if for any $\\epsilon\u003e0$ there exists a $\\delta$ s.t. $$ |f(x)- A|\u003c\\infty, $$ for any $x\\in U^o(x_0,\\delta)$, we say that $f$ has limit $A$ as $x\\to x_0$, denoted by $\\lim_{x\\to x_0}f(x) = A$. Definition 3 (One-sided limit) Given a function $f$ that is well defined on $U^o_+(x_0,\\delta')$ and a constant $A$, if for any $\\epsilon\u003e0$ there exists a $\\delta$ s.t. $$ |f(x)- A|\u003c\\infty, $$ for any $x\\in U^o_+(x_0,\\delta)$, we say that $f$ has right limit $A$ as $x\\to x_0$ from right, denoted by $\\lim_{x\\to x_0^+}f(x) = A$. Similarly, the left limit can be defined. Theorem 4 $\\lim_{x\\to x_0}f(x) = A\\Leftrightarrow \\lim_{x\\to x_0^+}f(x) = \\lim_{x\\to x_0^-}f(x) = A$. Proposition 5 (Properties) uniqueness. If $\\lim_{x\\to x_0}f(x)$ exists, then the limit is unique. local boundedness. If $\\lim_{x\\to x_0}f(x)$ exists, then $f$ is bouned on $U^o(x_0,\\delta)$ for some $\\delta$. signed. If $\\lim_{x\\to x_0}f(x) = A\u003e0$, then for any $0","permalink":"http://localhost:1313/posts/analysis_1/math_analysis_3/","summary":"\u003ch2 id=\"1-limit-of-functions\"\u003e1. Limit of Functions\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 1 (Limit at $\\infty$)\u003c/dt\u003e\n\u003cdd\u003eGiven a function $f$ defined on $[a,\\infty)$ and a constant $A$, if for any $\\epsilon\u003e0$ there exists $M$ s.t. for any $x\\geq M$,\n$$ \n |f(x) - A|\u003c\\infty,\n$$\nwe say that $f$ has limit $A$ as $x\\to+\\infty$, denoted by $\\lim_{x\\to +\\infty}f(x) = A$.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003cp\u003eIf $\\lim_{x\\to +\\infty}f(x)= \\lim_{x\\to -\\infty}f(x) = A$, we deonte it by $\\lim_{x\\to \\infty}f(x) = A$.\u003c/p\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 2 (Limit at $x_0$)\u003c/dt\u003e\n\u003cdd\u003eGiven a function $f$ that is well defined on $U^o(x_0,\\delta')$ and a constant $A$, if for any $\\epsilon\u003e0$ there exists a $\\delta$ s.t.\n$$ \n |f(x)- A|\u003c\\infty,\n$$\nfor any $x\\in U^o(x_0,\\delta)$, we say that $f$ has limit $A$ as $x\\to x_0$, denoted by $\\lim_{x\\to x_0}f(x) = A$.\u003c/dd\u003e\n\u003cdt\u003eDefinition 3 (One-sided limit)\u003c/dt\u003e\n\u003cdd\u003eGiven a function $f$ that is well defined on $U^o_+(x_0,\\delta')$ and a constant $A$, if for any $\\epsilon\u003e0$ there exists a $\\delta$ s.t.\n$$ \n |f(x)- A|\u003c\\infty,\n$$\nfor any $x\\in U^o_+(x_0,\\delta)$, we say that $f$ has right limit $A$ as $x\\to x_0$ from right, denoted by $\\lim_{x\\to x_0^+}f(x) = A$. Similarly, the left limit can be defined.\u003c/dd\u003e\n\u003cdt\u003eTheorem 4\u003c/dt\u003e\n\u003cdd\u003e$\\lim_{x\\to x_0}f(x) = A\\Leftrightarrow \\lim_{x\\to x_0^+}f(x) = \\lim_{x\\to x_0^-}f(x) = A$.\u003c/dd\u003e\n\u003cdt\u003eProposition 5 (Properties)\u003c/dt\u003e\n\u003cdd\u003e\u003c/dd\u003e\n\u003c/dl\u003e\n\u003col\u003e\n\u003cli\u003euniqueness. If $\\lim_{x\\to x_0}f(x)$ exists, then the limit is unique.\u003c/li\u003e\n\u003cli\u003elocal boundedness. If $\\lim_{x\\to x_0}f(x)$ exists, then $f$ is bouned on $U^o(x_0,\\delta)$ for some $\\delta$.\u003c/li\u003e\n\u003cli\u003esigned. If $\\lim_{x\\to x_0}f(x) = A\u003e0$, then for any $0\u003cr\u003cA$, there exist a $U^o(x_0,\\delta)$ on which $f(x)\u003er\u003e0$.\u003c/li\u003e\n\u003cli\u003eordered. If both $\\lim_{x\\to x_0}f(x)$ and $\\lim_{x\\to x_0}g(x)$ exist, and  $f(x)\\leq g(x)$ on some $U^o(x_0,\\delta)$, then $\\lim_{x\\to x_0}f(x)\\leq \\lim_{x\\to x_0}g(x)$.\u003c/li\u003e\n\u003cli\u003esqueeze. If $\\lim_{x\\to x_0}f(x) =  \\lim_{x\\to x_0}g(x) = A$, and $f(x)\\leq h(x)\\leq g(x)$ on some $U^o(x_0,\\delta)$, then  $\\lim_{x\\to x_0}h(x) = A$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"2-existence-of-limit\"\u003e2. Existence of Limit\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eTheorem 6 (Heine-Cantor)\u003c/dt\u003e\n\u003cdd\u003eSuppose $f$ is well defined on $U^o(x_0,\\delta')$. The limit of $f(x)$ exists iff for any sequence $\\{x_n\\}\\subset U^o(x_0,\\delta')$ that is convergent to $x_0$, the sequence $\\{f(x_n)\\}$  converges to a fixed point.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003cp\u003eProof: \u0026ldquo;$\\Rightarrow$\u0026rdquo; is trivial. We only show the inverse. Suppse that  $f(x_n)\\to A$ as $n\\to\\infty$ for all sequences described as the theorem. If $f(x)$ does not converge to $A$ as $x\\to x_0$, then there exists $\\epsilon_0$ s.t $\\forall \\delta\u003e0$, there is some $x\\in U^o(x_0,\\delta)$ with $|f(x)- A|\\geq \\epsilon_0$. We take $\\delta = \\delta'/n$, $n = 1,2,\\ldots$, and thus obtain a sequence $\\{x_n\\}$ s.t.\n\u003c/p\u003e","title":"Mathematical Analysis | 3 Limit of Functions"},{"content":"1. Limit of Sequences Given a function $f:\\mathbb{N}_{+}\\to \\mathbb{R}$, we call $f( \\mathbb{N}_{+})$ as a sequence, enumerated as $a_1,a_2,\\ldots$, denoted by $\\{a_n\\}$.\nDefinition 1 (Convergence) We say a sequence $\\\\{a_n\\\\}$ is convergent to $a$ if for any $\\epsilon\u003e0$, there exits a constant $N$ s.t. for any $n\\geq N$, $$ |a_n - a|\u003c\\epsilon.\r$$ And the point $a$ is called the limit of the sequence $\\{a_n\\}$, denoted by $\\lim_{n\\to\\infty}a_n = a$. An equivalent definition of the convergence of a sequence is given by,\nDefinition 2 (Convergence 2) The sequence $\\\\{a_n\\\\}$ is convergent to $a$ if for any $\\epsilon\u003e0$, there are at most finite elements $a_n$ outside the neighbourhood $U(a,\\epsilon)$. A sequence $\\{a_n\\}$ is called infinitesimal if $\\lim_{n\\to\\infty}a_n = 0$.\nProposition 3 (Properties) the limit of a sequence is unique. $\\{a_n\\}$ is bounded if $\\lim_{n\\to\\infty}a_n = a$. if $\\lim_{n\\to\\infty}a_n = a\u003e0$, then for any $b\\in(0,a)$ there exists a constant $N$ s.t. for every $n\\geq N$ $$ a_n \u003eb. $$ Given two convergent sequences $\\{a_n\\}$ and $\\{b_n\\}$. If there exist $N$ s.t for any $n\\geq N$, $a_n\\leq b_n$, then $\\lim_{n\\to\\infty}a_n\\leq \\lim_{n\\to\\infty}b_n$. Given sequences $\\{a_n\\}$,$\\{b_n\\}$ and $\\{c_n\\}$, if $\\lim_{n\\to\\infty}a_n = \\lim_{n\\to\\infty}b_n =a$ and there exists $N$ s.t for any $n\\geq N$ $a_n \\leq c_n\\leq b_n,$ then $\\lim_{n\\to\\infty}ac_n = a$. Theorem 4 (Convergence) A sequence $\\{a_n\\}$ is convergent iff every subsequence of $\\{a_n\\}$ is convergent. 2. Existence of the limit of sequence Theorem 5 (Monotone convergence theorem) If $\\{a_n\\}$ is monotonic and bounded sequence of real numbers, then $\\{a_n\\}$ is convergent. Theorem 6 (Dense) If $\\{a_n\\}$ is bounded, then $\\{a_n\\}$ possesses a convergent subsequence. Theorem 7 (Cauchy criterion) A sequence $\\{a_n\\}$ of real numbers is convergent iff it is a Cauchy sequence, i.e. for any $\\epsilon\u003e0$ there exists $N$ s.t. for any $n,m\\geq N$ $$ |a_n - a_m|\u003c\\epsilon.\r$$ ","permalink":"http://localhost:1313/posts/analysis_1/math_analysis_2/","summary":"\u003ch2 id=\"1-limit-of-sequences\"\u003e1. Limit of Sequences\u003c/h2\u003e\n\u003cp\u003eGiven a function $f:\\mathbb{N}_{+}\\to \\mathbb{R}$, we call $f( \\mathbb{N}_{+})$ as a \u003cem\u003esequence\u003c/em\u003e, enumerated as $a_1,a_2,\\ldots$, denoted by $\\{a_n\\}$.\u003c/p\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 1 (Convergence)\u003c/dt\u003e\n\u003cdd\u003eWe say a sequence $\\\\{a_n\\\\}$ is \u003cem\u003econvergent\u003c/em\u003e to $a$ if for any $\\epsilon\u003e0$, there exits a constant $N$ s.t. for any $n\\geq N$,\n$$ \r\n |a_n - a|\u003c\\epsilon.\r\n$$\nAnd the point $a$ is called the \u003cem\u003elimit\u003c/em\u003e of the sequence $\\{a_n\\}$, denoted by $\\lim_{n\\to\\infty}a_n = a$.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003cp\u003eAn equivalent definition of the convergence of a sequence is given by,\u003c/p\u003e","title":"Mathematical Analysis | 2 Limit of Sequences"},{"content":"1. Real Numbers Definition 1 (Bounds) $S\\in\\mathbb{R}$. We say that $S$ is bounded above if there exits $M$ s.t. for every $x\\in S$ $x\\leq M$, and $M$ is called the upper bond of $S$. Similarly, we say that $S$ is bounded from below if $x\\geq M$, and $M$ is called the lower bound of $S$. We say that $S$ is bounded if its upper bounds and lower bounds exist.\nDefinition 2 (Supremum and infimum) Let $\\eta$ be an upper bound of $S$. If for any $\\alpha\u003c\\eta$ there exists $x_0\\in S$ s.t. $x_0\u003e\\alpha$, then $\\eta$ is called the supremum of $S$, denoted by $\\eta = \\sup S$. Similarly, $\\eta$ is the infimum of $S$, denoted by $\\eta = \\inf S$, if for any $\\alpha\u003e\\eta$, there exits $x_0\\in S$ s.t. $x_0\u003c\\alpha$. Theorem 3 (Existence) If $S$ has an upper bound, then $\\sup S$ exists. If $S$ has an lower bound, then $\\inf S$ exists. Corollary 4 Given nonempty sets $A,B$. If $\\forall x\\in A,\\forall y\\in B$ $x\\leq y$, then $\\sup A$ and $\\inf B$ exist. Moreover, $\\sup A\\leq \\inf B$. proof: The existence of $\\sup A$ and $\\inf B$ is immediate by Theorem 3. We show the inequality only. Because $y$ is an upper bound of $A$ for any $y\\in B$, $\\sup A\\leq y$. It suggests that $\\sup A$ is a lower bound of $B$, hence $\\sup A\\leq \\inf B$.\n2. Functions Definition 5 (Functions) A mapping $f: D\\to M$ is a function if for every $x\\in D$, there exists a unique $y\\in M$ s.t. $f(x) = y$. $D$ is called the domain of $f$, $M$ is called the codomain of $f$, and $f(D):=\\{f(x): x\\in D\\}$ is the range of $f$. Definition 6 (Inverse functions) Given a function $f: D\\to M$, if for any $y\\in f(D)$ there is a unique $x\\in D$ s.t. $f(x) = y$, then we can define a function $$ f^{-1}: f(D)\\to D,\r$$ which is called the inverse function of $f$. It is obvious that $f^{-1}$ exists $\\Leftrightarrow$ $f$ is bijective.\nDefinition 7 (Elementary functions) constants, $f(x) = c$. power of $x$, $f(x) = x^a$. exponential, $f(x) = a^x$. logarithm, $f(x) = \\log_a x$. trigonometric, $f(x) = \\sin x$. inverse trigonometric, $f(x) = \\arcsin x$. Definition 8 (Bounded functions) A function $f: D\\to M$ is bounded if there exists a constant $M$ s.t. $f(x)\\leq M$ for every $x\\in D$. Definition 9 (Monotonicity) $f$ is an increasing function if $f(x_1)\\leq f(x_2)$ for every $x_1\u003c x_2$, and is strictly increasing if $f(x_1)","permalink":"http://localhost:1313/posts/analysis_1/math_analysis_1/","summary":"\u003ch2 id=\"1-real-numbers\"\u003e1. Real Numbers\u003c/h2\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 1 (Bounds)\u003c/dt\u003e\n\u003cdd\u003e$S\\in\\mathbb{R}$. We say that $S$ is \u003cem\u003ebounded above\u003c/em\u003e if there exits $M$ s.t. for every $x\\in S$ $x\\leq M$, and $M$ is called the \u003cem\u003eupper bond\u003c/em\u003e of $S$. Similarly, we say that $S$ is \u003cem\u003ebounded from below\u003c/em\u003e if $x\\geq M$, and $M$ is called the \u003cem\u003elower bound\u003c/em\u003e of $S$.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003cp\u003eWe say that $S$ is bounded if its upper bounds and lower bounds exist.\u003c/p\u003e\n\u003cdl\u003e\n\u003cdt\u003eDefinition 2 (Supremum and infimum)\u003c/dt\u003e\n\u003cdd\u003eLet $\\eta$ be an upper bound of $S$. If for any $\\alpha\u003c\\eta$ there exists $x_0\\in S$ s.t. $x_0\u003e\\alpha$, then $\\eta$ is called the \u003cem\u003esupremum\u003c/em\u003e of $S$, denoted by $\\eta = \\sup S$. Similarly, $\\eta$ is the \u003cem\u003einfimum\u003c/em\u003e of $S$, denoted by $\\eta = \\inf S$, if for any $\\alpha\u003e\\eta$, there exits $x_0\\in S$ s.t. $x_0\u003c\\alpha$.\u003c/dd\u003e\n\u003cdt\u003eTheorem 3 (Existence)\u003c/dt\u003e\n\u003cdd\u003eIf $S$ has an upper bound, then $\\sup S$ exists. If $S$ has an lower bound, then $\\inf S$ exists.\u003c/dd\u003e\n\u003cdt\u003eCorollary 4\u003c/dt\u003e\n\u003cdd\u003eGiven nonempty sets $A,B$. If $\\forall x\\in A,\\forall y\\in B$ $x\\leq y$, then $\\sup A$ and $\\inf B$ exist. Moreover, $\\sup A\\leq \\inf B$.\u003c/dd\u003e\n\u003c/dl\u003e\n\u003cp\u003eproof:\nThe existence of $\\sup A$ and $\\inf B$ is immediate by Theorem 3. We show the inequality only. Because $y$ is an upper bound of $A$ for any $y\\in B$, $\\sup A\\leq y$. It suggests that $\\sup A$ is a lower bound of $B$, hence $\\sup A\\leq \\inf B$.\u003c/p\u003e","title":"Mathematical Analysis | 1 Real Numbers and Functions"}]