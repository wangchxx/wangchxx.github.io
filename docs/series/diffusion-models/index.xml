<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Diffusion Models on My Math Notes</title>
    <link>https://wangchxx.github.io/series/diffusion-models/</link>
    <description>Recent content in Diffusion Models on My Math Notes</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Â© Copyright notice</copyright>
    <lastBuildDate>Thu, 24 Oct 2024 00:01:41 +0200</lastBuildDate>
    <atom:link href="https://wangchxx.github.io/series/diffusion-models/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DL | Diffusion Models 4 - Conditional Diffusion Models</title>
      <link>https://wangchxx.github.io/posts/diffusion_4_conditional_df/</link>
      <pubDate>Thu, 24 Oct 2024 00:01:41 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/diffusion_4_conditional_df/</guid>
      <description>&lt;h1 id=&#34;conditional-diffusion-models&#34;&gt;Conditional diffusion models&lt;/h1&gt;&#xA;&lt;p&gt;Given $Y$, the forward-time SDE generates $X_{t}$ conditioned on $Y$ and has $X_{t}\sim p_{t}(\cdot|Y)$:&#xA;&lt;/p&gt;&#xA;$$&#xA;d X_{t} = fdt + gdW_{t}, \quad X_{0} \sim q_{0} = p_{data}(\cdot|Y).&#xA;$$&lt;p&gt;&#xA;The reverse-time SDE generates $\bar{X}_{0}\sim p_{0}(\cdot|Y)=p_{data}(\cdot|Y)$:&#xA;&lt;/p&gt;&#xA;$$&#xA;d\bar{X}_{t} = (f-g^2 \nabla_{x}\log p_{t}(\bar{X}_{t}|Y))dt + gd\bar{W}_{t}, \quad \bar{X}_{T}\sim p_{T}(\cdot|Y)\approx \mathcal{N}(0, \sigma^2_{T} I).&#xA;$$&lt;p&gt;&#xA;So we need to learn the conditional score $\nabla_{x} \log p_{t}(X_{t}|Y)$. But how to do it?&lt;/p&gt;&#xA;&lt;h2 id=&#34;classifier-guidance&#34;&gt;Classifier guidance&lt;/h2&gt;&#xA;&lt;p&gt;By Bayes&amp;rsquo; rule&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>DL | Diffusion Models 2 - Preliminary ODE and SDE</title>
      <link>https://wangchxx.github.io/posts/diffusion_2_preliminary_sde/</link>
      <pubDate>Tue, 22 Oct 2024 00:01:41 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/diffusion_2_preliminary_sde/</guid>
      <description>&lt;h2 id=&#34;basics&#34;&gt;Basics&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;ODE definition&lt;/strong&gt;&#xA;Consider the ordinary differential equation (ODE)&#xA;&lt;/p&gt;&#xA;$$&#xA;\frac{dX}{dt}(t) = f(X(t), t),&#xA;$$&lt;p&gt;&#xA;which we also express as $dX_{t} = f(X_{t}, t)dt$, where $X(t), f(X(t),t)\in  \mathbb{R}^d$. Then, $\{X(t)\}_{t}$ is a deterministic curve.&lt;/p&gt;&#xA;&lt;p&gt;We can define the ODE by the limit&#xA;&lt;/p&gt;&#xA;$$&#xA;X_{k+1} = X_{k} + \Delta _{t} f(X_{k}, k\Delta_{t}), \quad k = 0,1,2,\dots,&#xA;$$&lt;p&gt;&#xA;under $\Delta_{t}\to 0$ with $t = k\Delta_{t}$. Precisely, $\left\{ X_{\left\lfloor  \frac{t}{\Delta_{t}}  \right\rfloor} \right\}_{t} \to \{X_{t}\}_{t}$ uniformly on compact intervals.&lt;/p&gt;</description>
    </item>
    <item>
      <title>DL | Diffusion Models 1 - DDMP</title>
      <link>https://wangchxx.github.io/posts/diffusion_1_ddmp/</link>
      <pubDate>Mon, 21 Oct 2024 04:34:23 +0800</pubDate>
      <guid>https://wangchxx.github.io/posts/diffusion_1_ddmp/</guid>
      <description>&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;&#xA;&lt;p&gt;A diffusion probabilistic model is a parameterized Markov chain that gradually adds noise to the data and then learn to reverse the diffusion process to generate data samples from noise.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;ddmp_graph.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;&#xA;&lt;p&gt;Compared with other AI tasks, image generation is harder, since it does not have a standard answer. To solve this issue, GAN and VAE are propsed.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;GAN uses another model (discriminator) to decide the quality of generated images.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
