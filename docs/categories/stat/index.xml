<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stat on My Math Notes</title>
    <link>https://wangchxx.github.io/categories/stat/</link>
    <description>Recent content in Stat on My Math Notes</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>© Copyright notice</copyright>
    <lastBuildDate>Sat, 21 Aug 2021 04:18:02 +0200</lastBuildDate>
    <atom:link href="https://wangchxx.github.io/categories/stat/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reading | Estimating a smooth function on a large graph (2)</title>
      <link>https://wangchxx.github.io/posts/reading/gp_graph_2/</link>
      <pubDate>Sat, 21 Aug 2021 04:18:02 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/reading/gp_graph_2/</guid>
      <description>&lt;p&gt;This paper follows from the one discussed in the previous article and shows that the introduced estimator achieves the optimal rate.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Kirichenko, A., &amp;amp; van Zanten, H. (2018). Minimax lower bounds for function estimation on graphs. ArXiv:1709.06360 [Math, Stat]. &lt;a href=&#34;http://arxiv.org/abs/1709.06360&#34;&gt;http://arxiv.org/abs/1709.06360&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;1-main-results&#34;&gt;1. Main results&lt;/h2&gt;&#xA;&lt;dl&gt;&#xA;&lt;dt&gt;Theorem 1 (&lt;em&gt;Regression&lt;/em&gt;)&lt;/dt&gt;&#xA;&lt;dd&gt;Under conditions (G), (L) and (S),&#xA;$$ &#xA; \inf_{\hat{f}} \sup_{f\in H^\beta(Q)} \mathbb{E}_f || \hat{f} - f||_n^2 \asymp n^{-2\beta/(2\beta+r)}.&#xA;$$&#xA;where the infimum is taken over all estimators $\hat{f} = \hat{f}(Y_1,...,Y_n)$.&lt;/dd&gt;&#xA;&lt;/dl&gt;&#xA;&lt;p&gt;This theorem shows that the minimax rate for the regression problem on the graph is equal to $n^{-\beta/(2\beta+r)}$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reading | Estimating a smooth function on a large graph (1)</title>
      <link>https://wangchxx.github.io/posts/reading/gp_graph_1/</link>
      <pubDate>Fri, 20 Aug 2021 23:28:03 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/reading/gp_graph_1/</guid>
      <description>&lt;p&gt;This paper proposed an approach to estimating smooth functions on graphs using GP priors.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Kirichenko, A., &amp;amp; van Zanten, H. (2015). Estimating a smooth function on a large graph by Bayesian Laplacian regularisation. ArXiv:1511.02515 [Math, Stat]. &lt;a href=&#34;http://arxiv.org/abs/1511.02515&#34;&gt;http://arxiv.org/abs/1511.02515&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;1-problem-setup&#34;&gt;1. Problem setup&lt;/h2&gt;&#xA;&lt;p&gt;Let $G = (V,E)$ be a connected undirected graph and $A$ its adjacency matrix, $D$ its degree matrix. Then $L = D-A$ is the Laplacian of the graph. Suppose that there is a function $f:[0,1]\to \mathbb{R}$ on the graph. We are interested in the $n$-dimentional vector $f = (f_1,...,f_n)^T$, where $f_i = f(i/n)$. We measure distances and norms of functions using the norm $||\cdot||_n$ defined by&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reading | Bayesian Classification of Multiclass Functional Data</title>
      <link>https://wangchxx.github.io/posts/reading/gp_multiclass_classification/</link>
      <pubDate>Sun, 08 Aug 2021 17:02:19 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/reading/gp_multiclass_classification/</guid>
      <description>&lt;p&gt;Traditionally, we reduce the multiclass classification problem to a binary problem by 1-vs-1 or 1-vs-rest. This article proposed an alternative method to solve the multiclass classification problem.&lt;/p&gt;&#xA;&lt;p&gt;Li, X., &amp;amp; Ghosal, S. (2018). Bayesian Classification of Multiclass Functional Data. ArXiv:1808.00662 [Stat]. &lt;a href=&#34;http://arxiv.org/abs/1808.00662&#34;&gt;http://arxiv.org/abs/1808.00662&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-problem-setup&#34;&gt;1. Problem setup&lt;/h2&gt;&#xA;&lt;p&gt;Consider a response $Y$ taking values $k = 1,...,K$, with functional covariate $(X(t), t\in[0,1])$. The main problem is to estimate the probability $P(Y = k|X)$, which can be modeld by&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reading | Posterior consistency of Gaussian process prior for nonparametric binary regression</title>
      <link>https://wangchxx.github.io/posts/reading/gpconsistency_for_binary_classification/</link>
      <pubDate>Sat, 07 Aug 2021 18:24:27 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/reading/gpconsistency_for_binary_classification/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Ghosal, S., &amp;amp; Roy, A. (2006). Posterior consistency of Gaussian process prior for nonparametric binary regression. The Annals of Statistics, 34(5). &lt;a href=&#34;https://doi.org/10.1214/009053606000000795&#34;&gt;https://doi.org/10.1214/009053606000000795&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The previous article discussed the methodology of GP priors for binary classification. This article would show the consistency results.&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-problem-setup&#34;&gt;1. Problem setup&lt;/h2&gt;&#xA;$$ &#xA; p(x) = P(Y=1|x) = H(\eta(x)).&#xA;$$&lt;p&gt;&#xA;A GP prior with mean function $\mu(x)$ and covariance kernle $\sigma(x,x&#39;)$ is put on the function $\eta$. The covariance kernel is assumed to be of the form&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reading | Nonparametric binary regression using a Gaussian process prior</title>
      <link>https://wangchxx.github.io/posts/reading/gppriors_for_binary_classification/</link>
      <pubDate>Sat, 07 Aug 2021 08:36:14 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/reading/gppriors_for_binary_classification/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s discuss this paper&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Choudhuri, N., Ghosal, S., &amp;amp; Roy, A. (2007). Nonparametric binary regression using a Gaussian process prior. Statistical Methodology, 17.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;1-problem-setup&#34;&gt;1. Problem Setup&lt;/h2&gt;&#xA;&lt;p&gt;Consider a binary classification probem given by&#xA;&lt;/p&gt;&#xA;$$ &#xA; p(x) = P(Y = 1|X=x) = 1- P(Y = -1|X = x).&#xA;$$&lt;p&gt;&#xA;This problem commonly occurs in many fields of application, such as medical and spatial statics. Traditionally, we would model this problem as&#xA;&lt;/p&gt;&#xA;$$ &#xA; p(x) = H(\eta(x)),&#xA;$$&lt;p&gt;&#xA;where $H$ is commonly choosen as a cdf, called the &lt;em&gt;link function&lt;/em&gt;, and $\eta$ can be choosen parametrically or nonparametrically. In this paper, a nonparametric approach was studied, where $H$ was known, and the function $\eta$ was estimated with a Gaussian prior.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian Statistics| Gaussian Process Priors (3)</title>
      <link>https://wangchxx.github.io/posts/bayes/bayes_7_gp3/</link>
      <pubDate>Thu, 05 Aug 2021 00:01:41 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/bayes/bayes_7_gp3/</guid>
      <description>&lt;p&gt;In GP(1) and GP(2), we studied the RKSH and consistency rate of GP priors. In this sectin we are going to consider the smoothness of the target function and see how the smoothness level impacts the consistency rates.&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Suppose that we want to estimate a density function $p_0 \in C^\beta[0,1]$, where $C^\beta[0,1]$ denotes the H$\mathrm{\&#34;o}$lder space of order $\beta$. By Assouad’s method it can shown that no estimator can achieve better rates than $n^{-\beta/(2\beta+1)}$ uniformly, in terms of the distance $d(p_0, p) = ||p_0 - p||_1$. It has been long known that some estimators can achieve this rate given known smoothness level $\beta$, for instance, kernel estimators.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian Statistics| Gaussian Process Priors (2)</title>
      <link>https://wangchxx.github.io/posts/bayes/bayes_6_gp2/</link>
      <pubDate>Thu, 22 Jul 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/bayes/bayes_6_gp2/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;&#xA;&lt;p&gt;In &lt;a href=&#34;https://wangchxx.github.io/posts/bayes/bayes_4_gp1/&#34;&gt;GP(1)&lt;/a&gt; we introduced the RKHS of the GP and some of its properties. The main content of this section is to derive the &lt;strong&gt;posterior contraction rate&lt;/strong&gt; of the GP.&lt;/p&gt;&#xA;&lt;p&gt;At the same time, in &lt;a href=&#34;https://wangchxx.github.io/posts/bayes/bayes_5_contraction/&#34;&gt;Posterior Consistency and Contraction&lt;/a&gt;, we derived the most important conclusion:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://wangchxx.github.io/img_bayes_GP/Contraction_2.PNG&#34; alt=&#34;contraction_thm&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;To show the posterior contraction of GP, we only need to check conditions 5.7-5.9.&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-posterior-contraction&#34;&gt;1. Posterior Contraction&lt;/h2&gt;&#xA;&lt;p&gt;Theorem (GP contraction)&#xA;:&lt;img src=&#34;https://wangchxx.github.io/img_bayes_GP/GPrate_1.PNG&#34; alt=&#34;GP contraction&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian Statistics| Gaussian Process Priors (1)</title>
      <link>https://wangchxx.github.io/posts/bayes/bayes_4_gp1/</link>
      <pubDate>Wed, 21 Jul 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/bayes/bayes_4_gp1/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;&#xA;&lt;p&gt;In the previous chapters, we introduced the &lt;strong&gt;Dirichlet Process (DP) prior&lt;/strong&gt;, which is primarily used as a prior on measure spaces. In this chapter, we will introduce the &lt;strong&gt;Gaussian Process (GP)&lt;/strong&gt;, which can be used as a prior on function spaces. Consider the scenario where we have sample pairs $(X_i,Y_i),i\leq n$. We are interested in studying the relationship between the inputs $X_i$ and the outputs $Y_i$.  A common model for such a relationship is&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian Statistics| Posterior Consistency and Contraction</title>
      <link>https://wangchxx.github.io/posts/bayes/bayes_5_contraction/</link>
      <pubDate>Tue, 20 Jul 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/bayes/bayes_5_contraction/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;&#xA;&lt;p&gt;This chapter discusses the theoretical question of whether non-parametric Bayes methods truly work. In other words, it addresses whether the posterior distribution really converges to the so-called &amp;ldquo;true&amp;rdquo; parameter $\theta_0$. Contraction is a richer concept than consistency, as it also involves the rate of convergence.&lt;/p&gt;&#xA;&lt;p&gt;Dirichlet Process priors and Gaussian Process priors are common non-parametric Bayesian methods, along with their various variants. It would be too cumbersome to discuss the convergence issues of these methods on a case-by-case basis. Therefore, this chapter will focus on some general theory, which can then be applied to specific models as needed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian Statistics| Dirichlet Process</title>
      <link>https://wangchxx.github.io/posts/bayes/bayes_3_dirichlet/</link>
      <pubDate>Mon, 19 Jul 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/bayes/bayes_3_dirichlet/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;&#xA;&lt;p&gt;The Dirichlet Process (DP) is widely used in &lt;strong&gt;Bayesian nonparametrics&lt;/strong&gt;, where it is often treated as a default prior on spaces of probability measures. As a &lt;strong&gt;prior&lt;/strong&gt; on probability measures, the DP reflects our prior belief about the potential distributions of data, with the assumption that the data might be drawn from an infinite mixture of possible underlying distributions. This is why the Dirichlet Process is often called a &lt;strong&gt;nonparametric prior&lt;/strong&gt;: it does not assume a fixed, finite number of parameters but rather allows the model to grow with the complexity of the data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian Statistics| Bernstein-von Mises Theorem</title>
      <link>https://wangchxx.github.io/posts/bayes/bayes_2_bvm/</link>
      <pubDate>Sun, 18 Jul 2021 10:52:59 +0200</pubDate>
      <guid>https://wangchxx.github.io/posts/bayes/bayes_2_bvm/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;&#xA;&lt;p&gt;From a non-Bayesian perspective, when we aim to estimate the parameters of a certain model, we often apply the Central Limit Theorem (CLT) and obtain a result similar to this.&lt;/p&gt;&#xA;$$&#xA;\sqrt{n} (\hat{\theta}_n - \theta_0) \to N(0,\Sigma)&#xA;$$&lt;p&gt;From a Bayesian perspective, a similar conclusion can be drawn, often referred to as the Bayesian Central Limit Theorem. This is precisely the Bernstein-von Mises Theorem (BvM), which will be introduced in this article.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
