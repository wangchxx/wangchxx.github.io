+++
author = "Wang Chaohua"
title = "Bayesian Statistics| Gaussian Process Priors (2)"
date = "2021-07-22T10:52:59+02:00"

tags = [
    "markdown",
]
categories = [
    "stat",
    "ml"
]
series = ["bayesian statistics"]
aliases = ["migrate-from-jekyl"]
math = true

+++

<!-- # Bayesian Statistics| Gaussian Process Priors (2) -->

在 [GP(1)]({{< ref "bayes_4_GP1.md" >}}) 中我们介绍了GP的RKHS，以及一些性质，这一节主要内容是推导GP的posterior contraction rate.



同时，在 [Posterior Consistency and Contraction]({{< ref "bayes_5_Contraction.md" >}}) 中我们得到了一个最重要的结论


![contraction_thm](/img_bayes_GP/Contraction_2.PNG)

To show the posterior contraction of GP, we only need to check conditions 5.7-5.9.
## 1. Posterior Contraction

Theorem (GP contraction)
:![GP contraction](/img_bayes_GP/GPrate_1.PNG)

这三个结论分别对应了上面Theorem 5.19 的那三个条件，因此这个结论意味着 GP contracts at $w_0$ at rate $\epsilon_n$.

Proof
: The assertion (7.7) is an immediate result of Lemma *Small ball*
![small ball](/img_bayes_GP/GP_9.PNG)
Then $Pr(||W-w_0||<2\epsilon_0) \geq e^{-\psi_{w_0}(\epsilon_n)}\geq e^{-n\epsilon_n^2}$.


Set $B_n = \epsilon_n\mathbb{B}_1 + M_n\mathbb{H}_1$, then by Borell's inequality we have 
$$ 
\Pr(W\notin B_n) \leq 1 - \Phi(\alpha_n + M_n),
$$
where $\alpha_n = \Phi^{-1}(e^{-\psi_0(\epsilon_n)})$. Since 
$$ 
 \psi_0(\epsilon_n) \leq \psi_w(\epsilon_n) \leq n\epsilon_n^2,
$$

we have that $\alpha_n \geq -M_n/2$ if $M_n = -2\Phi^{-1}(e^{-Cn \epsilon_n^2})$, for some $C>1$. It follows that 
$$ 
1 - \Phi(\alpha_n + M_n) \leq 1 - \Phi(M_n/2) =  e^{-Cn\epsilon_n^2}.
$$

For the assertion (7.8). Let $h_1,h_2,...,h_N \in M_n \mathbb{B}_n$ be $2\epsilon_n$-separated in terms of the Banach space norm, then the $\epsilon_n$-balls $h_i + \epsilon_n \mathbb{B}_1$ are disjoint, and hence by Lemma *decentered*,

$$ 
\begin{align}
1 \geq \sum_{j=1}^N \Pr(W\in h_j + \epsilon_n\mathbb{B}_1) &\geq \sum_{j=1}^N e^{-1/2 || h_j||^2} \Pr(||W||<\epsilon_n)\cr
    &\geq N e^{-1/2M_n^2} e^{-\psi_0(\epsilon_n)}.
\end{align}
$$

Let $N$ be the maximal size of $2\epsilon_n$-separated elements in $M_n\mathbb{B}_1$, then we have 
$$ 
 N(2\epsilon_n, M_n\mathbb{B}_1, ||\cdot||) \leq N\leq e^{1/2 M_n^2}e^{\psi_0(\epsilon_n)}.
$$
Notice that $d(w, M_n\mathbb{B}_1)\leq \epsilon_n$ for any $w\in B_n$, we have have 
$$ 
 N(3\epsilon_n, B_n, ||\cdot||) \leq   e^{1/2 M_n^2}e^{\psi_0(\epsilon_n)}.
$$
We can finish the proof by choosing $M_n$ s.t. $M_n^2/2 \leq 8C n \epsilon_n^2$.



## 2. Examples

#### Gaussian regression
$$ 
 Y_i = f(x_i) + \epsilon_i.
$$
We can construct a prior for $f$ by setting $f(x) = W_x$, where $(W_x)$ is a GP.

Suppose that observations are generated by the true function $f_0 \in W$, then by the preceding theorem, we can show that the posterior 
$$ 
 \Pi_n(|| f -f_0 || > M \epsilon_n| Y^{(n)}) \to 0
$$
in $P_0^n$-probability for some $M>0$.


#### Density estimation
GP cannot serve as a prior for a density function directly, because it can take negative values. Instead we use a link function, and construct a prior $\Pi$ for density function $p$ by 
$$ 
 p(x) = \frac{e^{W_x}}{\int e^{W_y} dy}.
$$
Because the distance between two densities $p_w, p_h$, corresponding to $w,h\in W$, respectively, satisfies 
$$ 
 d(p_w,p_h) \leq C || w- h ||,
$$
in terms of some norm $||\cdot||$, the posterior can also satisfy 
$$ 
 \Pi_n(d(p, p_0) > M\epsilon_n|  X^{(n)}) \to 0
$$
in $P_0^n$-probability for some $M>0$.


- [1] van der Vaart, A. W., & van Zanten, J. H. (2008). Reproducing kernel Hilbert spaces of Gaussian priors.

- [2] Ghosal, S., & Van der Vaart, A. (2017). Fundamentals of nonparametric Bayesian inference.