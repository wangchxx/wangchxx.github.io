<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>RL | Model-Free: Temporal Difference Learning | My Notes</title>
<meta name="keywords" content="">
<meta name="description" content="Temporal-difference (TD) learning is a combination of Monte Carlo ideas and DP ideas. Like MC methods, TD can learn from raw experience without knowledge of the environment&rsquo;s dynamics. Like DP, TD updates estimates based in part on other learned estimates, without waiting for a final output ($G_t$).
1. TD Prediction
Recall that a simple MC methods for nonstationary environment updates $V(S_t)$ by

$$ 
 V(S_t) \leftarrow V(S_t) &#43; \alpha [ G_t - V(S_t)],
$$
where $G_t$ can only be known when a visit to $S_t$ occurs. The simplest TD method updates $V$ by
">
<meta name="author" content="Chaohua Wang">
<link rel="canonical" href="https://wangchxx.github.io/posts/rl/rl_05_temporal_difference/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://wangchxx.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://wangchxx.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://wangchxx.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://wangchxx.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://wangchxx.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://wangchxx.github.io/posts/rl/rl_05_temporal_difference/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '$', right: '$', display: false},  
      ],
      throwOnError : false
    });
  });
</script>



<meta property="og:title" content="RL | Model-Free: Temporal Difference Learning">
<meta property="og:description" content="Temporal-difference (TD) learning is a combination of Monte Carlo ideas and DP ideas. Like MC methods, TD can learn from raw experience without knowledge of the environment&rsquo;s dynamics. Like DP, TD updates estimates based in part on other learned estimates, without waiting for a final output ($G_t$).
1. TD Prediction
Recall that a simple MC methods for nonstationary environment updates $V(S_t)$ by

$$ 
 V(S_t) \leftarrow V(S_t) &#43; \alpha [ G_t - V(S_t)],
$$
where $G_t$ can only be known when a visit to $S_t$ occurs. The simplest TD method updates $V$ by
">
<meta property="og:type" content="article">
<meta property="og:url" content="https://wangchxx.github.io/posts/rl/rl_05_temporal_difference/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-08-24T18:28:15+02:00">
<meta property="article:modified_time" content="2021-08-24T18:28:15+02:00">


<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RL | Model-Free: Temporal Difference Learning">
<meta name="twitter:description" content="Temporal-difference (TD) learning is a combination of Monte Carlo ideas and DP ideas. Like MC methods, TD can learn from raw experience without knowledge of the environment&rsquo;s dynamics. Like DP, TD updates estimates based in part on other learned estimates, without waiting for a final output ($G_t$).
1. TD Prediction
Recall that a simple MC methods for nonstationary environment updates $V(S_t)$ by

$$ 
 V(S_t) \leftarrow V(S_t) &#43; \alpha [ G_t - V(S_t)],
$$
where $G_t$ can only be known when a visit to $S_t$ occurs. The simplest TD method updates $V$ by
">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://wangchxx.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "RL | Model-Free: Temporal Difference Learning",
      "item": "https://wangchxx.github.io/posts/rl/rl_05_temporal_difference/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RL | Model-Free: Temporal Difference Learning",
  "name": "RL | Model-Free: Temporal Difference Learning",
  "description": "Temporal-difference (TD) learning is a combination of Monte Carlo ideas and DP ideas. Like MC methods, TD can learn from raw experience without knowledge of the environment\u0026rsquo;s dynamics. Like DP, TD updates estimates based in part on other learned estimates, without waiting for a final output ($G_t$).\n1. TD Prediction Recall that a simple MC methods for nonstationary environment updates $V(S_t)$ by $$ V(S_t) \\leftarrow V(S_t) + \\alpha [ G_t - V(S_t)], $$ where $G_t$ can only be known when a visit to $S_t$ occurs. The simplest TD method updates $V$ by ",
  "keywords": [
    
  ],
  "articleBody": "Temporal-difference (TD) learning is a combination of Monte Carlo ideas and DP ideas. Like MC methods, TD can learn from raw experience without knowledge of the environment’s dynamics. Like DP, TD updates estimates based in part on other learned estimates, without waiting for a final output ($G_t$).\n1. TD Prediction Recall that a simple MC methods for nonstationary environment updates $V(S_t)$ by $$ V(S_t) \\leftarrow V(S_t) + \\alpha [ G_t - V(S_t)], $$ where $G_t$ can only be known when a visit to $S_t$ occurs. The simplest TD method updates $V$ by $$ \\begin{equation} V(S_t) \\leftarrow V(S_t) + \\alpha \\big[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\big]. \\end{equation} $$It implies that TD can update $V$ immediately on transition to $S_{t+1}$ and receiving $R_{t+1}$. This TD method is called TD(0), or one-step TD, because it is a special case of the $TD(\\lambda)$, $n$-step TD.\nBecause TD(0) update $V(S_t)$ using an existing estimate $V(S_{t+1})$, we say that is is a bootstrapping method, like DP. We know that $$ \\begin{align} v_\\pi(s) \u0026= \\mathbb{E}_\\pi [G_t|S_t = s] \\cr \u0026= \\mathbb{E}_\\pi [R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t = s]. \\end{align} $$ MC methods use an estimate of (2) as a target, whereas DP methods use an estimate of (3) as a target. The MC methods estimate the expectation in (2) by sampling and TD methods estimate $v_\\pi$ using existing estimate $V(S_{t+1})$. The TD methods combine the sampling of MC with the bootstrapping of DP.\n2. TD($\\lambda$) and $n$-step TD $n$-step returns: $G_t^{(n)} := R_{t+1} + \\cdots + \\gamma^{n-1}R_{t+n} + \\gamma^n V(S_{t+n})$ for $n=1,2,...,\\infty$. Then the $n$-step TD learning is $$ V(S_t) \\leftarrow V(S_t) + \\alpha [ G_t^{(n)} - V(S_t)]. $$ $\\lambda$-returns: $G_t^\\lambda:= (1-\\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1}G_t^{(n)}$. Then the $TD(\\lambda)$ learning is $$ V(S_t) \\leftarrow V(S_t) + \\alpha [ G_t^{\\lambda} - V(S_t)]. $$Like MC, the $\\lambda$-returns can only be computed from complete episodes.\nDefinition 5.1 (TD error) The TD error is defined as $\\delta_t:= R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$. Definition 5.2 (Eligibility trace) The Eligibility trace is defined as $$ \\begin{align*} E_0(s) \u0026= 0 \\cr E_t(s) \u0026= \\gamma \\lambda E_{t-1}(s) + 1_{S_t = s}. \\end{align*} $$ If we view $TD(\\lambda)$ backward, keeping $E_t(s)$ for every $s$ and $\\delta_t$ when updating $V(s)$, the $TD(\\lambda)$ can be written as an online learning algorithm $$ V(s) \\leftarrow V(s) + \\alpha \\delta_t E_t(s). $$Note that if the array $V$ does not change during the episode (episodic offline), then the MC error can be written as a sum of TD errors: $$ \\begin{align} G_t - V(S_t) \u0026= \\delta_t + \\gamma \\delta_{t+1} + \\cdots + \\gamma_{T-t-1}\\delta_{T-1} + \\gamma^{T-t}(G_T - V(S_T)) \\cr \u0026= \\delta_t + \\gamma \\delta_{t+1} + \\cdots + \\gamma_{T-t-1}\\delta_{T-1} + \\gamma^{T-t}(0 - 0) \\cr \u0026= \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k. \\end{align} $$ Theorem 5.3 The sum of offline updates is identical for forward-view and backward-view $TD(\\lambda)$, i.e. $$ \\sum_{t=1}^T \\alpha \\delta_t E_t(s) = \\sum_{t=1}^T \\alpha(G_t^\\lambda - V(S_t)) 1_{S_t = s}. $$ Consider an episode where state $s$ is visited once at time-step $k$. The eligibility trace of TD(1) is $$ E_t(s) = \\gamma^{t-k} 1_{t\\geq k}. $$By the Theorem 5.3, we see that $$ \\sum_{t=1}^{T-1} \\alpha \\delta_t E_t(s) = \\alpha \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k = \\alpha ( G_t - V(S_t)), $$ which is exactly the total update for MC. It implies that TD(1) is roughly equivalent to every-visit MC.\n3. On-policy TD Control :SARSA We now turn to the use of TD prediction methods for the control problem. The first step is to learn an action-value function $q_\\pi$. In the first section, we considered transitions from state to state and learned the value function $v_\\pi$. The idea of estimating $q_\\pi$ is essentially identical to estimating $v_\\pi$. Apply TD(0) to updating $Q(S_t, A_t)$ by $$ \\begin{equation} Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]. \\end{equation} $$ This rules uses the tuple $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$, that makes up a transition from one state-action pair to the next, $(S_t, A_t)\\to R_{t+1} \\to (S_{t+1}, A_{t+1})$. This tuple gives rise to the name SARSA for the algorithm.\nSARSA for On-policy Control Initialize $Q(s,a)$ arbitrarily for all $a,s$, and $Q(S_T,\\cdot) = 0$.\ninitialize a policy $\\pi$ w.r.t. $Q$.\nFor each episode:\nInitialize $S_0$ Choose $A_0$ using the policy $\\pi$. Loop for each step $t = 0,...,T-1$: take action $A_t$, observe $R_{t+1}, S_{t+1}$. choose action $A_{t+1}$ using policy derived from $Q$ (e.g., $\\epsilon$-greedy). incremental update $Q$ by $$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]. $$ Theorem 5.4 (convergence of SARSA) SARSA converges to the optimal action-value function $q_*$, under the following conditions: GLIE sequence of policies $\\pi_t(a|s)$. Robbins-Monro sequence of $\\alpha_t$, $$ \\sum_{t=1}^\\infty \\alpha_t = \\infty, \\quad \\sum_{t=1}^\\infty \\alpha_t^2\u003c\\infty. $$ We can also apply the TD($\\lambda$) idea or $n$-step TD learning to estimating $q_*$.\n$n$-step SARSA: $q_t^{(n)} := R_{t+1} + \\cdots + \\gamma^{n-1}R_{t+n} + \\gamma^n Q(S_{t+n}, A_{t+n})$ for $n=1,2,...,\\infty$. Then the $n$-step SARSA updates $$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [ q_t^{(n)} - Q(S_t, A_t)]. $$ $q^\\lambda$-return: $q_t^\\lambda:= (1-\\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1}q_t^{(n)}$. Then the forward SARSA($\\lambda$) updates $$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [ q_t^{\\lambda} - Q(S_t, A_t)]. $$Note that $q_t^\\lambda$ is unknown until the terminal state. Just like TD($\\lambda$), we use eligibility traces in an online algorithm. We need the eligibility trace for each state-action pair, $$ \\begin{align*} E_0(s,a) \u0026= 0 \\cr E_t(s,a) \u0026= \\gamma \\lambda E_{t-1}(s,a) + 1_{(S_t = s, A_t = a)}. \\end{align*} $$And the TD error $\\delta_t$ now becomes $$ \\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t), $$ and hence we update $Q(s,a)$ by $$ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\delta_t E_t(s,a). $$ SARSA($\\lambda$) for On-policy Control Initialize $Q(s,a)$ arbitrarily for all $a,s$ and $Q(S_T,\\cdot) = 0$.\ninitialize a policy $\\pi$ w.r.t. $Q$.\nFor each episode:\nInitialize $E(s,a) = 0$ for all $a,s$. Initialize $S_0$ Choose $A_0$ using the policy $\\pi$. Loop for each step $t = 0,...,T-1$: take action $A_t$, observe $R_{t+1}, S_{t+1}$. choose action $A_{t+1}$ using policy derived from $Q$ (e.g., $\\epsilon$-greedy). compute $\\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$, update $E(S_t, A_t) += 1 $. incremental update $Q, E$ for all $s,a$ by $$ \\begin{align*} Q(s,a) \u0026\\leftarrow Q(s,a) + \\alpha \\delta_t E(s,a), \\cr E(s,a) \u0026\\leftarrow \\gamma \\lambda E(s,a). \\end{align*} $$ 4. Off-policy TD Control: Q-learning We now apply TD methods to off-policy control. Given the a policy $\\pi$ and a behavior policy $\\mu$, when choosing next action $A_{t+1}$ conditioned on state $S_{t+1}$ using the policy $\\mu$, we also consider alternative successor action $A'$ using the target policy $\\pi$. Then we can update $Q$ by $$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A') - Q(S_t, A_t)], $$ where reward $R_{t+1}$ is a result of action $A_{t+1}$.\nNow we allow both behavior and target policies to improve:\nthe target policy $\\pi$ is greedy w.r.t. $Q(s,a)$. the behavior policy $\\mu$ is e.g. $\\epsilon$-greedy w.r.t. $Q(s,a)$. The Q-learning target then simplifies: $$ \\begin{align*} R_{t+1} + \\gamma Q(S_{t+1}, A') \u0026= R_{t+1} + \\gamma Q(S_{t+1}, \\arg\\max_{a'} Q(S_{t+1}, a')) \\cr \u0026= R_{t+1} + \\max_{a'}\\gamma Q(S_{t+1}, a'). \\end{align*} $$Hence Q-learning becomes $$ \\begin{equation} Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\max_{a'}\\gamma Q(S_{t+1}, a') - Q(S_t, A_t)] \\end{equation} $$ Theorem 5.5 Q-learning control converges to the optimal action-value function $q_*$. Q-learning for Off-policy Control Initialize $Q(s,a)$ arbitrarily for all $a,s$ and $Q(S_T,\\cdot) = 0$.\ninitialize the behavior policy $\\mu$ w.r.t. $Q$.\nFor each episode:\nInitialize $S_0$\nChoose $A_0$ using the policy $\\mu$ (derived from $Q$).\nLoop for each step $t = 0,...,T-1$:\ntake action $A_t$, observe $R_{t+1}, S_{t+1}$. choose action $A_{t+1}$ using policy $\\mu$ derived from $Q$ (e.g., $\\epsilon$-greedy). incremental update $Q$ by $$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\max_{a'}\\gamma Q(S_{t+1}, a') - Q(S_t, A_t)]. $$ $Q(s,a)\\to q_*(s,a)$ by theorem 5.5, then we obtain $\\pi_*$ from $q_*$.\n",
  "wordCount" : "1289",
  "inLanguage": "en",
  "datePublished": "2021-08-24T18:28:15+02:00",
  "dateModified": "2021-08-24T18:28:15+02:00",
  "author":{
    "@type": "Person",
    "name": "Chaohua Wang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://wangchxx.github.io/posts/rl/rl_05_temporal_difference/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My Notes",
    "logo": {
      "@type": "ImageObject",
      "url": "https://wangchxx.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://wangchxx.github.io/" accesskey="h" title="My Notes (Alt + H)">My Notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://wangchxx.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/series/" title="Series">
                    <span>Series</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      RL | Model-Free: Temporal Difference Learning
    </h1>
    <div class="post-meta"><span title='2021-08-24 18:28:15 +0200 +0200'>August 24, 2021</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Chaohua Wang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-td-prediction" aria-label="1. TD Prediction">1. TD Prediction</a></li>
                <li>
                    <a href="#2-tdlambda-and-n-step-td" aria-label="2. TD($\lambda$) and $n$-step TD">2. TD($\lambda$) and $n$-step TD</a></li>
                <li>
                    <a href="#3-on-policy-td-control-sarsa" aria-label="3. On-policy TD Control :SARSA">3. On-policy TD Control :SARSA</a></li>
                <li>
                    <a href="#4-off-policy-td-control-q-learning" aria-label="4. Off-policy TD Control: Q-learning">4. Off-policy TD Control: Q-learning</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Temporal-difference (TD) learning is a combination of Monte Carlo ideas and DP ideas. Like MC methods, TD can learn from raw experience without knowledge of the environment&rsquo;s dynamics. Like DP, TD updates estimates based in part on other learned estimates, without waiting for a final output ($G_t$).</p>
<h2 id="1-td-prediction">1. TD Prediction<a hidden class="anchor" aria-hidden="true" href="#1-td-prediction">#</a></h2>
<p>Recall that a simple MC methods for nonstationary environment updates $V(S_t)$ by
</p>
$$ 
 V(S_t) \leftarrow V(S_t) + \alpha [ G_t - V(S_t)],
$$<p>
where $G_t$ can only be known when a visit to $S_t$ occurs. The simplest TD method updates $V$ by
</p>
$$ 
\begin{equation}
   V(S_t) \leftarrow V(S_t) + \alpha \big[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \big].
\end{equation}
$$<p>It implies that TD can update $V$ immediately on transition to $S_{t+1}$ and receiving $R_{t+1}$. This TD method is called TD(0), or <em>one-step TD</em>, because it is a special case of the $TD(\lambda)$, $n$-step TD.</p>
<p>Because TD(0) update $V(S_t)$ using an existing estimate $V(S_{t+1})$, we say that is is a bootstrapping method, like DP. We know that
</p>
$$ 
 \begin{align}
 v_\pi(s) &= \mathbb{E}_\pi [G_t|S_t = s] \cr 
    &= \mathbb{E}_\pi [R_{t+1} + \gamma v_\pi(S_{t+1})|S_t = s].
 \end{align}
$$<p>
MC methods use an estimate of (2) as a target, whereas DP methods use an estimate of (3) as a target. The MC methods estimate the expectation in (2) by sampling and TD methods estimate $v_\pi$ using existing estimate $V(S_{t+1})$. The TD methods combine the sampling of MC with the bootstrapping of DP.</p>
<h2 id="2-tdlambda-and-n-step-td">2. TD($\lambda$) and $n$-step TD<a hidden class="anchor" aria-hidden="true" href="#2-tdlambda-and-n-step-td">#</a></h2>
<ul>
<li>$n$-step returns: $G_t^{(n)} := R_{t+1} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n  V(S_{t+n})$ for $n=1,2,...,\infty$.</li>
</ul>
<p>Then the $n$-step TD learning is
</p>
$$ 
 V(S_t) \leftarrow V(S_t) + \alpha [ G_t^{(n)} - V(S_t)].
$$<ul>
<li>$\lambda$-returns: $G_t^\lambda:= (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1}G_t^{(n)}$.</li>
</ul>
<p>Then the $TD(\lambda)$ learning is
</p>
$$ 
 V(S_t) \leftarrow V(S_t) + \alpha [ G_t^{\lambda} - V(S_t)].
$$<p>Like MC, the $\lambda$-returns can only be computed from complete episodes.</p>
<dl>
<dt><em>Definition 5.1 (TD error)</em></dt>
<dd>The TD error is defined as $\delta_t:= R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$.</dd>
<dt><em>Definition 5.2 (Eligibility trace)</em></dt>
<dd>The Eligibility trace is defined as
$$ 
 \begin{align*}
 E_0(s) &= 0 \cr
 E_t(s) &= \gamma \lambda E_{t-1}(s) + 1_{S_t = s}.
 \end{align*}
$$</dd>
</dl>
<p>If we view   $TD(\lambda)$ backward, keeping $E_t(s)$ for every $s$ and $\delta_t$ when updating $V(s)$, the $TD(\lambda)$ can be  written as an online learning algorithm
</p>
$$ 
 V(s) \leftarrow V(s) + \alpha \delta_t E_t(s).
$$<p>Note that if the array $V$ does not change during the episode (episodic offline), then the MC error can be written as a sum of TD errors:
</p>
$$ 
\begin{align}
 G_t - V(S_t) &= \delta_t + \gamma \delta_{t+1} + \cdots + \gamma_{T-t-1}\delta_{T-1} + \gamma^{T-t}(G_T - V(S_T))  \cr
    &= \delta_t + \gamma \delta_{t+1} + \cdots + \gamma_{T-t-1}\delta_{T-1} + \gamma^{T-t}(0 - 0)  \cr
    &= \sum_{k=t}^{T-1} \gamma^{k-t} \delta_k.
\end{align}
$$<dl>
<dt><em>Theorem 5.3</em></dt>
<dd>The sum of offline updates is identical for forward-view and backward-view $TD(\lambda)$, i.e.
$$ 
 \sum_{t=1}^T \alpha \delta_t E_t(s) = \sum_{t=1}^T \alpha(G_t^\lambda - V(S_t)) 1_{S_t = s}.
$$</dd>
</dl>
<p>Consider an episode where state $s$ is visited once at time-step $k$. The eligibility trace of TD(1) is
</p>
$$ 
 E_t(s) = \gamma^{t-k} 1_{t\geq k}.
$$<p>By the Theorem 5.3, we see that
</p>
$$ 
 \sum_{t=1}^{T-1} \alpha \delta_t E_t(s) = \alpha  \sum_{k=t}^{T-1} \gamma^{k-t} \delta_k = \alpha ( G_t - V(S_t)),
$$<p>
which is exactly the total update for MC. It implies that TD(1) is roughly equivalent to every-visit MC.</p>
<h2 id="3-on-policy-td-control-sarsa">3. On-policy TD Control :SARSA<a hidden class="anchor" aria-hidden="true" href="#3-on-policy-td-control-sarsa">#</a></h2>
<p>We now turn to the use of TD prediction methods for the control problem. The first step is to learn an action-value function $q_\pi$. In the first section, we considered transitions from state to state and learned the value function $v_\pi$. The idea of estimating $q_\pi$ is essentially identical to estimating $v_\pi$. Apply TD(0) to updating $Q(S_t, A_t)$ by
</p>
$$ 
 \begin{equation}
 Q(S_t, A_t) \leftarrow  Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) -  Q(S_t, A_t)].
 \end{equation}
$$<p>
This rules uses the tuple $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$, that makes up a transition from one state-action pair to the next, $(S_t, A_t)\to R_{t+1} \to (S_{t+1}, A_{t+1})$. This tuple gives rise to the name <em>SARSA</em> for the algorithm.</p>
<hr>
<table>
  <thead>
      <tr>
          <th>SARSA for On-policy Control</th>
      </tr>
  </thead>
  <tbody>
  </tbody>
</table>
<ul>
<li>
<p>Initialize $Q(s,a)$ arbitrarily for all $a,s$, and $Q(S_T,\cdot) = 0$.</p>
</li>
<li>
<p>initialize a policy $\pi$ w.r.t. $Q$.</p>
</li>
<li>
<p>For each episode:</p>
<ul>
<li>Initialize $S_0$</li>
<li>Choose $A_0$ using the policy $\pi$.</li>
<li>Loop for each step $t = 0,...,T-1$:
<ul>
<li>take action $A_t$, observe $R_{t+1}, S_{t+1}$.</li>
<li>choose action $A_{t+1}$ using policy derived from $Q$ (e.g., $\epsilon$-greedy).</li>
<li>incremental update $Q$ by
$$ 
     Q(S_t, A_t) \leftarrow  Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) -  Q(S_t, A_t)].
    $$</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<dl>
<dt><em>Theorem 5.4 (convergence of SARSA)</em></dt>
<dd>SARSA converges to the optimal action-value function $q_*$, under the following conditions:
<ol>
<li>GLIE sequence of policies $\pi_t(a|s)$.</li>
<li>Robbins-Monro sequence of $\alpha_t$,
$$ 
     \sum_{t=1}^\infty \alpha_t = \infty, \quad   \sum_{t=1}^\infty \alpha_t^2<\infty.
    $$</li>
</ol>
</dd>
</dl>
<p>We can also apply the TD($\lambda$) idea or $n$-step TD learning to estimating $q_*$.</p>
<ul>
<li>$n$-step SARSA: $q_t^{(n)} := R_{t+1} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n  Q(S_{t+n}, A_{t+n})$ for $n=1,2,...,\infty$.</li>
</ul>
<p>Then the $n$-step SARSA updates
</p>
$$ 
 Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [ q_t^{(n)} - Q(S_t, A_t)].
$$<ul>
<li>$q^\lambda$-return: $q_t^\lambda:= (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1}q_t^{(n)}$.</li>
</ul>
<p>Then the forward SARSA($\lambda$) updates
</p>
$$ 
 Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [ q_t^{\lambda} - Q(S_t, A_t)].
$$<p>Note that $q_t^\lambda$ is unknown until the terminal state. Just like TD($\lambda$), we use eligibility traces in an online algorithm. We need the eligibility trace for each state-action pair,
</p>
$$ 
 \begin{align*}
 E_0(s,a) &= 0 \cr
 E_t(s,a) &= \gamma \lambda E_{t-1}(s,a) + 1_{(S_t = s, A_t = a)}.
 \end{align*}
$$<p>And the TD error $\delta_t$ now becomes
</p>
$$ 
 \delta_t =   R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t),
$$<p>
and hence we update $Q(s,a)$ by
</p>
$$ 
 Q(s,a) \leftarrow Q(s,a) + \alpha \delta_t E_t(s,a).
$$<hr>
<table>
  <thead>
      <tr>
          <th>SARSA($\lambda$) for On-policy Control</th>
      </tr>
  </thead>
  <tbody>
  </tbody>
</table>
<ul>
<li>
<p>Initialize $Q(s,a)$ arbitrarily for all $a,s$ and $Q(S_T,\cdot) = 0$.</p>
</li>
<li>
<p>initialize a policy $\pi$ w.r.t. $Q$.</p>
</li>
<li>
<p>For each episode:</p>
<ul>
<li>Initialize $E(s,a) = 0$ for all $a,s$.</li>
<li>Initialize $S_0$</li>
<li>Choose $A_0$ using the policy $\pi$.</li>
<li>Loop for each step $t = 0,...,T-1$:
<ul>
<li>take action $A_t$, observe $R_{t+1}, S_{t+1}$.</li>
<li>choose action $A_{t+1}$ using policy derived from $Q$ (e.g., $\epsilon$-greedy).</li>
<li>compute $\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$, update $E(S_t, A_t) += 1 $.</li>
<li>incremental update $Q, E$ for all $s,a$ by
$$ 
    \begin{align*}
     Q(s,a) &\leftarrow Q(s,a) + \alpha \delta_t E(s,a), \cr
     E(s,a) &\leftarrow \gamma \lambda E(s,a).
    \end{align*}
    $$</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="4-off-policy-td-control-q-learning">4. Off-policy TD Control: Q-learning<a hidden class="anchor" aria-hidden="true" href="#4-off-policy-td-control-q-learning">#</a></h2>
<p>We now apply TD methods to off-policy control. Given the a policy $\pi$ and a behavior policy $\mu$, when choosing next action $A_{t+1}$ conditioned on state $S_{t+1}$ using the policy $\mu$, we also consider alternative successor action $A'$ using the target policy $\pi$. Then we can update $Q$ by
</p>
$$ 
   Q(S_t, A_t) \leftarrow  Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A') -  Q(S_t, A_t)],
$$<p>
where reward $R_{t+1}$ is a result of action $A_{t+1}$.</p>
<p>Now we allow both behavior and target policies to improve:</p>
<ul>
<li>the target policy $\pi$ is greedy w.r.t. $Q(s,a)$.</li>
<li>the behavior policy $\mu$ is e.g. $\epsilon$-greedy w.r.t. $Q(s,a)$.</li>
</ul>
<p>The Q-learning target then simplifies:
</p>
$$ 
 \begin{align*}
 R_{t+1} + \gamma Q(S_{t+1}, A') &= R_{t+1} + \gamma Q(S_{t+1}, \arg\max_{a'} Q(S_{t+1}, a')) \cr
    &= R_{t+1} + \max_{a'}\gamma Q(S_{t+1}, a').
 \end{align*}
$$<p>Hence Q-learning becomes
</p>
$$ 
 \begin{equation}
 Q(S_t, A_t) \leftarrow  Q(S_t, A_t) + \alpha [R_{t+1} + \max_{a'}\gamma Q(S_{t+1}, a') -  Q(S_t, A_t)]
 \end{equation}
$$<dl>
<dt><em>Theorem 5.5</em></dt>
<dd>Q-learning control converges to the optimal action-value function $q_*$.</dd>
</dl>
<hr>
<table>
  <thead>
      <tr>
          <th>Q-learning for Off-policy Control</th>
      </tr>
  </thead>
  <tbody>
  </tbody>
</table>
<ul>
<li>
<p>Initialize $Q(s,a)$ arbitrarily for all $a,s$ and $Q(S_T,\cdot) = 0$.</p>
</li>
<li>
<p>initialize the behavior policy $\mu$ w.r.t. $Q$.</p>
</li>
<li>
<p>For each episode:</p>
<ul>
<li>
<p>Initialize $S_0$</p>
</li>
<li>
<p>Choose $A_0$ using the policy $\mu$ (derived from $Q$).</p>
</li>
<li>
<p>Loop for each step $t = 0,...,T-1$:</p>
<ul>
<li>take action $A_t$, observe $R_{t+1}, S_{t+1}$.</li>
<li>choose action $A_{t+1}$ using policy $\mu$ derived from $Q$ (e.g., $\epsilon$-greedy).</li>
<li>incremental update $Q$  by
$$ 
      Q(S_t, A_t) \leftarrow  Q(S_t, A_t) + \alpha [R_{t+1} + \max_{a'}\gamma Q(S_{t+1}, a') -  Q(S_t, A_t)].
    $$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>$Q(s,a)\to q_*(s,a)$ by theorem 5.5, then we obtain $\pi_*$ from $q_*$.</p>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://wangchxx.github.io/posts/rl/rl_06_value_function_approximation/">
    <span class="title">« Prev</span>
    <br>
    <span>RL | Value Function Approximation</span>
  </a>
  <a class="next" href="https://wangchxx.github.io/posts/rl/rl_04_monte_carlo/">
    <span class="title">Next »</span>
    <br>
    <span>RL | Model-Free: Monte Carlo Methods</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://wangchxx.github.io/">My Notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
