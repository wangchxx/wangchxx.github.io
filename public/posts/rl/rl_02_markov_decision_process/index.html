<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>RL | Markov Decision Processes | My Notes</title>
<meta name="keywords" content="">
<meta name="description" content="Notations


$\mathcal{S}$: state space.
$\mathcal{A}$: action space.
$\mathcal{R}$: reward space.


1. Agent-Environment Interaction
On each step, the agent selects an action $A_t\in\mathcal{A}$ based on the state $S_t$, and then the environment reveals a reward $R_{t&#43;1}$ and moves to a new state $S_{t&#43;1}$ corresponding the selected action $A_t$ and state $S_t$.
Write

$$ 
 p(s&#39;,r|s,a) = \Pr(S_t = s&#39;, R_t = r| S_{t-1} = s, A_{t-1} = a).
$$
The function $p$ defines the dynamics of the MDP. Given the four-argument function $p$, one can compute anything else one might want tot know about the environment, such as">
<meta name="author" content="Chaohua Wang">
<link rel="canonical" href="https://wangchxx.github.io/posts/rl/rl_02_markov_decision_process/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://wangchxx.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://wangchxx.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://wangchxx.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://wangchxx.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://wangchxx.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://wangchxx.github.io/posts/rl/rl_02_markov_decision_process/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '$', right: '$', display: false},  
      ],
      throwOnError : false
    });
  });
</script>



<meta property="og:title" content="RL | Markov Decision Processes">
<meta property="og:description" content="Notations


$\mathcal{S}$: state space.
$\mathcal{A}$: action space.
$\mathcal{R}$: reward space.


1. Agent-Environment Interaction
On each step, the agent selects an action $A_t\in\mathcal{A}$ based on the state $S_t$, and then the environment reveals a reward $R_{t&#43;1}$ and moves to a new state $S_{t&#43;1}$ corresponding the selected action $A_t$ and state $S_t$.
Write

$$ 
 p(s&#39;,r|s,a) = \Pr(S_t = s&#39;, R_t = r| S_{t-1} = s, A_{t-1} = a).
$$
The function $p$ defines the dynamics of the MDP. Given the four-argument function $p$, one can compute anything else one might want tot know about the environment, such as">
<meta property="og:type" content="article">
<meta property="og:url" content="https://wangchxx.github.io/posts/rl/rl_02_markov_decision_process/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-08-24T03:00:02+02:00">
<meta property="article:modified_time" content="2021-08-24T03:00:02+02:00">


<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RL | Markov Decision Processes">
<meta name="twitter:description" content="Notations


$\mathcal{S}$: state space.
$\mathcal{A}$: action space.
$\mathcal{R}$: reward space.


1. Agent-Environment Interaction
On each step, the agent selects an action $A_t\in\mathcal{A}$ based on the state $S_t$, and then the environment reveals a reward $R_{t&#43;1}$ and moves to a new state $S_{t&#43;1}$ corresponding the selected action $A_t$ and state $S_t$.
Write

$$ 
 p(s&#39;,r|s,a) = \Pr(S_t = s&#39;, R_t = r| S_{t-1} = s, A_{t-1} = a).
$$
The function $p$ defines the dynamics of the MDP. Given the four-argument function $p$, one can compute anything else one might want tot know about the environment, such as">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://wangchxx.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "RL | Markov Decision Processes",
      "item": "https://wangchxx.github.io/posts/rl/rl_02_markov_decision_process/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RL | Markov Decision Processes",
  "name": "RL | Markov Decision Processes",
  "description": "Notations $\\mathcal{S}$: state space. $\\mathcal{A}$: action space. $\\mathcal{R}$: reward space. 1. Agent-Environment Interaction On each step, the agent selects an action $A_t\\in\\mathcal{A}$ based on the state $S_t$, and then the environment reveals a reward $R_{t+1}$ and moves to a new state $S_{t+1}$ corresponding the selected action $A_t$ and state $S_t$.\nWrite $$ p(s',r|s,a) = \\Pr(S_t = s', R_t = r| S_{t-1} = s, A_{t-1} = a). $$ The function $p$ defines the dynamics of the MDP. Given the four-argument function $p$, one can compute anything else one might want tot know about the environment, such as\n",
  "keywords": [
    
  ],
  "articleBody": "Notations $\\mathcal{S}$: state space. $\\mathcal{A}$: action space. $\\mathcal{R}$: reward space. 1. Agent-Environment Interaction On each step, the agent selects an action $A_t\\in\\mathcal{A}$ based on the state $S_t$, and then the environment reveals a reward $R_{t+1}$ and moves to a new state $S_{t+1}$ corresponding the selected action $A_t$ and state $S_t$.\nWrite $$ p(s',r|s,a) = \\Pr(S_t = s', R_t = r| S_{t-1} = s, A_{t-1} = a). $$ The function $p$ defines the dynamics of the MDP. Given the four-argument function $p$, one can compute anything else one might want tot know about the environment, such as\nstate-transition probabilities. $$ p(s'|s,a) = \\sum_{r\\in\\mathcal{R}} p(s',r|s,a). $$ expected rewards given $(s,a)$. $$ r(s,a) = \\mathbb{E}[R_t|S_{t-1}=s ,A_{t-1} =a] = \\sum_{r} r \\sum_{s'} p(s',r|s,a). $$ expected rewards given $(s,a,s')$. $$ r(s,a,s') = \\mathbb{E}[R_t|S_{t-1}=s ,A_{t-1} =a, S_t = s'] = \\sum_{r} r \\frac{p(s',r|s,a)}{p(s'|s,a)}. $$ The objective is to maximize the expected return, denoted $G_t$, that measures the cumulative reward in the long run. The simplest definition is given by $$ G_t = R_{t+1} + R_{t+2} + \\cdots $$Given a discount factor $\\gamma\\in[0,1]$, we can define the discounted return by $$ G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots = \\sum_{k=1}^\\infty \\gamma^k R_{t+k+1} = R_{t+1} + \\gamma G_{t+1}. $$2. Policies and Value Functions Definition 2.1 (policy) A policy is a mapping $\\pi:\\mathcal{S}\\to \\mu(\\mathcal{A})$ where $\\mu$ is a probability measure. We use the notion $\\pi(a|s)$ to denote the probability that $A_t =a $ given the state $S_t = s$. Definition 2.2 (value function) The value function of a state $s$ under a policy $\\pi$, denoted $v_\\pi(s)$, is the expected return when starting in state $s$ and following the policy $\\pi$. For MDPs, we can define $v_\\pi(s)$ by $$ v_\\pi(s) = \\mathbb{E}_{\\pi}[G_t|S_t = s] = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}| S_t = s \\right]. $$ Definition 2.3 (action-value function) The action-value function of taking action $a$ in state $s$ under a policy $\\pi$, denoted $q_\\pi(s,a)$, is defined as the expected return starting from state $s$, taking the action $a$ and following the policy $\\pi$, i.e. $$ \\begin{align*} q_\\pi(s,a) \u0026= \\mathbb{E}_{\\pi}[G_t|S_t = s, A_t = a] \\cr \u0026= \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}| S_t = s, A_t = a \\right] \\cr \u0026= r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) \\sum_{a'} \\pi(a'|s') q_\\pi(s',a'). \\end{align*} $$ A fundamental property of value functions is that they satisfy recursive relationships $$ \\begin{align} v_\\pi(s) \u0026= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1}|S_t = s] \\cr \u0026= \\sum_a \\pi(a|s) \\sum_{s'}\\sum_{r} p(s',r|s,a) \\left( r + \\gamma \\mathbb{E}_\\pi[G_{t+1}|S_{t+1} = s'] \\right) \\cr \u0026= \\sum_a \\pi(a|s) \\left( r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) v_\\pi(s') \\right). \\end{align} $$Equation (3) is the Bellman equation for $v_\\pi$. It expresses a relationship between the value of a state and the values of its successor states.\n3. Optimal Policies and Optimal Value Functions We would like to find a policy that achieves highest expected return. We say a policy $\\pi$ is better than policy $\\pi'$ if $v_\\pi(s) \\geq v_{\\pi'}(s)$ for all $s\\in\\mathcal{S}$. There is always at least one optimal policy, denoted $\\pi_*$. Then we can define\noptimal value function. $$ v_*(s):= \\max_{\\pi}v_\\pi(s). $$ optimal action-value function. $$ q_*(s,a) = \\max_\\pi q_\\pi(s,a). $$ If there are more than one optimal policies, they share the same value functions and action-value functions.\nThe Bellman equation (2) for the optimal value function $v_*$ can be written in a specific form. $$ \\begin{align} v_*(s) \u0026= \\max_{a} q_{\\pi_*}(s,a) \\cr \u0026= \\max_a \\mathbb{E}_{\\pi_*}[R_{t+1}+ \\gamma G_{t+1}|S_t = s, A_t = a] \\cr \u0026= \\max_a \\mathbb{E}[R_{t+1}+ \\gamma v_*(S_{t+1})|S_t = s, A_t = a] \\cr \u0026= \\max_a \\sum_{s'}\\sum_r p(s',r|s,a)[r+ \\gamma v_*(s')] \\cr \u0026= \\max_a r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) v_*(s') \\end{align} $$The Bellman equation for optimal action-value function $q_*$ is $$ \\begin{align} q_*(s,a) \u0026= \\mathbb{E}[R_{t+1}+ \\gamma v_*(S_{t+1})|S_t = s, A_t = a] \\cr \u0026= \\mathbb{E}[R_{t+1}+ \\gamma \\max_{a'} q_*(S_{t+1},a')|S_t = s, A_t = a] \\cr \u0026=\\sum_{s'}\\sum_{r} p(s',r|s,a) [r + \\gamma \\max_{a'} q_*(s',a')] \\cr \u0026= r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) v_*(s'). \\end{align} $$",
  "wordCount" : "641",
  "inLanguage": "en",
  "datePublished": "2021-08-24T03:00:02+02:00",
  "dateModified": "2021-08-24T03:00:02+02:00",
  "author":{
    "@type": "Person",
    "name": "Chaohua Wang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://wangchxx.github.io/posts/rl/rl_02_markov_decision_process/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My Notes",
    "logo": {
      "@type": "ImageObject",
      "url": "https://wangchxx.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://wangchxx.github.io/" accesskey="h" title="My Notes (Alt + H)">My Notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://wangchxx.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/series/" title="Series">
                    <span>Series</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      RL | Markov Decision Processes
    </h1>
    <div class="post-meta"><span title='2021-08-24 03:00:02 +0200 +0200'>August 24, 2021</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Chaohua Wang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#notations" aria-label="Notations">Notations</a></li>
                <li>
                    <a href="#1-agent-environment-interaction" aria-label="1. Agent-Environment Interaction">1. Agent-Environment Interaction</a></li>
                <li>
                    <a href="#2-policies-and-value-functions" aria-label="2. Policies and Value Functions">2. Policies and Value Functions</a></li>
                <li>
                    <a href="#3-optimal-policies-and-optimal-value-functions" aria-label="3. Optimal Policies and Optimal Value Functions">3. Optimal Policies and Optimal Value Functions</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="notations">Notations<a hidden class="anchor" aria-hidden="true" href="#notations">#</a></h2>
<hr>
<ul>
<li>$\mathcal{S}$: state space.</li>
<li>$\mathcal{A}$: action space.</li>
<li>$\mathcal{R}$: reward space.</li>
</ul>
<hr>
<h2 id="1-agent-environment-interaction">1. Agent-Environment Interaction<a hidden class="anchor" aria-hidden="true" href="#1-agent-environment-interaction">#</a></h2>
<p>On each step, the agent selects an action $A_t\in\mathcal{A}$ based on the state $S_t$, and then the environment reveals a reward $R_{t+1}$ and moves to a new state $S_{t+1}$ corresponding the selected action $A_t$ and state $S_t$.</p>
<p>Write
</p>
$$ 
 p(s',r|s,a) = \Pr(S_t = s', R_t = r| S_{t-1} = s, A_{t-1} = a).
$$<p>
The function $p$ defines the <em>dynamics</em> of the MDP. Given the four-argument function $p$, one can compute anything else one might want tot know about the environment, such as</p>
<ol>
<li>state-transition probabilities.
$$ 
 p(s'|s,a) = \sum_{r\in\mathcal{R}} p(s',r|s,a).
$$</li>
<li>expected rewards given $(s,a)$.
$$ 
 r(s,a) = \mathbb{E}[R_t|S_{t-1}=s ,A_{t-1} =a] = \sum_{r} r \sum_{s'} p(s',r|s,a).
$$</li>
<li>expected rewards given $(s,a,s')$.
$$ 
 r(s,a,s') =   \mathbb{E}[R_t|S_{t-1}=s ,A_{t-1} =a, S_t = s'] = \sum_{r} r \frac{p(s',r|s,a)}{p(s'|s,a)}.
$$</li>
</ol>
<p>The objective is to maximize the expected <em>return</em>, denoted $G_t$, that measures the cumulative reward in the long run. The simplest definition is given by
</p>
$$ 
 G_t = R_{t+1} + R_{t+2} + \cdots
$$<p>Given a discount factor $\gamma\in[0,1]$, we can define the <em>discounted return</em> by
</p>
$$ 
 G_t =   R_{t+1} + \gamma R_{t+2} + \cdots  = \sum_{k=1}^\infty \gamma^k R_{t+k+1} = R_{t+1} + \gamma G_{t+1}.
$$<h2 id="2-policies-and-value-functions">2. Policies and Value Functions<a hidden class="anchor" aria-hidden="true" href="#2-policies-and-value-functions">#</a></h2>
<dl>
<dt><em>Definition 2.1 (policy)</em></dt>
<dd>A policy is a mapping $\pi:\mathcal{S}\to \mu(\mathcal{A})$ where $\mu$ is a probability measure. We use the notion $\pi(a|s)$ to denote the probability that $A_t  =a $ given the state $S_t = s$.</dd>
<dt><em>Definition 2.2 (value function)</em></dt>
<dd>The value function of a state $s$ under a policy $\pi$, denoted $v_\pi(s)$, is the expected return when starting in state $s$ and following the policy $\pi$. For MDPs, we can define $v_\pi(s)$ by
$$ 
 v_\pi(s) = \mathbb{E}_{\pi}[G_t|S_t = s] = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1}| S_t = s \right].
$$</dd>
<dt><em>Definition 2.3 (action-value function)</em></dt>
<dd>The action-value function of taking action $a$ in state $s$ under a policy $\pi$, denoted $q_\pi(s,a)$, is defined as the expected return starting from state $s$, taking the action $a$ and following the policy $\pi$, i.e.
$$ 
\begin{align*}
 q_\pi(s,a) &=    \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a] \cr 
   &= \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1}| S_t = s, A_t = a \right] \cr
   &= r(s,a) + \gamma \sum_{s'} p(s'|s,a) \sum_{a'} \pi(a'|s') q_\pi(s',a').
\end{align*}
$$</dd>
</dl>
<p>A fundamental property of value functions is that they satisfy recursive relationships
</p>
$$ 
 \begin{align}
 v_\pi(s) &= \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1}|S_t = s] \cr
    &= \sum_a \pi(a|s) \sum_{s'}\sum_{r} p(s',r|s,a) \left( r + \gamma \mathbb{E}_\pi[G_{t+1}|S_{t+1} = s']  \right) \cr
    &= \sum_a \pi(a|s) \left( r(s,a) + \gamma \sum_{s'} p(s'|s,a) v_\pi(s') \right).
 \end{align}
$$<p>Equation (3) is the <em>Bellman equation for $v_\pi$</em>. It expresses a relationship between the value of a state and the values of its successor states.</p>
<h2 id="3-optimal-policies-and-optimal-value-functions">3. Optimal Policies and Optimal Value Functions<a hidden class="anchor" aria-hidden="true" href="#3-optimal-policies-and-optimal-value-functions">#</a></h2>
<p>We would like to find a policy that achieves highest expected return. We say a policy $\pi$ is better than policy $\pi'$ if $v_\pi(s) \geq v_{\pi'}(s)$ for all $s\in\mathcal{S}$. There is always at least one optimal policy, denoted $\pi_*$. Then we can define</p>
<ul>
<li>optimal value function.
$$ 
 v_*(s):= \max_{\pi}v_\pi(s).
$$</li>
<li>optimal action-value function.
$$ 
 q_*(s,a) = \max_\pi q_\pi(s,a).
$$</li>
</ul>
<p>If there are more than one optimal policies, they share the same value functions and action-value functions.</p>
<p>The Bellman equation (2) for the optimal value function $v_*$ can be written in a specific form.
</p>
$$ 
 \begin{align}
 v_*(s) &= \max_{a} q_{\pi_*}(s,a) \cr
    &= \max_a \mathbb{E}_{\pi_*}[R_{t+1}+ \gamma G_{t+1}|S_t = s, A_t = a] \cr
    &= \max_a \mathbb{E}[R_{t+1}+ \gamma v_*(S_{t+1})|S_t = s, A_t = a] \cr
    &= \max_a \sum_{s'}\sum_r p(s',r|s,a)[r+ \gamma v_*(s')] \cr
    &= \max_a r(s,a) + \gamma \sum_{s'} p(s'|s,a) v_*(s')
 \end{align}
$$<p>The Bellman equation for optimal action-value function $q_*$ is
</p>
$$ 
 \begin{align}
 q_*(s,a) &= \mathbb{E}[R_{t+1}+ \gamma v_*(S_{t+1})|S_t = s, A_t = a] \cr
    &= \mathbb{E}[R_{t+1}+ \gamma \max_{a'} q_*(S_{t+1},a')|S_t = s, A_t = a] \cr
    &=\sum_{s'}\sum_{r} p(s',r|s,a) [r + \gamma \max_{a'} q_*(s',a')] \cr
    &= r(s,a) + \gamma \sum_{s'} p(s'|s,a) v_*(s').
 \end{align}
$$

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://wangchxx.github.io/posts/rl/rl_03_dynamic_programming/">
    <span class="title">« Prev</span>
    <br>
    <span>RL | Dynamic Programming</span>
  </a>
  <a class="next" href="https://wangchxx.github.io/posts/rl/rl_01_multi_armed_bandits/">
    <span class="title">Next »</span>
    <br>
    <span>RL | Multi-armed Bandits</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://wangchxx.github.io/">My Notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
