<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>RL | Value Function Approximation | My Notes</title>
<meta name="keywords" content="">
<meta name="description" content="So far we have represented value function by a lookup table. But sometimes we may have large MDPs where there are too many states or actions to store in memory. One solution to this problem is estimating value function with function approximation
$$ 
 \hat{v}(s,w) \approx v_\pi(s), \quad \hat{q}(s,a,w) \approx q_\pi(s,a).
$$For the simplicity of notation, we denote the function approximator by $f$. With a differentiable loss function $J_w$, e.g.

$$ 
 J_w = \mathbb{E}_\pi (f(s,w) - v_\pi(s))^2,
$$
we can approximate $v$ or $q$ by GD, i.e.
">
<meta name="author" content="Chaohua Wang">
<link rel="canonical" href="https://wangchxx.github.io/posts/rl/rl_06_value_function_approximation/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://wangchxx.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://wangchxx.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://wangchxx.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://wangchxx.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://wangchxx.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://wangchxx.github.io/posts/rl/rl_06_value_function_approximation/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '$', right: '$', display: false},  
      ],
      throwOnError : false
    });
  });
</script>



<meta property="og:title" content="RL | Value Function Approximation">
<meta property="og:description" content="So far we have represented value function by a lookup table. But sometimes we may have large MDPs where there are too many states or actions to store in memory. One solution to this problem is estimating value function with function approximation
$$ 
 \hat{v}(s,w) \approx v_\pi(s), \quad \hat{q}(s,a,w) \approx q_\pi(s,a).
$$For the simplicity of notation, we denote the function approximator by $f$. With a differentiable loss function $J_w$, e.g.

$$ 
 J_w = \mathbb{E}_\pi (f(s,w) - v_\pi(s))^2,
$$
we can approximate $v$ or $q$ by GD, i.e.
">
<meta property="og:type" content="article">
<meta property="og:url" content="https://wangchxx.github.io/posts/rl/rl_06_value_function_approximation/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-08-25T22:17:58+02:00">
<meta property="article:modified_time" content="2021-08-25T22:17:58+02:00">


<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RL | Value Function Approximation">
<meta name="twitter:description" content="So far we have represented value function by a lookup table. But sometimes we may have large MDPs where there are too many states or actions to store in memory. One solution to this problem is estimating value function with function approximation
$$ 
 \hat{v}(s,w) \approx v_\pi(s), \quad \hat{q}(s,a,w) \approx q_\pi(s,a).
$$For the simplicity of notation, we denote the function approximator by $f$. With a differentiable loss function $J_w$, e.g.

$$ 
 J_w = \mathbb{E}_\pi (f(s,w) - v_\pi(s))^2,
$$
we can approximate $v$ or $q$ by GD, i.e.
">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://wangchxx.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "RL | Value Function Approximation",
      "item": "https://wangchxx.github.io/posts/rl/rl_06_value_function_approximation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RL | Value Function Approximation",
  "name": "RL | Value Function Approximation",
  "description": "So far we have represented value function by a lookup table. But sometimes we may have large MDPs where there are too many states or actions to store in memory. One solution to this problem is estimating value function with function approximation\n$$ \\hat{v}(s,w) \\approx v_\\pi(s), \\quad \\hat{q}(s,a,w) \\approx q_\\pi(s,a). $$For the simplicity of notation, we denote the function approximator by $f$. With a differentiable loss function $J_w$, e.g. $$ J_w = \\mathbb{E}_\\pi (f(s,w) - v_\\pi(s))^2, $$ we can approximate $v$ or $q$ by GD, i.e. ",
  "keywords": [
    
  ],
  "articleBody": "So far we have represented value function by a lookup table. But sometimes we may have large MDPs where there are too many states or actions to store in memory. One solution to this problem is estimating value function with function approximation\n$$ \\hat{v}(s,w) \\approx v_\\pi(s), \\quad \\hat{q}(s,a,w) \\approx q_\\pi(s,a). $$For the simplicity of notation, we denote the function approximator by $f$. With a differentiable loss function $J_w$, e.g. $$ J_w = \\mathbb{E}_\\pi (f(s,w) - v_\\pi(s))^2, $$ we can approximate $v$ or $q$ by GD, i.e. $$ w \\leftarrow w - \\frac{1}{2}\\alpha \\nabla_w J_w, $$ where $\\nabla_w J_w = 2 \\mathbb{E}_{\\pi} [(f(s,w) - v_\\pi(s)) \\nabla_w f(s,w)]$. We assume that samples are generated by policy $\\pi$, then we can estimate the gradient by sampling, $$ \\begin{equation} \\nabla_w J_w = 2(f(s,w) - v_\\pi(s)) \\nabla_w f(s,w). \\end{equation} $$ This is known as SGD.\nThe function approximator can be of various forms, e.g. linear function, neural network, decision tree, etc.\n1. MC and TD with Value Function Approximation So far we have assumed that the target function $v_\\pi$ or $q_\\pi$ is known, but in RL where is no supervisor, on rewards. So, in practice, we do not approximate $v_\\pi$ or $q_\\pi$ directly, but substitute a target for it.\nFor MC, the target is the return $G_t$ $$ \\begin{align*} \\nabla_w J_w \u0026= 2(f(S_t,(A_t),w) - G_t) \\nabla_w f(S_t,(A_t),w), \\cr \\end{align*} $$ For TD($0$), the target is the TD target $R_{t+1} + \\gamma f(S_{t+1},(A_{t+1}), w)$ $$ \\nabla_w J_w = 2(f(S_t,(A_t),w) - (R_{t+1} + \\gamma f(S_{t+1},(A_{t+1}), w))) \\nabla_w f(S_t,(A_t),w), $$ For TD($\\lambda$), the target is the $\\lambda$-return $G_t^\\lambda$ or $q_t^\\lambda$ $$ \\nabla_w J_w = 2(f(S_t,(A_t),w) - G_t^\\lambda) \\nabla_w f(S_t,(A_t),w). $$ When the argument $A_t$ if function $f$ exists, it means that $f$ is an approximator for $q_\\pi(s,a)$.\nThe backward view of TD($\\lambda$) becomes $$ \\begin{align*} E_0(s,(a)) \u0026= 0 \\cr E_t(s,(a)) \u0026= \\gamma \\lambda E_{t-1}(s,(a)) + \\nabla_w f(S_t,(A_t),w)\\cr \\delta_t \u0026= R_{t+1} - \\gamma f(S_{t+1},(A_{t+1}), w) - f(S_t,(A_t),w) \\cr w \u0026\\leftarrow w + \\alpha \\delta_t E_t. \\end{align*} $$2.Batch Methods All the methods we have discussed in the previous section required computation per state transition. In this section we present a method for batch computation.\nTake approximating state-value function $v_\\pi$ as an example, and the case for $q_\\pi$ is essentially the same. Assume that we have a training date set $D$ consisting of state-value pairs $$ D = \\{ (s_1, v_{\\pi,1}),...,(s_n, v_{\\pi,n}) \\}. $$ Define the loss function by $$ J_w = \\frac{1}{n}\\sum_{i=1}^n (v_{\\pi,i} - f(s_i,w))^2 = \\mathbb{E}_D [v_\\pi - f(s,w)]^2. $$The SGD yields $$ w \\leftarrow w - \\frac{1}{2} \\alpha \\nabla_w J_w. $$In particular, if the approximator $f$ is a linear function, i.e. $f(s,w) = x(s)^T w$, we can solve $w$ analytically $$ w = (X^T X)^{-1}X^T v_\\pi. $$However, the training data $D$ can be generated from many policies. To evaluate $q_\\pi$, we must learn off-policy. We use the same idea as $Q$-learning:\nLeast Square TD Q-learning (LSTDQ) Initialize parameters $w$ and target policy $\\pi_{0}$, behavior policy $\\pi$.\nGiven dataset $D$\nRepeat:\nGenerate transitions $(S_t, A_t, R_{t+1}, S_{t+1})$ from policy $\\pi$ and store in replay memory $D$.\nConsider alternative successor action $A' = \\pi_0(S_{t+1})$.\nSample mini-batch from $D$ and update $w$ by SGD $$ w \\leftarrow w - \\alpha(f(S_t, A_t,w) - (R_{t+1} + \\gamma f(S_{t+1}, A', w)))\\nabla_w f $$ update the policy $\\pi_0$ w.r.t. the function $f$ with new $w$.\nWith the policy evaluation, we can solve the control problem by policy iteration.\nLeast Square Policy Iteration Initialize parameters $w$, and derive a policy $\\pi$ w.r.t. $w$. Given the training dataset $D$ Repeat policy evaluation LSTDQ$(\\pi, D)$. policy improvement, update $\\pi$. Example 6.1 (Experience replay in deep Q-networks) Deep Q-networks (DQN) uses deep neural networks as the approximator for the action-value function $q_\\pi(s,a)$. Initialize parameters $w$ and a target action-value function $Q$. Given states $s$, take actions $a$ by $\\epsilon$-greedy w.r.t. $f(s,a,w)$. Store transitions $(S_t, A_t, R_{t+1}, S_{t+1})$ in replay memory $D$. Sample random mini-batch, of size $n$, of transitions $(s,a,r,s')$ from $D$. Compute Q-learning targets w.r.t network $f(s,a,w)$. Compute MSE $$ J_w = \\frac{1}{n} \\sum_{i=1}^n \\left( r + \\gamma \\max_{a'} Q(s',a') - f(s,a,w) \\right)^2. $$ Compute $w$ by SGD, and update the target $Q$ using $f(s,a,w^{old}).$ ",
  "wordCount" : "687",
  "inLanguage": "en",
  "datePublished": "2021-08-25T22:17:58+02:00",
  "dateModified": "2021-08-25T22:17:58+02:00",
  "author":{
    "@type": "Person",
    "name": "Chaohua Wang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://wangchxx.github.io/posts/rl/rl_06_value_function_approximation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My Notes",
    "logo": {
      "@type": "ImageObject",
      "url": "https://wangchxx.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://wangchxx.github.io/" accesskey="h" title="My Notes (Alt + H)">My Notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://wangchxx.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/series/" title="Series">
                    <span>Series</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      RL | Value Function Approximation
    </h1>
    <div class="post-meta"><span title='2021-08-25 22:17:58 +0200 +0200'>August 25, 2021</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Chaohua Wang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-mc-and-td-with-value-function-approximation" aria-label="1. MC and TD with Value Function Approximation">1. MC and TD with Value Function Approximation</a></li>
                <li>
                    <a href="#2batch-methods" aria-label="2.Batch Methods">2.Batch Methods</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>So far we have represented value function by a lookup table. But sometimes we may have large MDPs where there are too many states or actions to store in memory. One solution to this problem is estimating value function with function approximation</p>
$$ 
 \hat{v}(s,w) \approx v_\pi(s), \quad \hat{q}(s,a,w) \approx q_\pi(s,a).
$$<p>For the simplicity of notation, we denote the function approximator by $f$. With a differentiable loss function $J_w$, e.g.
</p>
$$ 
 J_w = \mathbb{E}_\pi (f(s,w) - v_\pi(s))^2,
$$<p>
we can approximate $v$ or $q$ by GD, i.e.
</p>
$$ 
 w \leftarrow w - \frac{1}{2}\alpha \nabla_w J_w,
$$<p>
where $\nabla_w J_w = 2 \mathbb{E}_{\pi} [(f(s,w) - v_\pi(s)) \nabla_w f(s,w)]$. We assume that samples are generated by policy $\pi$, then we can estimate the gradient by sampling,
</p>
$$ 
 \begin{equation}
 \nabla_w J_w = 2(f(s,w) - v_\pi(s)) \nabla_w f(s,w).
 \end{equation}
$$<p>
This is known as SGD.</p>
<p>The function approximator can be of various forms, e.g. linear function, neural network, decision tree, etc.</p>
<h2 id="1-mc-and-td-with-value-function-approximation">1. MC and TD with Value Function Approximation<a hidden class="anchor" aria-hidden="true" href="#1-mc-and-td-with-value-function-approximation">#</a></h2>
<p>So far we have assumed that the target function $v_\pi$ or $q_\pi$ is known, but in RL where is no supervisor, on rewards. So, in practice, we do not approximate $v_\pi$ or $q_\pi$ directly, but substitute a target for it.</p>
<ul>
<li>
<p>For MC, the target is the return $G_t$
</p>
$$ 
\begin{align*}
   \nabla_w J_w &= 2(f(S_t,(A_t),w) - G_t) \nabla_w f(S_t,(A_t),w), \cr
\end{align*}
$$</li>
<li>
<p>For TD($0$), the target is the TD target $R_{t+1} + \gamma f(S_{t+1},(A_{t+1}), w)$
</p>
$$ 
   \nabla_w J_w = 2(f(S_t,(A_t),w) - (R_{t+1} + \gamma f(S_{t+1},(A_{t+1}), w))) \nabla_w f(S_t,(A_t),w),
$$</li>
<li>
<p>For TD($\lambda$), the target is the $\lambda$-return $G_t^\lambda$ or $q_t^\lambda$
</p>
$$ 
   \nabla_w J_w = 2(f(S_t,(A_t),w) - G_t^\lambda) \nabla_w f(S_t,(A_t),w).
$$</li>
</ul>
<p>When the argument $A_t$ if function $f$ exists, it means that $f$ is an approximator for $q_\pi(s,a)$.</p>
<p>The backward view of TD($\lambda$) becomes
</p>
$$ 
 \begin{align*}
 E_0(s,(a)) &= 0 \cr
 E_t(s,(a)) &= \gamma \lambda E_{t-1}(s,(a)) + \nabla_w f(S_t,(A_t),w)\cr
 \delta_t &= R_{t+1} - \gamma f(S_{t+1},(A_{t+1}), w) - f(S_t,(A_t),w) \cr
 w &\leftarrow w + \alpha \delta_t E_t.
 \end{align*}
$$<h2 id="2batch-methods">2.Batch Methods<a hidden class="anchor" aria-hidden="true" href="#2batch-methods">#</a></h2>
<p>All the methods we have discussed in the previous section required computation per state transition. In this section we present a method for batch computation.</p>
<p>Take approximating state-value function $v_\pi$ as an example, and the case for $q_\pi$ is essentially the same.  Assume that we have a training date set $D$ consisting of state-value pairs
</p>
$$ 
 D = \{ (s_1, v_{\pi,1}),...,(s_n, v_{\pi,n}) \}.
$$<p>
Define the loss function by
</p>
$$ 
 J_w = \frac{1}{n}\sum_{i=1}^n (v_{\pi,i} - f(s_i,w))^2 = \mathbb{E}_D [v_\pi - f(s,w)]^2.
$$<p>The SGD yields
</p>
$$ 
 w \leftarrow w - \frac{1}{2} \alpha \nabla_w J_w.
$$<p>In particular, if the approximator $f$ is a linear function, i.e. $f(s,w) = x(s)^T w$, we can solve $w$ analytically
</p>
$$ 
 w = (X^T X)^{-1}X^T v_\pi.
$$<p>However, the training data $D$ can be generated from many policies. To evaluate $q_\pi$, we must learn off-policy. We use the same idea as $Q$-learning:</p>
<hr>
<table>
  <thead>
      <tr>
          <th>Least Square TD Q-learning (LSTDQ)</th>
      </tr>
  </thead>
  <tbody>
  </tbody>
</table>
<ul>
<li>
<p>Initialize parameters $w$ and target policy $\pi_{0}$, behavior policy $\pi$.</p>
</li>
<li>
<p>Given dataset $D$</p>
</li>
<li>
<p>Repeat:</p>
<ul>
<li>
<p>Generate transitions $(S_t, A_t, R_{t+1}, S_{t+1})$ from policy $\pi$ and store in replay memory $D$.</p>
</li>
<li>
<p>Consider alternative successor action $A' = \pi_0(S_{t+1})$.</p>
</li>
<li>
<p>Sample mini-batch from $D$ and update $w$ by SGD
</p>
$$ 
        w \leftarrow w  - \alpha(f(S_t, A_t,w) - (R_{t+1} + \gamma f(S_{t+1}, A', w)))\nabla_w f
    $$</li>
<li>
<p>update the policy $\pi_0$ w.r.t. the function $f$ with new $w$.</p>
</li>
</ul>
</li>
</ul>
<hr>
<p>With the policy evaluation, we can solve the control problem by policy iteration.</p>
<hr>
<table>
  <thead>
      <tr>
          <th>Least Square Policy Iteration</th>
      </tr>
  </thead>
  <tbody>
  </tbody>
</table>
<ul>
<li>Initialize parameters $w$, and derive a policy $\pi$ w.r.t. $w$.</li>
<li>Given the training dataset $D$</li>
<li>Repeat
<ul>
<li>policy evaluation LSTDQ$(\pi, D)$.</li>
<li>policy improvement, update $\pi$.</li>
</ul>
</li>
</ul>
<hr>
<dl>
<dt><em>Example 6.1 (Experience replay in deep Q-networks)</em></dt>
<dd>Deep Q-networks (DQN) uses deep neural networks as the approximator for the action-value function $q_\pi(s,a)$.
<ul>
<li>Initialize parameters $w$ and a target action-value function $Q$.</li>
<li>Given states $s$, take actions $a$ by $\epsilon$-greedy w.r.t. $f(s,a,w)$.</li>
<li>Store transitions $(S_t, A_t, R_{t+1}, S_{t+1})$ in replay memory $D$.</li>
<li>Sample random mini-batch, of size $n$, of transitions $(s,a,r,s')$ from $D$.</li>
<li>Compute Q-learning targets w.r.t network $f(s,a,w)$.</li>
<li>Compute MSE
$$ 
     J_w = \frac{1}{n} \sum_{i=1}^n \left( r + \gamma \max_{a'} Q(s',a') - f(s,a,w) \right)^2.
    $$</li>
<li>Compute $w$ by SGD, and update the target $Q$ using $f(s,a,w^{old}).$</li>
</ul>
</dd>
</dl>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://wangchxx.github.io/posts/rl/rl_07_policy_gradient/">
    <span class="title">« Prev</span>
    <br>
    <span>RL | Policy Gradient Methods</span>
  </a>
  <a class="next" href="https://wangchxx.github.io/posts/rl/rl_05_temporal_difference/">
    <span class="title">Next »</span>
    <br>
    <span>RL | Model-Free: Temporal Difference Learning</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://wangchxx.github.io/">My Notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
