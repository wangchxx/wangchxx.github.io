<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>DL | Diffusion Models 3 - Understanding DDMP from a Unified Prospective | My Notes</title>
<meta name="keywords" content="">
<meta name="description" content="Understand DDMP from VLB
In the Diffusion-models-1 we have introduced the DDMP model

$$
X_t|X_{t-1} = \mathcal{N}(\sqrt{1-\beta_t}X_{t-1}, \beta_t I), \quad X_t|X_0 = \mathcal{N}(\sqrt{\bar{\alpha}_t} X_0, (1-\bar{\alpha}_{t})I),
$$
with $X_{0}\sim q_{0} = p_{data}$, $\alpha_{t} = 1-\beta_{t}$ and $\bar{\alpha}_{t} = \prod_{s=1}^t (1-\beta_{s})$.
The loss function is derived by minimizing the negative log-likelihood $-\log p_{\theta}(X_{0})$ with a variational lower bound $L_{VLB}$

$$
L_{VLB} = \mathbb{E}_{q}\left[ \log  \frac{q(x_{1:T}|x_{0})}{p_{\theta}(x_{0:T})}  \right]  = \sum_{t=0}^T L_{t}
$$
with

$$
L_{t} = D_{KL}(q(x_{t}|x_{t&#43;1},x_{0})\| p_{\theta}(x_{t}|x_{t&#43;1})),\; 1\leq t\leq T-1. 
$$
Since the conditional distribution $X_{t}|(X_{t-1}, X_{0})$ is Gaussian
">
<meta name="author" content="Chaohua Wang">
<link rel="canonical" href="https://wangchxx.github.io/posts/diffusion_3_from_a_unified_prospective/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://wangchxx.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://wangchxx.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://wangchxx.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://wangchxx.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://wangchxx.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://wangchxx.github.io/posts/diffusion_3_from_a_unified_prospective/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '$', right: '$', display: false},  
      ],
      throwOnError : false
    });
  });
</script>



<meta property="og:title" content="DL | Diffusion Models 3 - Understanding DDMP from a Unified Prospective">
<meta property="og:description" content="Understand DDMP from VLB
In the Diffusion-models-1 we have introduced the DDMP model

$$
X_t|X_{t-1} = \mathcal{N}(\sqrt{1-\beta_t}X_{t-1}, \beta_t I), \quad X_t|X_0 = \mathcal{N}(\sqrt{\bar{\alpha}_t} X_0, (1-\bar{\alpha}_{t})I),
$$
with $X_{0}\sim q_{0} = p_{data}$, $\alpha_{t} = 1-\beta_{t}$ and $\bar{\alpha}_{t} = \prod_{s=1}^t (1-\beta_{s})$.
The loss function is derived by minimizing the negative log-likelihood $-\log p_{\theta}(X_{0})$ with a variational lower bound $L_{VLB}$

$$
L_{VLB} = \mathbb{E}_{q}\left[ \log  \frac{q(x_{1:T}|x_{0})}{p_{\theta}(x_{0:T})}  \right]  = \sum_{t=0}^T L_{t}
$$
with

$$
L_{t} = D_{KL}(q(x_{t}|x_{t&#43;1},x_{0})\| p_{\theta}(x_{t}|x_{t&#43;1})),\; 1\leq t\leq T-1. 
$$
Since the conditional distribution $X_{t}|(X_{t-1}, X_{0})$ is Gaussian
">
<meta property="og:type" content="article">
<meta property="og:url" content="https://wangchxx.github.io/posts/diffusion_3_from_a_unified_prospective/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2024-10-23T10:52:59+02:00">
<meta property="article:modified_time" content="2024-10-23T10:52:59+02:00">



<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DL | Diffusion Models 3 - Understanding DDMP from a Unified Prospective">
<meta name="twitter:description" content="Understand DDMP from VLB
In the Diffusion-models-1 we have introduced the DDMP model

$$
X_t|X_{t-1} = \mathcal{N}(\sqrt{1-\beta_t}X_{t-1}, \beta_t I), \quad X_t|X_0 = \mathcal{N}(\sqrt{\bar{\alpha}_t} X_0, (1-\bar{\alpha}_{t})I),
$$
with $X_{0}\sim q_{0} = p_{data}$, $\alpha_{t} = 1-\beta_{t}$ and $\bar{\alpha}_{t} = \prod_{s=1}^t (1-\beta_{s})$.
The loss function is derived by minimizing the negative log-likelihood $-\log p_{\theta}(X_{0})$ with a variational lower bound $L_{VLB}$

$$
L_{VLB} = \mathbb{E}_{q}\left[ \log  \frac{q(x_{1:T}|x_{0})}{p_{\theta}(x_{0:T})}  \right]  = \sum_{t=0}^T L_{t}
$$
with

$$
L_{t} = D_{KL}(q(x_{t}|x_{t&#43;1},x_{0})\| p_{\theta}(x_{t}|x_{t&#43;1})),\; 1\leq t\leq T-1. 
$$
Since the conditional distribution $X_{t}|(X_{t-1}, X_{0})$ is Gaussian
">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://wangchxx.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "DL | Diffusion Models 3 - Understanding DDMP from a Unified Prospective",
      "item": "https://wangchxx.github.io/posts/diffusion_3_from_a_unified_prospective/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "DL | Diffusion Models 3 - Understanding DDMP from a Unified Prospective",
  "name": "DL | Diffusion Models 3 - Understanding DDMP from a Unified Prospective",
  "description": "Understand DDMP from VLB In the Diffusion-models-1 we have introduced the DDMP model $$ X_t|X_{t-1} = \\mathcal{N}(\\sqrt{1-\\beta_t}X_{t-1}, \\beta_t I), \\quad X_t|X_0 = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t} X_0, (1-\\bar{\\alpha}_{t})I), $$ with $X_{0}\\sim q_{0} = p_{data}$, $\\alpha_{t} = 1-\\beta_{t}$ and $\\bar{\\alpha}_{t} = \\prod_{s=1}^t (1-\\beta_{s})$.\nThe loss function is derived by minimizing the negative log-likelihood $-\\log p_{\\theta}(X_{0})$ with a variational lower bound $L_{VLB}$ $$ L_{VLB} = \\mathbb{E}_{q}\\left[ \\log \\frac{q(x_{1:T}|x_{0})}{p_{\\theta}(x_{0:T})} \\right] = \\sum_{t=0}^T L_{t} $$ with $$ L_{t} = D_{KL}(q(x_{t}|x_{t+1},x_{0})\\| p_{\\theta}(x_{t}|x_{t+1})),\\; 1\\leq t\\leq T-1. $$ Since the conditional distribution $X_{t}|(X_{t-1}, X_{0})$ is Gaussian ",
  "keywords": [
    
  ],
  "articleBody": "Understand DDMP from VLB In the Diffusion-models-1 we have introduced the DDMP model $$ X_t|X_{t-1} = \\mathcal{N}(\\sqrt{1-\\beta_t}X_{t-1}, \\beta_t I), \\quad X_t|X_0 = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t} X_0, (1-\\bar{\\alpha}_{t})I), $$ with $X_{0}\\sim q_{0} = p_{data}$, $\\alpha_{t} = 1-\\beta_{t}$ and $\\bar{\\alpha}_{t} = \\prod_{s=1}^t (1-\\beta_{s})$.\nThe loss function is derived by minimizing the negative log-likelihood $-\\log p_{\\theta}(X_{0})$ with a variational lower bound $L_{VLB}$ $$ L_{VLB} = \\mathbb{E}_{q}\\left[ \\log \\frac{q(x_{1:T}|x_{0})}{p_{\\theta}(x_{0:T})} \\right] = \\sum_{t=0}^T L_{t} $$ with $$ L_{t} = D_{KL}(q(x_{t}|x_{t+1},x_{0})\\| p_{\\theta}(x_{t}|x_{t+1})),\\; 1\\leq t\\leq T-1. $$ Since the conditional distribution $X_{t}|(X_{t-1}, X_{0})$ is Gaussian $$ q(x_{t-1}|x_t, x_0) = \\mathcal{N}(\\tilde{\\mu}_t(x_t, x_0), \\tilde{\\beta}_t I), $$ where $$ \\begin{equation} \\tilde{\\mu}_t(x_t, x_0) := \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}x_t +\\frac{\\bar{\\alpha}_{t-1}\\beta_t}{1-\\bar{\\alpha}_t}x_0;\\quad \\tilde{\\beta}_t:= \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}\\beta_t. \\end{equation} $$ If we model the conditional distribution $X_{t-1}|X_{t}$ as a Gaussian $X_{t-1}|X_{t} \\sim \\mathcal{N}(\\mu_\\theta(X_t, t), \\Sigma_\\theta(X_t, t))$, the loss function is basically the KL-divergence between two Gaussian distributions, which has the closed form $$ \\begin{equation} L_{t} = \\frac{1}{2\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\|\\tilde{\\mu}(x_{t},x_{0}) - \\mu_{\\theta}(x_{t},t)\\|^2 + C. \\end{equation} $$ Because $q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha})I)$ we can express $x_{t}|x_{0}$ as $x_{t}(x_{0}, \\epsilon) = \\sqrt{ \\bar{\\alpha}_{t} }x_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t} }\\epsilon$ for $\\epsilon\\sim\\mathcal{N}(0, I)$, therefore we can rewrite $\\tilde{\\mu}_{t}(x_{t}, x_{0})$ as $$ \\tilde{\\mu}_{t} = \\tilde{\\mu}(x_{0}, \\epsilon) = \\frac{1}{\\sqrt{ \\alpha_{t} }}\\left( x_{t} - \\frac{1-\\alpha_{t}}{\\sqrt{ 1-\\bar{\\alpha}_{t} }} \\epsilon \\right). $$ Similarly, we can also model $\\mu_{\\theta}(x_{t}, t)$ as $$ \\mu_{\\theta}(x_{t}, t) = \\frac{1}{\\sqrt{ \\alpha_{t} }}\\left( x_{t} - \\frac{1-\\alpha_{t}}{\\sqrt{ 1-\\bar{\\alpha}_{t} }} \\epsilon_{\\theta}(x_{t,t}) \\right), $$ where $\\epsilon_{\\theta}$ is an approximation of the noise $\\epsilon$. Overall, the loss function $L_{t}$ becomes $$ L_{t} = \\mathbb{E}_{x_{0}, \\epsilon} \\left[\\frac{(1-\\alpha_{t})^2}{2 \\alpha_{t}(1-\\bar{\\alpha}_{t})\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\| \\epsilon - \\epsilon_{\\theta}(\\sqrt{ \\bar{\\alpha}_{t} }x_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t} }\\epsilon,t)\\|^2 \\right] + C. $$Understand DDMP from score matching Consider the random variable $$ Y = X + \\sigma Z, \\quad X\\sim P_{X}, \\quad Z\\sim \\mathcal{N}(0,I). $$ By definition, $p_{Y|X}$ is Gaussian, but in general $p_{X|Y}$ is not Gaussian since we did not assume $p_{X}$ is Gaussian. But $p_{X|Y}$ is approximately Gaussian in the limit of $\\sigma\\to 0$. $$ p_{X|Y}(x|y) \\approx \\mathcal{N}(y + \\sigma^2\\nabla \\log p_{Y}(y), \\sigma^2 I). $$ See Proof of Gaussian. And for $Y = \\gamma X + \\sigma Z$ with $\\gamma \\neq 0$, then in the limit of $\\sigma\\to 0$,\n$$ p_{X|Y}(x|y) \\approx \\mathcal{N}\\left( \\frac{1}{\\gamma}(y + \\sigma^2\\nabla \\log p_{Y}(y)), \\frac{\\sigma^2}{\\gamma^2} I \\right). $$ In the setting of DDMP, we know the forward process is $$ X_t|X_{t-1} = \\mathcal{N}(\\sqrt{1-\\beta_t}X_{t-1}, \\beta_t I), \\quad X_t|X_0 = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t} X_0, (1-\\bar{\\alpha})I), \\quad \\bar{\\alpha}_{t} = \\prod_{s=1}^t(1-\\beta_{s}). $$ If $\\beta_{t}$ is small enough, approximately the backward process is $$ p(x_{t-1}|x_{t}) \\approx \\mathcal{N}(\\mu(x_{t},t), \\beta_{t}I), $$ with $\\mu(x_{t},t) = \\frac{1}{\\sqrt{ 1-\\beta_{t} }}(x_{t} - \\beta_{t}\\nabla \\log p_{t}(x_{t}))$. Similarly, we can a model $p_{\\theta}(x_{t-1}|x_{t})$ to learn it $$ p_{\\theta}(x_{t-1}|x_{t}) = \\mathcal{N}(\\mu_{\\theta}(x_{t}, t), \\Sigma_{\\theta}(x_{t},t)), \\quad \\mu_{\\theta}(x_{t},t) = \\frac{1}{\\sqrt{ 1-\\beta_{t} }}(x_{t} - \\beta_{t}s_{\\theta}(x_{t},t)) $$ Essentially, we are using $s_{\\theta}$ to learn the score function $\\nabla_{x}\\log p$. Naturally, we can think about using the loss function MSE of means or KL-divergence. There is not a big difference between them, so we take KL-divergence as an example $$ \\begin{aligned} \\mathcal{L}(\\theta) \u0026= \\sum_{t=1}^T \\lambda_{t} KL(p(x_{t-1}|x_{t}\\|p_{\\theta}(x_{t-1}|x_{t}))) \\\\ \u0026= \\sum_{t=1}^T\\lambda_{t} \\frac{1}{2\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\|\\mu(x_{t},x_{0}) - \\mu_{\\theta}(x_{t},t)\\|^2 + C\\\\ \u0026= \\sum_{t=1}^T\\lambda_{t} \\frac{(1-\\alpha_{t})^2}{2 \\alpha_{t}\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\| \\nabla_{x}\\log p_{t}(x_{t}) - s_{\\theta}(x_{t},t)\\|^2 + C \\end{aligned} $$ Here we see the form of score matching. However, the score function $\\nabla_{x}\\log p_{t}$ is inaccessible, so we cannot compute the above loss function. Recall that $X_{t}|X_{0}$ is a known Gaussian, thus we can try to use $\\nabla_{x}\\log p_{t|0}(x_{t}|x_{0})$ in the loss function. Moreover, for Gaussian distribution $Y\\sim \\mathcal{N}(\\mu, \\sigma^2 I)$, the score function can be easily computed as $$ \\nabla_{y}\\log p_{Y}(Y) \\overset{\\mathcal{D}}{=} - \\frac{\\epsilon}{\\sigma}, \\quad \\epsilon \\sim \\mathcal{N}(0,I). $$ Like what we have done for DDPM VLB loss function, we use $\\epsilon_{\\theta} = -\\sqrt{ 1-\\bar{\\alpha}_{t} } s_{\\theta}$. Now we can rewrite the loss function as $$ \\begin{aligned} \\mathcal{L}(\\theta) \u0026= \\sum_{t=1}^T\\lambda_{t} \\frac{(1-\\alpha_{t})^2}{2 \\alpha_{t}\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\| \\nabla_{x}\\log p_{t}(x_{t}) - s_{\\theta}(x_{t},t)\\|^2 + C \\\\ \u0026= \\sum_{t=1}^T\\lambda_{t} \\frac{(1-\\alpha_{t})^2}{2 \\alpha_{t}\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\mathbb{E}_{x_{0}\\sim p_{0}} \\left[\\| \\nabla_{x}\\log p_{t|0}(x_{t}|x_{0}) - s_{\\theta}(x_{t},t)\\|^2 \\right] + C \\\\ \u0026= \\sum_{t=1}^T\\lambda_{t} \\frac{(1-\\alpha_{t})^2}{2 \\alpha_{t}\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\mathbb{E}_{x_{0}\\sim p_{0}, \\epsilon\\sim\\mathcal{N}(0,I)} \\left[\\| - \\frac{\\epsilon}{\\sqrt{ 1-\\bar{\\alpha}_{t} }} - s_{\\theta}(x_{t},t)\\|^2 \\right] + C \\\\ \u0026= \\sum_{t=1}^T\\lambda_{t} \\frac{(1-\\alpha_{t})^2}{2 \\alpha_{t} (1-\\bar{\\alpha}_{t})\\|\\Sigma_{\\theta}(x_{t},t)\\|_{2}^2} \\mathbb{E}_{x_{0}\\sim p_{0}, \\epsilon\\sim\\mathcal{N}(0,I)} \\left[\\| \\epsilon- \\epsilon_{\\theta}(\\sqrt{ \\bar{\\alpha}_{t} } x_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t} }\\epsilon,t)\\|^2 \\right] + C, \\\\ \\end{aligned} $$ which is exactly the VLB loss function of DDMP.\nDDMP is VP SDE with SDE sampling DDMP forward process in the limit $\\beta_{t}\\to 0$ $$ X_{t+1} = \\sqrt{ 1-\\beta_{t} } X_{t} + \\sqrt{ \\beta_{t} } Z_{t} \\approx \\left( 1-\\frac{\\beta_{t}}{2} \\right)X_{t} + \\sqrt{ \\beta_{t} }Z_{t}. $$ Therefore when the timestep goes to zero, the DDMP forward process converges to the following VP SDE: $$ dX_{t} = -\\frac{1}{2}\\beta_{t} X_{t} dt + \\sqrt{ \\beta_{t} }dW_{t}. $$DDMP sampling process is $$ \\bar{X}_{t-1} = \\frac{1}{\\sqrt{ 1-\\beta_{t} }}\\left( \\bar{X}_{t} - \\frac{\\beta_{t}}{\\sqrt{ 1-\\bar{\\alpha}_{t} }} \\epsilon_{\\theta}(\\bar{X}_{t},t) \\right) + \\sigma_{t} Z_{t}, $$ where $\\sigma_{t}$ a unlearned constant used in $\\Sigma_{\\theta}(x_{t},t) = \\sigma_{t}^2I$. We set $\\sigma_{t}^2 = \\beta_{t}$. If $\\beta_{t}$ is slowly varying and $\\beta_{t}\\to 0$, we have $$ \\bar{\\alpha}_{t} = \\prod_{s=0}^T (1-\\beta_{s}) \\approx \\prod_{s=0}^T e^{-\\beta_{s}} \\approx \\exp\\left( -\\int_{0}^t \\beta_{s} ds\\right). $$ We the approximate the discrete DDMP sampling process as $$ \\bar{X}_{t-1} \\approx \\left( 1+\\frac{\\beta_{t}}{2} \\right)\\bar{X}_{t} - \\frac{\\beta_{t}}{\\sqrt{ 1- \\exp\\left( -\\int_{0}^t \\beta_{s} ds\\right)}} \\epsilon_{\\theta}(\\bar{X}_{t}, t) + \\sqrt{ \\beta_{t} }Z_{t} $$ This implies the reverse-time SDE $$ d\\bar{X}_{t} = \\left( -\\frac{\\beta_{t}}{2} \\bar{X}_{t} + \\frac{\\beta_{t}}{\\sqrt{ 1- \\exp\\left( -\\int_{0}^t \\beta_{s} ds\\right)}} \\epsilon_{\\theta}(\\bar{X}_{t}, t) \\right)dt + \\sqrt{ \\beta_{t} } d \\bar{W}_{t}. $$By the Anderson’s theorem, we can get the reverse-time is $$ \\begin{aligned} d \\bar{X}_{t} \u0026= \\left( -\\frac{\\beta_{t}}{2}\\bar{X}_{t} - \\beta_{t}\\nabla_{x}\\log p_{t}(\\bar{X}_{t}) \\right)dt + \\sqrt{ \\beta_{t} }d\\bar{W}_{t} \\\\ \u0026\\approx \\left( -\\frac{\\beta_{t}}{2}\\bar{X}_{t} - \\beta_{t}\\nabla_{x}\\log p_{t|0}(\\bar{X}_{t}) \\right)dt + \\sqrt{ \\beta_{t} }d\\bar{W}_{t} \\\\ \u0026\\approx \\left( -\\frac{\\beta_{t}}{2}\\bar{X}_{t} + \\beta_{t} \\frac{\\epsilon_{\\theta}(\\bar{X}_{t},t)}{\\sigma_{t}} \\right)dt + \\sqrt{ \\beta_{t} }d\\bar{W}_{t} ;\\quad \\text{by } \\nabla_{x}\\log p_{t|0}(x) = - \\frac{\\epsilon}{\\sigma_{t}},\\\\ \\end{aligned} $$ where $\\sigma_{t}^2$ is the variance of $X_{t}|X_{0}$. We have stated in Diffusion-models-2#Examples with O-U process#VP SDE that the VP SDE has the property that $X_{t}|X_{0}$ follows a Gaussian distribution with $$ X_{t}|X_{0} \\sim \\mathcal{N}\\left(\\exp\\left( -\\frac{1}{2}\\int_{0}^t\\beta_{s} ds\\right) X_{0}, \\left(1- \\exp\\left( -\\int_{0}^t \\beta_{s} ds\\right)\\right)I\\right). $$ Therefore substituting $\\sigma_{t} = \\sqrt{ 1- \\exp\\left( -\\int_{0}^t \\beta_{s} ds\\right)}$ we get $$ d \\bar{X}_{t} \\approx \\left( -\\frac{\\beta_{t}}{2}\\bar{X}_{t} + \\beta_{t} \\frac{\\epsilon_{\\theta}(\\bar{X}_{t},t)}{\\sqrt{ 1- \\exp\\left( -\\int_{0}^t \\beta_{s} ds\\right)}} \\right)dt + \\sqrt{ \\beta_{t} }d\\bar{W}_{t} $$ Therefore the reverse-time SDE derived from the sampling process of DDMP is consistent to the reverse-time SDE of VP SDE.\nDDIM is VP SDE with ODE sampling The loss function of DDPM depends on $q(x_{t}|x_{0})$, but not directly on the joint $q(x_{1:T}|x_{0})$. Thus DDIM explore alternative forward process that non-Markovian but with the same marginals $q(x_{t}|x_{0})$. Specifically, $$ \\begin{aligned} q_{\\rho}(x_{1},\\dots x_{T}|x_{0}) \u0026= q_{\\rho}(x_{T}|x_{0}) \\prod_{t=1}^{T-1}q_{\\rho}(x_{t}|x_{t+1}, x_{0}) \\\\ q_{\\rho}(x_{T}|x_{0}) \u0026= \\mathcal{N}(\\sqrt{ \\bar{\\alpha}_{t} }x_{0}, (1-\\bar{\\alpha}_{T})I) \\\\ q_{\\rho}(x_{t-1}|x_{t}, x_{0}) \u0026= \\mathcal{N}\\left( \\sqrt{\\bar{\\alpha}_{t-1} }x_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t-1} - \\rho_{t}^2 }\\frac{x_{t}- \\sqrt{ \\bar{\\alpha}_{t}}x_{0}}{\\sqrt{ 1-\\bar{\\alpha}_{t} }} , \\rho_{t}^2 I\\right) \\end{aligned} $$ The transition kernel $X_{0}\\mapsto X_{T}$ and $(X_{0},X_{t+1})\\mapsto X_{t}$ are chosen to make sure that the marginals of DDIM match the marginals of DDMP: $$ q(x_{t}|x_{0}) = \\mathcal{N}(\\sqrt{ \\bar{\\alpha}_{t} }x_{0}, (1-\\bar{\\alpha}_{t})I). $$ We can prove it by induction: $$ \\begin{align} q_{\\rho}(x_{t+1}|x_{0}) \u0026= \\mathcal{N}(\\sqrt{ \\bar{\\alpha}_{t+1} }x_{0}, (1-\\bar{\\alpha}_{t+1})I) \\\\ q_{\\rho}(x_{t}|x_{0}) \u0026= \\int q_{\\rho}(x_{t+1}|x_{0}) q_{\\rho}(x_{t}|x_{t+1}, x_{0}) dx_{t+1}. \\end{align} $$ Since both $q_{\\rho}(x_{t+1}|x_{0})$ and $q_{\\rho}(x_{t}|x_{t+1}, x_{0})$ are Gaussian, we have that $q(x_{t}|x_{0})$ is Gaussian with mean $\\sqrt{ \\bar{\\alpha}_{t} }x_{0}$ and variance $(1-\\bar{\\alpha}_{t})I$.\nSince DDIM and DDPM have the same conditional marginals, their conditional and unconditional score functions are the same. Given a DDMP, it is unnecessary to retrain it. The contribution of DDIM is to find a faster sampling method.\nThe DDIM sampling is done with $$ p_{\\theta}(x_{t-1}|x_{t}) = q_{\\rho}(x_{t-1}|x_{t}, x_{0}). $$ However, the $x_{0}$ is unknown. But it has an unbiased estimator through $$ \\begin{aligned} X_{t} \u0026= \\sqrt{ \\bar{\\alpha}_{t} }X_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t} }\\epsilon \\\\ \\hat{X}_{0} \u0026= \\mathbb{E}[X_{0}|X_{t}] \\approx \\frac{X_{t} - \\sqrt{ 1-\\bar{\\alpha}_{t}}\\epsilon_{\\theta}(X_{t}, t)}{\\sqrt{ \\bar{\\alpha}_{t} }} \\end{aligned}, $$ where $\\epsilon_{\\theta}$ is our trained model that predicts the added noise from $X_{t}$. Then the sampling process is given via: $$ \\begin{align} \\bar{X}_{t-1} \u0026= \\sqrt{ \\bar{\\alpha}_{t-1} }\\hat{X}_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t-1} - \\rho_{t}^2 } \\frac{\\bar{X}_{t} - \\sqrt{ \\bar{\\alpha}_{t} }X_{0}}{\\sqrt{ 1-\\bar{\\alpha}_{t} }} + \\rho_{t} Z_{t} \\\\ \u0026= \\sqrt{ \\bar{\\alpha}_{t-1} }\\hat{X}_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t-1} - \\rho_{t}^2 } \\frac{\\bar{X}_{t} - \\sqrt{ \\bar{\\alpha}_{t} }\\hat{X}_{0}}{\\sqrt{ 1-\\bar{\\alpha}_{t} }} + \\rho_{t} Z_{t} \\\\ \u0026= \\sqrt{ \\bar{\\alpha}_{t-1} }\\hat{X}_{0} + \\sqrt{ 1-\\bar{\\alpha}_{t-1} - \\rho_{t}^2 } \\epsilon_{\\theta}(\\bar{X}_{t}, t)+ \\rho_{t} Z_{t} \\\\ \\end{align} $$Recall that in DDMP, $q(x_{t-1}|x_{t}, x_{0}) = \\mathcal{N}(\\tilde{\\mu}_{t}(x_{t},x_{0}), \\tilde{\\beta}_{t}I)$, therefore we have $$ \\tilde{\\beta}_{t} = \\rho_{t}^2 = \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_{t}}\\beta_{t}. $$ It other words, when $\\rho_{t}^2 = \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_{t}}\\beta_{t}$ the sampling process becomes DDMP. The special case of $\\rho_{t} = 0$ makes the sampling process deterministic, which is named the DDIM. If we rewrite the above sampling process in one-step form, we have $$ \\begin{align} \\bar{X}_{t-1} \u0026= \\frac{1}{\\sqrt{ 1-\\beta_{t} }} \\bar{X}_{t} - \\left( \\frac{\\sqrt{ 1-\\bar{\\alpha}_{t} }}{\\sqrt{1-\\beta_{t}}} - \\sqrt{ 1-\\bar{\\alpha}_{t-1} } \\right)\\epsilon_{\\theta}(\\bar{X}_{t},t) \\\\ \u0026= \\frac{1}{\\sqrt{ 1-\\beta_{t} }} \\bar{X}_{t} - \\left( \\frac{\\sqrt{ 1-\\bar{\\alpha}_{t} }}{\\sqrt{1-\\beta_{t}}} - \\sqrt{ 1- \\frac{\\bar{\\alpha}_{t}}{1-\\beta_{t}} } \\right)\\epsilon_{\\theta}(\\bar{X}_{t},t) \\end{align} $$If $\\beta_{t}$ is slowly varying and $\\beta_{t}\\to 0$, we have $$ \\begin{align} \\bar{X}_{t-1} \u0026\\approx \\left( 1 + \\frac{\\beta_{t}}{2} \\right)\\bar{X}_{t} - \\frac{\\beta_{t}}{2\\sqrt{ 1-\\bar{\\alpha}_{t} }}\\epsilon_{\\theta}(\\bar{X}_{t},t) \\\\ \u0026\\approx \\left( 1+\\frac{\\beta_{t}}{2} \\right)\\bar{X}_{t} - \\frac{\\beta_{t}}{2\\sqrt{ 1- \\exp\\left( -\\int_{0}^t \\beta_{s} ds\\right)}} \\epsilon_{\\theta}(\\bar{X}_{t}, t) \\\\ \u0026\\approx \\left( 1+\\frac{\\beta_{t}}{2} \\right)\\bar{X}_{t} + \\frac{\\beta_{t}}{2}\\nabla_{x}\\log p_{t|0}(\\bar{X}_{t}) \\end{align} $$ It agrees with the reverse-time ODE of VP SDE: $$ dX_{t} = - \\frac{\\beta_{t}}{2} X_{t} dt + \\sqrt{ \\beta_{t} }dW_{t}. $$Noise conditional score network (NCSN) is VE SDE with SDE sampling The forward-time process of NCSN is $$ X_{t} = X_{t-1} + \\sqrt{ \\sigma_{t}^2 - \\sigma_{t-1}^2} Z_{t},\\quad X_{0}\\sim p_{data}. $$ Since $\\sqrt{ \\sigma_{t}^2 - \\sigma_{t-1}^2} = \\sqrt{\\frac{ \\sigma_{t}^2 - \\sigma_{t-1}^2}{\\Delta_{t}}} \\sqrt{ \\Delta_{t} }$, it converges to $$ dX_{t} = \\sqrt{ \\frac{d(\\sigma_{t}^2)}{dt} }dW_{t}. $$ The sampling process of NCSN is defined as $$ \\begin{aligned} \\bar{X}_{t-1} \u0026= \\bar{X}_{t} +(\\sigma_{t}^2 - \\sigma_{t-1}^2)s_{\\theta} + \\sqrt{ \\sigma_{t}^2 - \\sigma_{t-1}^2} Z_{t} \\\\ \u0026\\approx \\bar{X}_{t} +(\\sigma_{t}^2 - \\sigma_{t-1}^2)\\nabla_{x}\\log p_{t}(\\bar{X}_{t}) + \\sqrt{ \\sigma_{t}^2 - \\sigma_{t-1}^2} Z_{t} \\\\ \\end{aligned} $$ It can be written as $$ d\\bar{X}_{t} = - \\frac{d(\\sigma_{t}^2)}{dt} \\nabla_{x}\\log p_{t}(\\bar{X}_{t}) dt + \\sqrt{\\frac{d(\\sigma_{t}^2)}{dt} }d\\bar{W}_{t}, $$ which is the reverse-time SDE of VE SDE.\nSupplementary Tweedie’s formula Consider the random variable $$ Y = X + \\sigma Z, \\quad X\\sim p_{X}, \\quad Z\\sim\\mathcal{N}(0,I), $$ where $p_{X}$ is not necessarily Gaussian, then $$ \\begin{align} \\mathbb{E}[X|Y] \u0026= Y + \\sigma^2 \\nabla_{y}\\log p_{y}(Y) \\\\ \\mathrm{Var}(X|Y) \u0026= \\sigma^2I + \\sigma^4 \\nabla_{y}^2\\log p_{y}(Y). \\end{align} $$ Easily, we can see that if $Y = \\gamma X+\\sigma Z$, $\\gamma\\neq 0$, then we have $$ \\begin{align} \\mathbb{E}[X|Y] \u0026= \\frac{1}{\\gamma}(Y + \\sigma^2 \\nabla_{y}\\log p_{y}(Y)) \\\\ \\\\ \\mathrm{Var}(X|Y) \u0026= \\frac{\\sigma^2}{\\gamma^2}(I + \\sigma^2\\nabla_{y}^2\\log p_{y}(Y)). \\end{align} $$ See proof here\nApproximation Gaussian ^08dcf7 $$ \\begin{aligned} p_{X|Y}(x) \u0026= \\frac{p_{Y|X}(y) p_{X}(x)}{p_{Y}(y)} \\\\ \u0026= \\frac{p_{Y|X}(y) p_{X}(x)}{\\int p_{Y|X}(y)p_{X}(x)dx} \\\\ \\end{aligned} $$ Note that $p_{Y|X}$ is Gaussian and $$ \\begin{aligned} p_{Y|X} p_{X}(x) \u0026= \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|y-x\\|^2 \\right) p_{X}(x) \\\\ \u0026= \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|y-x\\|^2 \\right) (p_{X}(y) + \\langle \\nabla p_{X}(y), x-y \\rangle + O(\\|x-y\\|^2) ) \\end{aligned} $$ Then the denominator can be calculated as $\\text{denom} = p_{X}(y) + O(\\sigma^2)$ since $$ \\begin{aligned} \\int \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|y-x\\|^2 \\right) p_{X}(y) dx \u0026= p_{X}(y) \\\\ \\int \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|y-x\\|^2 \\right) (x-y) dx \u0026= 0 \\\\ \\int \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|y-x\\|^2 \\right) O(\\|x-y\\|^2) \u0026= O(\\sigma^2). \\end{aligned} $$It follows that $$ \\begin{aligned} p_{X|Y}(x) \u0026= \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|y-x\\|^2 \\right) \\frac{p_{X}(y) + \\langle \\nabla p_{X}(y), x-y \\rangle + O(\\|x-y\\|^2)}{p_{X}(y)+ O(\\sigma^2)} \\\\ \u0026= \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|y-x\\|^2 \\right) (1 + \\langle \\nabla\\log p_{X}(y), x-y \\rangle + \\text{h.o.t.} ) \\\\ \u0026= \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|x-y\\|^2 \\right) \\exp(\\langle \\nabla \\log p_{X}(y), x-y \\rangle ) + \\text{h.o.t.}\\\\ \u0026= \\frac{1}{(2\\pi \\sigma)^{d/2}}\\exp\\left( -\\frac{1}{2\\sigma^2} \\|x-y - \\sigma^2\\nabla\\log p_{Y}(y)\\|^2 + \\text{h.o.t.} \\right) + \\text{h.o.t.} \\\\ \u0026\\approx \\mathcal{N}(y+\\sigma^2\\nabla\\log p_{Y}(y), \\sigma^2 I). \\end{aligned} $$",
  "wordCount" : "1876",
  "inLanguage": "en",
  "datePublished": "2024-10-23T10:52:59+02:00",
  "dateModified": "2024-10-23T10:52:59+02:00",
  "author":{
    "@type": "Person",
    "name": "Chaohua Wang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://wangchxx.github.io/posts/diffusion_3_from_a_unified_prospective/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My Notes",
    "logo": {
      "@type": "ImageObject",
      "url": "https://wangchxx.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://wangchxx.github.io/" accesskey="h" title="My Notes (Alt + H)">My Notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://wangchxx.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/series/" title="Series">
                    <span>Series</span>
                </a>
            </li>
            <li>
                <a href="https://wangchxx.github.io/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      DL | Diffusion Models 3 - Understanding DDMP from a Unified Prospective
    </h1>
    <div class="post-meta"><span title='2024-10-23 10:52:59 +0200 +0200'>October 23, 2024</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Chaohua Wang

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#understand-ddmp-from-vlb" aria-label="Understand DDMP from VLB">Understand DDMP from VLB</a></li>
                <li>
                    <a href="#understand-ddmp-from-score-matching" aria-label="Understand DDMP from score matching">Understand DDMP from score matching</a></li>
                <li>
                    <a href="#ddmp-is-vp-sde-with-sde-sampling" aria-label="DDMP is VP SDE with SDE sampling">DDMP is VP SDE with SDE sampling</a></li>
                <li>
                    <a href="#ddim-is-vp-sde-with-ode-sampling" aria-label="DDIM is VP SDE with ODE sampling">DDIM is VP SDE with ODE sampling</a></li>
                <li>
                    <a href="#noise-conditional-score-network-ncsn-is-ve-sde-with-sde-sampling" aria-label="Noise conditional score network (NCSN) is VE SDE with SDE sampling">Noise conditional score network (NCSN) is VE SDE with SDE sampling</a></li>
                <li>
                    <a href="#supplementary" aria-label="Supplementary">Supplementary</a><ul>
                        
                <li>
                    <a href="#tweedies-formula" aria-label="Tweedie&rsquo;s formula">Tweedie&rsquo;s formula</a></li>
                <li>
                    <a href="#approximation-gaussian" aria-label="Approximation Gaussian">Approximation Gaussian</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="understand-ddmp-from-vlb">Understand DDMP from VLB<a hidden class="anchor" aria-hidden="true" href="#understand-ddmp-from-vlb">#</a></h2>
<p>In the <a href="/posts/diffusion_2_preliminary_sde/">Diffusion-models-1</a> we have introduced the DDMP model
</p>
$$
X_t|X_{t-1} = \mathcal{N}(\sqrt{1-\beta_t}X_{t-1}, \beta_t I), \quad X_t|X_0 = \mathcal{N}(\sqrt{\bar{\alpha}_t} X_0, (1-\bar{\alpha}_{t})I),
$$<p>
with $X_{0}\sim q_{0} = p_{data}$, $\alpha_{t} = 1-\beta_{t}$ and $\bar{\alpha}_{t} = \prod_{s=1}^t (1-\beta_{s})$.</p>
<p>The loss function is derived by minimizing the negative log-likelihood $-\log p_{\theta}(X_{0})$ with a variational lower bound $L_{VLB}$
</p>
$$
L_{VLB} = \mathbb{E}_{q}\left[ \log  \frac{q(x_{1:T}|x_{0})}{p_{\theta}(x_{0:T})}  \right]  = \sum_{t=0}^T L_{t}
$$<p>
with
</p>
$$
L_{t} = D_{KL}(q(x_{t}|x_{t+1},x_{0})\| p_{\theta}(x_{t}|x_{t+1})),\; 1\leq t\leq T-1. 
$$<p>
Since the conditional distribution $X_{t}|(X_{t-1}, X_{0})$ is Gaussian
</p>
$$
q(x_{t-1}|x_t, x_0) = \mathcal{N}(\tilde{\mu}_t(x_t, x_0), \tilde{\beta}_t I),
$$<p>
where
</p>
$$
\begin{equation}
\tilde{\mu}_t(x_t, x_0) := \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t +\frac{\bar{\alpha}_{t-1}\beta_t}{1-\bar{\alpha}_t}x_0;\quad \tilde{\beta}_t:= \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t.
\end{equation}
$$<p>
If we model the conditional distribution $X_{t-1}|X_{t}$ as a Gaussian $X_{t-1}|X_{t} \sim \mathcal{N}(\mu_\theta(X_t, t), \Sigma_\theta(X_t, t))$, the loss function is basically the KL-divergence between two Gaussian distributions, which has the closed form
</p>
$$
\begin{equation}
L_{t} = \frac{1}{2\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \|\tilde{\mu}(x_{t},x_{0}) - \mu_{\theta}(x_{t},t)\|^2 + C.
\end{equation}
$$<p>
Because $q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha})I)$ we can express $x_{t}|x_{0}$ as $x_{t}(x_{0}, \epsilon) = \sqrt{ \bar{\alpha}_{t} }x_{0} + \sqrt{ 1-\bar{\alpha}_{t} }\epsilon$ for $\epsilon\sim\mathcal{N}(0, I)$, therefore we can rewrite $\tilde{\mu}_{t}(x_{t}, x_{0})$ as
</p>
$$
\tilde{\mu}_{t} = \tilde{\mu}(x_{0}, \epsilon) = \frac{1}{\sqrt{ \alpha_{t} }}\left( x_{t} - \frac{1-\alpha_{t}}{\sqrt{ 1-\bar{\alpha}_{t} }} \epsilon \right).
$$<p>
Similarly, we can also model $\mu_{\theta}(x_{t}, t)$ as
</p>
$$
\mu_{\theta}(x_{t}, t) = \frac{1}{\sqrt{ \alpha_{t} }}\left( x_{t} - \frac{1-\alpha_{t}}{\sqrt{ 1-\bar{\alpha}_{t} }} \epsilon_{\theta}(x_{t,t}) \right),
$$<p>
where $\epsilon_{\theta}$ is an approximation of the noise $\epsilon$. Overall, the loss function $L_{t}$ becomes
</p>
$$
L_{t} = \mathbb{E}_{x_{0}, \epsilon} \left[\frac{(1-\alpha_{t})^2}{2 \alpha_{t}(1-\bar{\alpha}_{t})\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \| \epsilon - \epsilon_{\theta}(\sqrt{ \bar{\alpha}_{t} }x_{0} + \sqrt{ 1-\bar{\alpha}_{t} }\epsilon,t)\|^2 \right] + C.
$$<h2 id="understand-ddmp-from-score-matching">Understand DDMP from score matching<a hidden class="anchor" aria-hidden="true" href="#understand-ddmp-from-score-matching">#</a></h2>
<p>Consider the random variable
</p>
$$
Y = X + \sigma Z, \quad X\sim P_{X}, \quad Z\sim \mathcal{N}(0,I).
$$<p>
By definition, $p_{Y|X}$ is Gaussian, but in general $p_{X|Y}$ is not Gaussian since we did not assume $p_{X}$ is Gaussian. But $p_{X|Y}$ is approximately Gaussian in the limit of $\sigma\to 0$.
</p>
$$
p_{X|Y}(x|y) \approx \mathcal{N}(y + \sigma^2\nabla \log p_{Y}(y), \sigma^2 I).
$$<p>
See <a href="/posts/diffusion_3_from_a_unified_prospective/#approximation-gaussian">Proof of Gaussian</a>. And for $Y = \gamma X + \sigma Z$ with $\gamma \neq 0$, then in the limit of $\sigma\to 0$,<br>
</p>
$$
p_{X|Y}(x|y) \approx \mathcal{N}\left( \frac{1}{\gamma}(y + \sigma^2\nabla \log p_{Y}(y)), \frac{\sigma^2}{\gamma^2} I \right).
$$<p>
In the setting of DDMP, we know the forward process is
</p>
$$
X_t|X_{t-1} = \mathcal{N}(\sqrt{1-\beta_t}X_{t-1}, \beta_t I), \quad X_t|X_0 = \mathcal{N}(\sqrt{\bar{\alpha}_t} X_0, (1-\bar{\alpha})I), \quad \bar{\alpha}_{t} = \prod_{s=1}^t(1-\beta_{s}).
$$<p>
If $\beta_{t}$ is small enough, approximately the backward process is
</p>
$$
p(x_{t-1}|x_{t}) \approx \mathcal{N}(\mu(x_{t},t), \beta_{t}I),
$$<p>
with $\mu(x_{t},t) = \frac{1}{\sqrt{ 1-\beta_{t} }}(x_{t} - \beta_{t}\nabla \log p_{t}(x_{t}))$. Similarly, we can a model $p_{\theta}(x_{t-1}|x_{t})$ to learn it
</p>
$$
p_{\theta}(x_{t-1}|x_{t}) = \mathcal{N}(\mu_{\theta}(x_{t}, t), \Sigma_{\theta}(x_{t},t)), \quad \mu_{\theta}(x_{t},t) = \frac{1}{\sqrt{ 1-\beta_{t} }}(x_{t} - \beta_{t}s_{\theta}(x_{t},t))
$$<p>
Essentially, we are using $s_{\theta}$ to learn the score function $\nabla_{x}\log p$. Naturally, we can think about using the loss function MSE of means or KL-divergence. There is not a big difference between them, so we take KL-divergence as an example
</p>
$$
\begin{aligned}
\mathcal{L}(\theta) &= \sum_{t=1}^T \lambda_{t} KL(p(x_{t-1}|x_{t}\|p_{\theta}(x_{t-1}|x_{t}))) \\
    &= \sum_{t=1}^T\lambda_{t} \frac{1}{2\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \|\mu(x_{t},x_{0}) - \mu_{\theta}(x_{t},t)\|^2 + C\\
    &= \sum_{t=1}^T\lambda_{t} \frac{(1-\alpha_{t})^2}{2 \alpha_{t}\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \| \nabla_{x}\log p_{t}(x_{t}) - s_{\theta}(x_{t},t)\|^2 + C
\end{aligned}
$$<p>
Here we see the form of score matching. However, the score function $\nabla_{x}\log p_{t}$ is inaccessible, so we cannot compute the above loss function. Recall that $X_{t}|X_{0}$ is a known Gaussian,  thus we can try to use $\nabla_{x}\log p_{t|0}(x_{t}|x_{0})$ in the loss function. Moreover, for Gaussian distribution $Y\sim \mathcal{N}(\mu, \sigma^2 I)$, the score function can be easily computed as
</p>
$$
\nabla_{y}\log p_{Y}(Y) \overset{\mathcal{D}}{=} - \frac{\epsilon}{\sigma}, \quad \epsilon \sim \mathcal{N}(0,I).
$$<p>
Like what we have done for DDPM VLB loss function, we use $\epsilon_{\theta} = -\sqrt{ 1-\bar{\alpha}_{t} }  s_{\theta}$. Now we can rewrite the loss function as
</p>
$$
\begin{aligned}
\mathcal{L}(\theta) &= \sum_{t=1}^T\lambda_{t} \frac{(1-\alpha_{t})^2}{2 \alpha_{t}\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \| \nabla_{x}\log p_{t}(x_{t}) - s_{\theta}(x_{t},t)\|^2 + C \\
    &= \sum_{t=1}^T\lambda_{t} \frac{(1-\alpha_{t})^2}{2 \alpha_{t}\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \mathbb{E}_{x_{0}\sim p_{0}} \left[\| \nabla_{x}\log p_{t|0}(x_{t}|x_{0}) - s_{\theta}(x_{t},t)\|^2 \right] + C \\
    &= \sum_{t=1}^T\lambda_{t} \frac{(1-\alpha_{t})^2}{2 \alpha_{t}\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \mathbb{E}_{x_{0}\sim p_{0}, \epsilon\sim\mathcal{N}(0,I)} \left[\| - \frac{\epsilon}{\sqrt{ 1-\bar{\alpha}_{t} }} - s_{\theta}(x_{t},t)\|^2 \right] + C \\
    &= \sum_{t=1}^T\lambda_{t} \frac{(1-\alpha_{t})^2}{2 \alpha_{t} (1-\bar{\alpha}_{t})\|\Sigma_{\theta}(x_{t},t)\|_{2}^2} \mathbb{E}_{x_{0}\sim p_{0}, \epsilon\sim\mathcal{N}(0,I)} \left[\| \epsilon- \epsilon_{\theta}(\sqrt{ \bar{\alpha}_{t} } x_{0} + \sqrt{ 1-\bar{\alpha}_{t} }\epsilon,t)\|^2 \right] + C, \\
\end{aligned}
$$<p>
which is exactly the VLB loss function of DDMP.</p>
<h2 id="ddmp-is-vp-sde-with-sde-sampling">DDMP is VP SDE with SDE sampling<a hidden class="anchor" aria-hidden="true" href="#ddmp-is-vp-sde-with-sde-sampling">#</a></h2>
<p>DDMP forward process in the limit $\beta_{t}\to 0$
</p>
$$
X_{t+1} = \sqrt{ 1-\beta_{t} } X_{t} + \sqrt{ \beta_{t} } Z_{t} \approx \left( 1-\frac{\beta_{t}}{2} \right)X_{t} + \sqrt{ \beta_{t} }Z_{t}.
$$<p>
Therefore when the timestep goes to zero, the DDMP forward process converges to the following VP SDE:
</p>
$$
dX_{t} = -\frac{1}{2}\beta_{t} X_{t} dt + \sqrt{ \beta_{t} }dW_{t}.
$$<p>DDMP sampling process is
</p>
$$
\bar{X}_{t-1} = \frac{1}{\sqrt{ 1-\beta_{t} }}\left( \bar{X}_{t} - \frac{\beta_{t}}{\sqrt{ 1-\bar{\alpha}_{t} }} \epsilon_{\theta}(\bar{X}_{t},t) \right) + \sigma_{t} Z_{t},
$$<p>
where $\sigma_{t}$ a unlearned constant used in $\Sigma_{\theta}(x_{t},t) = \sigma_{t}^2I$. We set $\sigma_{t}^2 = \beta_{t}$. If $\beta_{t}$ is slowly varying and $\beta_{t}\to 0$, we have
</p>
$$
\bar{\alpha}_{t} = \prod_{s=0}^T (1-\beta_{s}) \approx \prod_{s=0}^T e^{-\beta_{s}} \approx \exp\left( -\int_{0}^t \beta_{s} ds\right).
$$<p>
We the approximate the discrete DDMP sampling process as
</p>
$$
\bar{X}_{t-1} \approx \left( 1+\frac{\beta_{t}}{2} \right)\bar{X}_{t} - \frac{\beta_{t}}{\sqrt{ 1- \exp\left( -\int_{0}^t \beta_{s} ds\right)}} \epsilon_{\theta}(\bar{X}_{t}, t) + \sqrt{ \beta_{t} }Z_{t}
$$<p>
This implies the reverse-time SDE
</p>
$$
d\bar{X}_{t} = \left( -\frac{\beta_{t}}{2} \bar{X}_{t}  + \frac{\beta_{t}}{\sqrt{ 1- \exp\left( -\int_{0}^t \beta_{s} ds\right)}} \epsilon_{\theta}(\bar{X}_{t}, t)  \right)dt + \sqrt{ \beta_{t} } d \bar{W}_{t}.
$$<p>By the Anderson&rsquo;s theorem, we can get the reverse-time is
</p>
$$
\begin{aligned}
d \bar{X}_{t} &= \left( -\frac{\beta_{t}}{2}\bar{X}_{t} - \beta_{t}\nabla_{x}\log p_{t}(\bar{X}_{t}) \right)dt + \sqrt{ \beta_{t} }d\bar{W}_{t} \\
    &\approx \left( -\frac{\beta_{t}}{2}\bar{X}_{t} - \beta_{t}\nabla_{x}\log p_{t|0}(\bar{X}_{t}) \right)dt + \sqrt{ \beta_{t} }d\bar{W}_{t} \\
    &\approx \left( -\frac{\beta_{t}}{2}\bar{X}_{t} + \beta_{t} \frac{\epsilon_{\theta}(\bar{X}_{t},t)}{\sigma_{t}} \right)dt + \sqrt{ \beta_{t} }d\bar{W}_{t} ;\quad \text{by } \nabla_{x}\log p_{t|0}(x) = - \frac{\epsilon}{\sigma_{t}},\\
\end{aligned}
$$<p>
where $\sigma_{t}^2$ is the variance of $X_{t}|X_{0}$. We have stated in <a href="/posts/diffusion_2_preliminary_sde/#dm2_o-u_examples">Diffusion-models-2#Examples with O-U process#VP SDE</a> that the VP SDE has the property that $X_{t}|X_{0}$ follows a Gaussian distribution with
</p>
$$
X_{t}|X_{0} \sim \mathcal{N}\left(\exp\left( -\frac{1}{2}\int_{0}^t\beta_{s} ds\right) X_{0}, \left(1- \exp\left( -\int_{0}^t \beta_{s} ds\right)\right)I\right).
$$<p>
Therefore substituting $\sigma_{t} = \sqrt{ 1- \exp\left( -\int_{0}^t \beta_{s} ds\right)}$ we get
</p>
$$
d \bar{X}_{t} \approx \left( -\frac{\beta_{t}}{2}\bar{X}_{t} + \beta_{t} \frac{\epsilon_{\theta}(\bar{X}_{t},t)}{\sqrt{ 1- \exp\left( -\int_{0}^t \beta_{s} ds\right)}} \right)dt + \sqrt{ \beta_{t} }d\bar{W}_{t}
$$<p>
Therefore the reverse-time SDE derived from the sampling process of DDMP is consistent to the reverse-time SDE of VP SDE.</p>
<h2 id="ddim-is-vp-sde-with-ode-sampling">DDIM is VP SDE with ODE sampling<a hidden class="anchor" aria-hidden="true" href="#ddim-is-vp-sde-with-ode-sampling">#</a></h2>
<p><img loading="lazy" src="/posts/diffusion_3_from_a_unified_prospective/ddim_graph.png"></p>
<p>The loss function of DDPM depends on $q(x_{t}|x_{0})$, but not directly on the joint $q(x_{1:T}|x_{0})$. Thus DDIM explore alternative forward process that non-Markovian but with the same marginals $q(x_{t}|x_{0})$. Specifically,
</p>
$$
\begin{aligned}
q_{\rho}(x_{1},\dots x_{T}|x_{0}) &= q_{\rho}(x_{T}|x_{0}) \prod_{t=1}^{T-1}q_{\rho}(x_{t}|x_{t+1}, x_{0}) \\
q_{\rho}(x_{T}|x_{0}) &= \mathcal{N}(\sqrt{ \bar{\alpha}_{t} }x_{0}, (1-\bar{\alpha}_{T})I) \\
q_{\rho}(x_{t-1}|x_{t}, x_{0}) &= \mathcal{N}\left( \sqrt{\bar{\alpha}_{t-1} }x_{0} +  \sqrt{ 1-\bar{\alpha}_{t-1} - \rho_{t}^2 }\frac{x_{t}- \sqrt{ \bar{\alpha}_{t}}x_{0}}{\sqrt{ 1-\bar{\alpha}_{t} }} , \rho_{t}^2 I\right)
\end{aligned}
$$<p>
The transition kernel $X_{0}\mapsto X_{T}$ and $(X_{0},X_{t+1})\mapsto X_{t}$ are chosen to make sure that the marginals of DDIM match the marginals of DDMP:
</p>
$$
q(x_{t}|x_{0}) = \mathcal{N}(\sqrt{ \bar{\alpha}_{t} }x_{0}, (1-\bar{\alpha}_{t})I).
$$<p>
We can prove it by induction:
</p>
$$
\begin{align}
q_{\rho}(x_{t+1}|x_{0}) &= \mathcal{N}(\sqrt{ \bar{\alpha}_{t+1} }x_{0}, (1-\bar{\alpha}_{t+1})I)  \\
q_{\rho}(x_{t}|x_{0}) &= \int q_{\rho}(x_{t+1}|x_{0}) q_{\rho}(x_{t}|x_{t+1}, x_{0}) dx_{t+1}.
\end{align}
$$<p>
Since both $q_{\rho}(x_{t+1}|x_{0})$ and $q_{\rho}(x_{t}|x_{t+1}, x_{0})$ are Gaussian, we have that $q(x_{t}|x_{0})$ is Gaussian with mean $\sqrt{ \bar{\alpha}_{t} }x_{0}$ and variance $(1-\bar{\alpha}_{t})I$.</p>
<p>Since DDIM and DDPM have the same conditional marginals, their conditional and unconditional score functions are the same. Given a DDMP, it is unnecessary to retrain it. The contribution of DDIM is to find a faster sampling method.</p>
<p>The DDIM sampling is done with
</p>
$$
p_{\theta}(x_{t-1}|x_{t}) = q_{\rho}(x_{t-1}|x_{t}, x_{0}).
$$<p>
However, the $x_{0}$ is unknown. But it has an unbiased estimator through
</p>
$$
\begin{aligned}
X_{t} &= \sqrt{ \bar{\alpha}_{t} }X_{0} + \sqrt{ 1-\bar{\alpha}_{t} }\epsilon \\
\hat{X}_{0} &= \mathbb{E}[X_{0}|X_{t}] \approx  \frac{X_{t} - \sqrt{ 1-\bar{\alpha}_{t}}\epsilon_{\theta}(X_{t}, t)}{\sqrt{ \bar{\alpha}_{t} }}
\end{aligned},
$$<p>
where $\epsilon_{\theta}$ is our trained model that predicts the added noise from $X_{t}$. Then the sampling process is given via:
</p>
$$
\begin{align}
\bar{X}_{t-1} &= \sqrt{ \bar{\alpha}_{t-1} }\hat{X}_{0} + \sqrt{ 1-\bar{\alpha}_{t-1} - \rho_{t}^2 } \frac{\bar{X}_{t} - \sqrt{ \bar{\alpha}_{t} }X_{0}}{\sqrt{ 1-\bar{\alpha}_{t} }} + \rho_{t} Z_{t} \\ 
    &= \sqrt{ \bar{\alpha}_{t-1} }\hat{X}_{0} + \sqrt{ 1-\bar{\alpha}_{t-1} - \rho_{t}^2 } \frac{\bar{X}_{t} - \sqrt{ \bar{\alpha}_{t} }\hat{X}_{0}}{\sqrt{ 1-\bar{\alpha}_{t} }} + \rho_{t} Z_{t} \\ 
    &= \sqrt{ \bar{\alpha}_{t-1} }\hat{X}_{0} + \sqrt{ 1-\bar{\alpha}_{t-1} - \rho_{t}^2 } \epsilon_{\theta}(\bar{X}_{t}, t)+ \rho_{t} Z_{t} \\ 
\end{align}
$$<p>Recall that in DDMP, $q(x_{t-1}|x_{t}, x_{0}) = \mathcal{N}(\tilde{\mu}_{t}(x_{t},x_{0}), \tilde{\beta}_{t}I)$, therefore we have
</p>
$$
\tilde{\beta}_{t} = \rho_{t}^2 = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\beta_{t}.
$$<p>
It other words, when $\rho_{t}^2 =  \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\beta_{t}$ the sampling process becomes DDMP. The special case of $\rho_{t} = 0$ makes the sampling process deterministic, which is named the DDIM. If we rewrite the above sampling process in one-step form, we have
</p>
$$
\begin{align}
\bar{X}_{t-1} &= \frac{1}{\sqrt{ 1-\beta_{t} }} \bar{X}_{t} - \left( \frac{\sqrt{ 1-\bar{\alpha}_{t} }}{\sqrt{1-\beta_{t}}} - \sqrt{ 1-\bar{\alpha}_{t-1} } \right)\epsilon_{\theta}(\bar{X}_{t},t) \\
    &= \frac{1}{\sqrt{ 1-\beta_{t} }} \bar{X}_{t} - \left( \frac{\sqrt{ 1-\bar{\alpha}_{t} }}{\sqrt{1-\beta_{t}}} - \sqrt{ 1- \frac{\bar{\alpha}_{t}}{1-\beta_{t}} } \right)\epsilon_{\theta}(\bar{X}_{t},t)
\end{align}
$$<p>If $\beta_{t}$ is slowly varying and $\beta_{t}\to 0$, we have
</p>
$$
\begin{align}
\bar{X}_{t-1} &\approx \left( 1 + \frac{\beta_{t}}{2} \right)\bar{X}_{t} - \frac{\beta_{t}}{2\sqrt{ 1-\bar{\alpha}_{t} }}\epsilon_{\theta}(\bar{X}_{t},t)  \\
    &\approx \left( 1+\frac{\beta_{t}}{2} \right)\bar{X}_{t} - \frac{\beta_{t}}{2\sqrt{ 1- \exp\left( -\int_{0}^t \beta_{s} ds\right)}} \epsilon_{\theta}(\bar{X}_{t}, t)  \\
    &\approx  \left( 1+\frac{\beta_{t}}{2} \right)\bar{X}_{t} + \frac{\beta_{t}}{2}\nabla_{x}\log p_{t|0}(\bar{X}_{t})
\end{align}
$$<p>
It agrees with the reverse-time ODE of VP SDE:
</p>
$$
dX_{t} = - \frac{\beta_{t}}{2} X_{t} dt + \sqrt{ \beta_{t} }dW_{t}.
$$<h2 id="noise-conditional-score-network-ncsn-is-ve-sde-with-sde-sampling">Noise conditional score network (NCSN) is VE SDE with SDE sampling<a hidden class="anchor" aria-hidden="true" href="#noise-conditional-score-network-ncsn-is-ve-sde-with-sde-sampling">#</a></h2>
<p>The forward-time process of NCSN is
</p>
$$
X_{t} = X_{t-1} + \sqrt{ \sigma_{t}^2 - \sigma_{t-1}^2} Z_{t},\quad X_{0}\sim p_{data}.
$$<p>
Since $\sqrt{ \sigma_{t}^2 - \sigma_{t-1}^2} = \sqrt{\frac{ \sigma_{t}^2 - \sigma_{t-1}^2}{\Delta_{t}}} \sqrt{ \Delta_{t} }$, it converges to
</p>
$$
dX_{t} = \sqrt{ \frac{d(\sigma_{t}^2)}{dt} }dW_{t}.
$$<p>
The sampling process of NCSN is defined as
</p>
$$
\begin{aligned}
\bar{X}_{t-1} &= \bar{X}_{t} +(\sigma_{t}^2 - \sigma_{t-1}^2)s_{\theta} + \sqrt{ \sigma_{t}^2 - \sigma_{t-1}^2}  Z_{t} \\
&\approx \bar{X}_{t} +(\sigma_{t}^2 - \sigma_{t-1}^2)\nabla_{x}\log p_{t}(\bar{X}_{t}) + \sqrt{ \sigma_{t}^2 - \sigma_{t-1}^2}  Z_{t} \\
\end{aligned}
$$<p>
It can be written as
</p>
$$
d\bar{X}_{t} = - \frac{d(\sigma_{t}^2)}{dt} \nabla_{x}\log p_{t}(\bar{X}_{t}) dt + \sqrt{\frac{d(\sigma_{t}^2)}{dt} }d\bar{W}_{t},
$$<p>
which is the reverse-time SDE of VE SDE.</p>
<h2 id="supplementary">Supplementary<a hidden class="anchor" aria-hidden="true" href="#supplementary">#</a></h2>
<h3 id="tweedies-formula">Tweedie&rsquo;s formula<a hidden class="anchor" aria-hidden="true" href="#tweedies-formula">#</a></h3>
<p>Consider the random variable
</p>
$$
Y = X + \sigma Z, \quad X\sim p_{X}, \quad Z\sim\mathcal{N}(0,I),
$$<p>
where $p_{X}$ is not necessarily Gaussian, then
</p>
$$
\begin{align}
\mathbb{E}[X|Y] &= Y + \sigma^2 \nabla_{y}\log p_{y}(Y) \\
\mathrm{Var}(X|Y) &= \sigma^2I + \sigma^4 \nabla_{y}^2\log p_{y}(Y).
\end{align}
$$<p>
Easily, we can see that if $Y = \gamma X+\sigma Z$, $\gamma\neq 0$, then we have
</p>
$$
\begin{align}
\mathbb{E}[X|Y] &= \frac{1}{\gamma}(Y + \sigma^2 \nabla_{y}\log p_{y}(Y)) \\ \\
\mathrm{Var}(X|Y) &= \frac{\sigma^2}{\gamma^2}(I + \sigma^2\nabla_{y}^2\log p_{y}(Y)).
\end{align}
$$<p>
See proof <a href="https://efron.ckirby.su.domains/papers/2011TweediesFormula.pdf">here</a></p>
<h3 id="approximation-gaussian">Approximation Gaussian<a hidden class="anchor" aria-hidden="true" href="#approximation-gaussian">#</a></h3>
<span id = "approx_gaussian">
^08dcf7
$$
\begin{aligned}
p_{X|Y}(x) &= \frac{p_{Y|X}(y) p_{X}(x)}{p_{Y}(y)} \\
    &= \frac{p_{Y|X}(y) p_{X}(x)}{\int p_{Y|X}(y)p_{X}(x)dx} \\ 
\end{aligned}
$$<p>
Note that $p_{Y|X}$ is Gaussian and
</p>
$$
\begin{aligned}
p_{Y|X} p_{X}(x) &= \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|y-x\|^2 \right) p_{X}(x) \\
    &= \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|y-x\|^2 \right) (p_{X}(y) + \langle \nabla p_{X}(y), x-y \rangle + O(\|x-y\|^2) )
\end{aligned}
$$<p>
Then the denominator can be calculated as $\text{denom} = p_{X}(y)  + O(\sigma^2)$ since
</p>
$$
\begin{aligned}
\int \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|y-x\|^2 \right) p_{X}(y) dx &= p_{X}(y) \\
\int \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|y-x\|^2 \right) (x-y) dx &= 0 \\
\int \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|y-x\|^2 \right) O(\|x-y\|^2) &= O(\sigma^2).
\end{aligned}
$$<p>It follows that
</p>
$$
\begin{aligned}
p_{X|Y}(x) &= \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|y-x\|^2 \right)  \frac{p_{X}(y) + \langle \nabla p_{X}(y), x-y \rangle + O(\|x-y\|^2)}{p_{X}(y)+ O(\sigma^2)} \\
 &=  \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|y-x\|^2 \right) (1 + \langle \nabla\log p_{X}(y), x-y \rangle + \text{h.o.t.} ) \\
 &=  \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|x-y\|^2 \right)  \exp(\langle \nabla \log p_{X}(y), x-y \rangle ) + \text{h.o.t.}\\
 &=  \frac{1}{(2\pi \sigma)^{d/2}}\exp\left( -\frac{1}{2\sigma^2} \|x-y - \sigma^2\nabla\log p_{Y}(y)\|^2 + \text{h.o.t.} \right) + \text{h.o.t.} \\ 
 &\approx \mathcal{N}(y+\sigma^2\nabla\log p_{Y}(y), \sigma^2 I).
\end{aligned}
$$

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://wangchxx.github.io/posts/diffusion_4_conditional_df/">
    <span class="title">« Prev</span>
    <br>
    <span>DL | Diffusion Models 4 - Conditional Diffusion Models</span>
  </a>
  <a class="next" href="https://wangchxx.github.io/posts/diffusion_2_preliminary_sde/">
    <span class="title">Next »</span>
    <br>
    <span>DL | Diffusion Models 2 - Preliminary ODE and SDE</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://wangchxx.github.io/">My Notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
